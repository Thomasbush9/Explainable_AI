{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imnn2qjZHZiA"
      },
      "source": [
        "# [0.5] - VAEs & GANs (exercises)\n",
        "\n",
        "> **ARENA [Streamlit Page](https://arena-chapter0-fundamentals.streamlit.app/05_[0.5]_VAEs_&_GANs)**\n",
        ">\n",
        "> **Colab: [exercises](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part5_vaes_and_gans/0.5_VAEs_&_GANs_exercises.ipynb?t=20250330) | [solutions](https://colab.research.google.com/github/callummcdougall/ARENA_3.0/blob/main/chapter0_fundamentals/exercises/part5_vaes_and_gans/0.5_VAEs_&_GANs_solutions.ipynb?t=20250330)**\n",
        "\n",
        "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-uk/shared_invite/zt-2zick19fl-6GY1yoGaoUozyM3wObwmnQ), and ask any questions on the dedicated channels for this chapter of material.\n",
        "\n",
        "You can collapse each section so only the headers are visible, by clicking the arrow symbol on the left hand side of the markdown header cells.\n",
        "\n",
        "Links to all other chapters: [(0) Fundamentals](https://arena-chapter0-fundamentals.streamlit.app/), [(1) Transformer Interpretability](https://arena-chapter1-transformer-interp.streamlit.app/), [(2) RL](https://arena-chapter2-rl.streamlit.app/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM3Mo4ejHZiC"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/headers/header-05.png\" width=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NENRBH-OHZiC"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7fxZ-2-HZiC"
      },
      "source": [
        "Today, we're studying two important classes of generative image models: **Generative Adversarial Networks (GANs)** and **Variational Autoencoders (VAEs)**. Although these generally aren't SOTA any more (thanks in part to the rise of diffusion models), there are some deep conceptual insights which can be gleaned from studying these models (VAEs in particular) which help lay the groundwork for more advanced models.\n",
        "\n",
        "These exercises will also hopefully bring much of this chapter full-circle:\n",
        "\n",
        "* We'll cover transposed convolutions, which will serve as a refresher on some of the ideas behind convolutions **(day 2: CNNs & ResNets)**\n",
        "* We'll be assembling NNet architectures from scratch, like in the ResNets exercises **(day 2: CNNs & ResNets)**\n",
        "* We'll work with different loss functions, and think intuitively about what it means to optimize them **(day 3: Optimization & Hyperparameters)**\n",
        "* We'll be working with `wandb`, and will learn how to log outputs produced by our models **(day 3: Optimization & Hyperparameters)**\n",
        "* We'll have to think carefully about how gradient propagation works between different parts of our model **(day 4: Backpropagation)**\n",
        "\n",
        "Note, many of today's exercises (especially those involving building models & writing forward pass functions) don't have as rigorous unit tests as previous days of content. Part of this is because the design spec for these models is less strict (we're not copying over weights from a pretrained model like last time, all that matters is that our model actually trains correctly, and so small changes to the solution architecture can sometimes be allowed). However, another part is that we're trying to move participants away from being too reliant on unit tests, and towards being able to independently reason through exercises or replications given just a description of the task or a paper implementation to follow. This is an invaluable skill to develop for any aspiring ML practitioner!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHJpyi8HZiC"
      },
      "source": [
        "## Content & Learning Objectives\n",
        "\n",
        "### 1️⃣ Autoencoders & VAEs\n",
        "\n",
        "Autoencoders are a relatively simple architecture, at least compared to GANs: you learn a compressed representation of your data (mainly using linear layers and convolutions), then reconstruct it back into an image (with linear layers and transposed convolutions).\n",
        "\n",
        "Although autoencoders can learn some interesting low-dimensional representations, they are less good for generating images because their latent spaces aren't generally meaningful. This leads to VAEs, which solve this problem by having their encoders map to a distribution over latent vectors, rather than a single latent vector. This incentivises the latent space to be more meaningful, and we can more easily generate images from sample vectors in this space.\n",
        "\n",
        "We start with some reading material on autoencoders and transposed convolutions (which are often used in parallel with convolutions, to take a latent space and map it back into a full-size image). Then, we actually implement and train VAEs to generate MNIST images, as well as a do a bit of exploring our our autoencoders' latent spaces.\n",
        "\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Learn about the transposed convolution operation\n",
        "> - Understand the basic architecture of autoencoders and VAEs\n",
        "> - Learn about the reparameterization trick for VAEs\n",
        "> - Implement your own autoencoder\n",
        "> - Implement your own VAE, and use it to generate realistic MNIST images\n",
        "> - (optional) Dive deeper into the mathematical underpinnings of VAEs, and learn about the ELBO loss function\n",
        "\n",
        "\n",
        "### 2️⃣ GANs\n",
        "\n",
        "Relative to autoencoders, GANs have a few more moving pieces in their architecture. They're best thought of as two separate networks (the generator and the discriminator) which are learning different goals simultaneously. The goal of the generator is to create images which fool the discriminator, and the goal of the discriminator is to distinguish between real and fake images. The ideal equilibrium point of training is when the generator produces perfect images and the discriminator can't tell the difference between real and fake - however, that's much simpler said than done! GANs are notoriously difficult to train, and we'll have to engage with some of these difficulties during our exercises.\n",
        "\n",
        "By the end of these exercises, you should have built and trained your own GANs, to generate celebrity pictures. By the time you're done, you'll hopefully have produced output like this (below), and you'll have everything you need to set up a competitor to Midjourney (plus or minus a few other foundational ML papers and an investment of a few hundred million dollars).\n",
        "                \n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan-last-output.png\" width=\"1100\">\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Understand the loss function used in GANs, and why it can be expected to result in the generator producing realistic outputs.\n",
        "> - Implement the DCGAN architecture from the paper, with relatively minimal guidance.\n",
        "> - Learn how to identify and fix bugs in your GAN architecture, to improve convergence properties.\n",
        "\n",
        "\n",
        "### 3️⃣ Bonus - Transposed Convolutions\n",
        "\n",
        "In this section, you'll implement the transposed convolution operation manually. This is similar to a regular convolution, but designed for upsampling rather than downsampling (i.e. producing an image from a latent vector rather producing output from an image). These are very important in many generative algorithms. Once you implement this, you'll be able to build your own GANs and VAEs from scratch, without using any pre-built layers.\n",
        "\n",
        "*Note - the bonus section from the CNNs day is a prerequisite for these bonus exercises. If you haven't completed that section, you'll need to do so before attempting these.*\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Learn about & implement the transposed convolution operation.\n",
        "> - Implement GANs and/or VAEs entirely from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWqNKjYvHZiD"
      },
      "source": [
        "## Setup code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LLjE6xqHZiD",
        "outputId": "2240abfa-b3d6-453a-ac4c-091f9a7df62e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting jaxtyping\n",
            "  Downloading jaxtyping-0.3.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading jaxtyping-0.3.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, wadler-lindig, torchinfo, fsspec, dill, multiprocess, jaxtyping, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 jaxtyping-0.3.0 multiprocess-0.70.16 torchinfo-1.8.0 wadler-lindig-0.1.4 xxhash-3.5.0\n",
            "--2025-03-31 21:33:40--  https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/callummcdougall/ARENA_3.0/zip/refs/heads/main [following]\n",
            "--2025-03-31 21:33:40--  https://codeload.github.com/callummcdougall/ARENA_3.0/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/content/main.zip’\n",
            "\n",
            "main.zip                [         <=>        ]  21.08M  9.31MB/s    in 2.3s    \n",
            "\n",
            "2025-03-31 21:33:43 (9.31 MB/s) - ‘/content/main.zip’ saved [22107639]\n",
            "\n",
            "Archive:  /content/main.zip\n",
            "24d2f21d9e998f8f0112ea89ab8230eae0d28f62\n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/\n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/0.0_Prerequisites_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/0.0_Prerequisites_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/numbers.npy  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part0_prereqs/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/0.1_Ray_Tracing_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/0.1_Ray_Tracing_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/pikachu.pt  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/pikachu.stl  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/test_with_pytest.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part1_ray_tracing/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/0.2_CNNs_&_ResNets_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/0.2_CNNs_&_ResNets_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/imagenet_labels.json  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/astronaut.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/chimpanzee.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/dragonfly.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/fireworks.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/frogs.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/golden_retriever.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/goofy.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/hourglass.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/iguana.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/platypus.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/resnet_inputs/volcano.jpg  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part2_cnns/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/0.3_Optimization_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/0.3_Optimization_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part3_optimization/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/0.4_Backprop_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/0.4_Backprop_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part4_backprop/utils.py  \n",
            "   creating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/\n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/0.5_VAEs_&_GANs_exercises.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/0.5_VAEs_&_GANs_solutions.ipynb  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/solutions.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/tests.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/part5_vaes_and_gans/utils.py  \n",
            "  inflating: /content/ARENA_3.0-main/chapter0_fundamentals/exercises/plotly_utils.py  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "chapter = \"chapter0_fundamentals\"\n",
        "repo = \"ARENA_3.0\"\n",
        "branch = \"main\"\n",
        "\n",
        "# Install dependencies\n",
        "try:\n",
        "    import torchinfo\n",
        "except:\n",
        "    %pip install torchinfo jaxtyping einops datasets\n",
        "\n",
        "# Get root directory, handling 3 different cases: (1) Colab, (2) notebook not in ARENA repo, (3) notebook in ARENA repo\n",
        "root = (\n",
        "    \"/content\"\n",
        "    if IN_COLAB\n",
        "    else \"/root\"\n",
        "    if repo not in os.getcwd()\n",
        "    else str(next(p for p in Path.cwd().parents if p.name == repo))\n",
        ")\n",
        "\n",
        "if Path(root).exists() and not Path(f\"{root}/{chapter}\").exists():\n",
        "    if not IN_COLAB:\n",
        "        !sudo apt-get install unzip\n",
        "        %pip install jupyter ipython --upgrade\n",
        "\n",
        "    if not os.path.exists(f\"{root}/{chapter}\"):\n",
        "        !wget -P {root} https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/{branch}.zip\n",
        "        !unzip {root}/{branch}.zip '{repo}-{branch}/{chapter}/exercises/*' -d {root}\n",
        "        !mv {root}/{repo}-{branch}/{chapter} {root}/{chapter}\n",
        "        !rm {root}/{branch}.zip\n",
        "        !rmdir {root}/{repo}-{branch}\n",
        "\n",
        "\n",
        "if f\"{root}/{chapter}/exercises\" not in sys.path:\n",
        "    sys.path.append(f\"{root}/{chapter}/exercises\")\n",
        "\n",
        "os.chdir(f\"{root}/{chapter}/exercises\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "le6EDYpTHZiD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Literal\n",
        "\n",
        "import einops\n",
        "import torch as t\n",
        "import torchinfo\n",
        "import wandb\n",
        "from datasets import load_dataset\n",
        "from einops.layers.torch import Rearrange\n",
        "from jaxtyping import Float, Int\n",
        "from torch import Tensor, nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Make sure exercises are in the path\n",
        "chapter = \"chapter0_fundamentals\"\n",
        "section = \"part5_vaes_and_gans\"\n",
        "root_dir = next(p for p in Path.cwd().parents if (p / chapter).exists())\n",
        "exercises_dir = root_dir / chapter / \"exercises\"\n",
        "section_dir = exercises_dir / section\n",
        "if str(exercises_dir) not in sys.path:\n",
        "    sys.path.append(str(exercises_dir))\n",
        "\n",
        "\n",
        "import part5_vaes_and_gans.tests as tests\n",
        "import part5_vaes_and_gans.utils as utils\n",
        "from part2_cnns.utils import print_param_count\n",
        "from plotly_utils import imshow\n",
        "\n",
        "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI-qd35QHZiE"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I get a NumPy-related error</summary>\n",
        "\n",
        "This is an annoying colab-related issue which I haven't been able to find a satisfying fix for. If you restart runtime (but don't delete runtime), and run just the imports cell above again (but not the `%pip install` cell), the problem should go away.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQb3ZK3zHZiE"
      },
      "source": [
        "# 1️⃣ Autoencoders & VAEs\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Learn about the transposed convolution operation\n",
        "> - Understand the basic architecture of autoencoders and VAEs\n",
        "> - Learn about the reparameterization trick for VAEs\n",
        "> - Implement your own autoencoder\n",
        "> - Implement your own VAE, and use it to generate realistic MNIST images\n",
        "> - (optional) Dive deeper into the mathematical underpinnings of VAEs, and learn about the ELBO loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NswfENCQHZiE"
      },
      "source": [
        "## Reading\n",
        "\n",
        "**Note** - before you start the reading, you might want to run the first block of code in the \"Loading data\" section, because it can take a few minutes to run.\n",
        "\n",
        "* [Understanding VAEs (Medium)](https://medium.com/towards-data-science/understanding-variational-autoencoders-vaes-f70510919f73)\n",
        "    * A clear and accessible explanation of autoencoders and VAEs.\n",
        "    * You can stop at \"Mathematical details of VAEs\"; we'll (optionally) cover this in more detail later.\n",
        "* [Six (and a half) intuitions for KL divergence](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence)\n",
        "    * Optional reading.\n",
        "    * KL divergence is an important concept in VAEs (and will continue to be a useful concept for the rest of this course).\n",
        "* [From Autoencoder to Beta-VAE](https://lilianweng.github.io/posts/2018-08-12-vae/)\n",
        "    * Optional reading.\n",
        "    * This is a more in-depth look at VAEs, the maths behind them, and different architecture variants.\n",
        "* [Transposed Convolutions explained with… MS Excel!](https://medium.com/apache-mxnet/transposed-convolutions-explained-with-ms-excel-52d13030c7e8) (optional)\n",
        "    * Optional reading.\n",
        "    * The first part (up to the highlighted comment) is most valuable, since understanding transposed convolutions at a high level is more important than understanding the exact low-level operations that go into them (that's what the bonus is for!).\n",
        "    * [These visualisations](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) may also help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja-xom_zHZiE"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "In these exercises, we'll be using either the Celeb-A dataset or the MNIST dataset. For convenience, we'll include a few functions here to load that data in.\n",
        "\n",
        "You should already be familiar with MNIST. You can read about the Celeb-A dataset [here](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) - essentially it's a large-scale face attributes dataset with more than 200k celebrity images, but we'll only be taking the images from this dataset rather the classifications. Run the code below to download the data from HuggingFace, and save it in your filesystem as images.\n",
        "\n",
        "The code should take 4-15 minutes to run in total, but feel free to move on if it's taking longer (you'll mostly be using MNIST in this section, and only using Celeb-A when you move on to GANs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxyvXvJ8HZiE"
      },
      "outputs": [],
      "source": [
        "celeb_data_dir = section_dir / \"data/celeba\"\n",
        "celeb_image_dir = celeb_data_dir / \"img_align_celeba\"\n",
        "\n",
        "os.makedirs(celeb_image_dir, exist_ok=True)\n",
        "\n",
        "if len(list(celeb_image_dir.glob(\"*.jpg\"))) > 0:\n",
        "    print(\"Dataset already loaded.\")\n",
        "else:\n",
        "    dataset = load_dataset(\"nielsr/CelebA-faces\")\n",
        "    print(\"Dataset loaded.\")\n",
        "\n",
        "    for idx, item in tqdm(enumerate(dataset[\"train\"]), total=len(dataset[\"train\"]), desc=\"Saving imgs...\", ascii=True):\n",
        "        # The image is already a JpegImageFile, so we can directly save it\n",
        "        item[\"image\"].save(celeb_image_dir / f\"{idx:06}.jpg\")\n",
        "\n",
        "    print(\"All images have been saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ8Pvpc2HZiE"
      },
      "source": [
        "<details>\n",
        "<summary>Note on why we double-nest our saving paths, i.e. <code>celeba/img_align_celeba</code></summary>\n",
        "\n",
        "In the code above, each image is saved in the format `'data/celeba/img_align_celeba/000001.jpg'`, etc. The reason for this double nesting (rather than e.g. `data/celeba/000001.jpg`) is that the child folders represent the image classes. If we were training a classifier, we'd have multiple folders within `data/celeba`, with each one being a different class. In this dataset, we only have one class (real celeb images), so we only need one child folder.\n",
        "\n",
        "</details>\n",
        "\n",
        "Now, here's some code to load in either the Celeb-A or MNIST data. It also applies transformations to the data, to get it into the right input format for us.\n",
        "\n",
        "The function below allows you to load in either the Celeb-A or MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V0S6LVwAHZiE"
      },
      "outputs": [],
      "source": [
        "def get_dataset(dataset: Literal[\"MNIST\", \"CELEB\"], train: bool = True) -> Dataset:\n",
        "    assert dataset in [\"MNIST\", \"CELEB\"]\n",
        "\n",
        "    if dataset == \"CELEB\":\n",
        "        image_size = 64\n",
        "        assert train, \"CelebA dataset only has a training set\"\n",
        "        transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(image_size),\n",
        "                transforms.CenterCrop(image_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ]\n",
        "        )\n",
        "        trainset = datasets.ImageFolder(root=exercises_dir / \"part5_vaes_and_gans/data/celeba\", transform=transform)\n",
        "\n",
        "    elif dataset == \"MNIST\":\n",
        "        img_size = 28\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        )\n",
        "        trainset = datasets.MNIST(\n",
        "            root=exercises_dir / \"part5_vaes_and_gans/data\",\n",
        "            transform=transform,\n",
        "            download=True,\n",
        "        )\n",
        "\n",
        "    return trainset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtAzltzQHZiE"
      },
      "source": [
        "We've also given you some code for visualising your data. You should run this code to make sure your data is correctly loaded in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B7eVuU08HZiE"
      },
      "outputs": [],
      "source": [
        "def display_data(x: Tensor, nrows: int, title: str):\n",
        "    \"\"\"Displays a batch of data, using plotly.\"\"\"\n",
        "    ncols = x.shape[0] // nrows\n",
        "    # Reshape into the right shape for plotting (make it 2D if image is monochrome)\n",
        "    y = einops.rearrange(x, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=nrows).squeeze()\n",
        "    # Normalize in the 0-1 range, then map to integer type\n",
        "    y = (y - y.min()) / (y.max() - y.min())\n",
        "    y = (y * 255).to(dtype=t.uint8)\n",
        "    # Display data\n",
        "    imshow(\n",
        "        y,\n",
        "        binary_string=(y.ndim == 2),\n",
        "        height=50 * (nrows + 4),\n",
        "        width=50 * (ncols + 5),\n",
        "        title=f\"{title}<br>single input shape = {x[0].shape}\",\n",
        "    )\n",
        "\n",
        "\n",
        "trainset_mnist = get_dataset(\"MNIST\")\n",
        "#trainset_celeb = get_dataset(\"CELEB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "phj6bH5aHZiF",
        "outputId": "3bf37820-714a-4b3a-a128-a935ced59319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"8a4ff130-3f5b-4b2d-994d-d130407db97a\" class=\"plotly-graph-div\" style=\"height:450px; width:500px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8a4ff130-3f5b-4b2d-994d-d130407db97a\")) {                    Plotly.newPlot(                        \"8a4ff130-3f5b-4b2d-994d-d130407db97a\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAAIwAAACMCAAAAACLqx7iAAARE0lEQVR4Xu2ceXBVZZrGH4Y7F0MlVAhJkYJKKsqSIQOSDEIYJRlMgYooKTGhRVIaoS0iFovtgNCAwyoiOCAgq4ALiwEbZRHUJgE1tCBLCB2WsIQGErJQQBbCZXLe973zx13Pd74TFMu2uyq\\u002fv873PLn3vvc73\\u002fp+5wZopplfj17reX2SKv5qtIyIiJj+9ucdNrldb6oekHiDiK6rajBplfGq5GUqu1NVrQliO7+wegszM1\\u002f6jOu+76\\u002f66HOF6WYV9XUGpJRnAtcA8MYum2Cybxmcoor2JN1gL0bW0KHJljcN6XeRmH7MIJ4SEKd\\u002fErgG0GJVkeV1Ht40tMEkLytinjBsWbKiR5xjZuYDu121iuPhEyJiouw83hwQz5uD6Sgfm8p+BlwzTsS2UlUMqyTOKyLiT1Un\\u002fYMxzEdbI2G16gBArxvM+a9xWc90d9ArS83B7JFpprKPfmWG8YIqwtG3jvIfdYTuJn5d9RDWYjU\\u002fr4peEm8Q7QwdPDkK4Hp\\u002fh3qwwRzMAVHr28Ma5jxVA7KJ9oQBWUSXolQPwALOb6FqAICuG7nqeIbnmmmjT35DTMG0r5CY4LKPSDaq01QRc5iWhAE4TZSuegDQOp8fUzUAcO6gmsfbdfQUmL736evlDd8lAHwiZ8KDy17ijrIxXRUxnV1fhACthjTwTNXz0Kn20oevWiunL1FgkDAFE\\u002fjCYZk7XDLCXwwix+Cv2qhieCV9AaDzIaLc1qrp5Zka5knRqnqA8wMFNxf4LtdLJoCeSa8vXl5bX72zlnQdO73G+La9KiKKKDZqUkEtk\\u002fG06vnp8Q3zcu8N8fHUbRofKDEt810u5+uFhYUsjTUHFo3o6KhqDPyVnzhmXq+KQHgFMRFdvkIVqhVEeBbxn81SJl31V5ZzHn8T6ncmbd++ffv2kZ4+9LKc9xsBVhiGoauw5GoueSchej8tUh0Td\\u002fhOf5OQSaW+S+dsuvR4sBdErsxXJSDxgmF8pooBUtw8VtUCPDhrD3OhuQ1n0nveq8SN9CeTFUyu9FUloNowCgI1aeFxJt0YAwCIX1bOzI27zWomX\\u002fJcvHaDbcZ7wCYYNozhqhaMbTDRr11gZj40RNEz6c6SxJjMHZe4dLN+iAUA5LqtQ\\u002f56N3OsKgZhVzPt004yMx94xjLOZBJR+WkiKpilWsHkSrYqJZaxa6F1ggzwijaYiK3nmJm\\u002fTw9RHaDjD0RMVOVrOTbkyipV6m+wrocF6OFmSzDJn11mZr41Vz8YRs8gpne7qLJCrvvnB4OzZLnxbzNz8bw54ar+s8i21kz0t3cLJpvyElTtNyPsK9qivx2\\u002fBWFL6R+nappp5u\\u002fDe+4TTc0yv5C8oLVkgNDoUVOCNrB+4q4z2a1kuibkuJmZeZvmlY5U\\u002fzLVnkUuy4AJ3L\\u002fiOBEtUWUArT+3CyZhwcXLxEREROvCVBeRElgn2vG2qy5T1eJX3iK+eIK0uYTFdsHsIM\\u002fOl4iIHlFdRIokqprKfvKsdB1+pc07vwsDzj3W6nRkVIlf9RHeU1V8\\u002fPlJVK9r4cZ\\u002f\\u002fpfqeLGsSfykTBt+A8Dwf7\\u002fw34qTTURUEoMuRP0UC0CHC0xTtS3YERMTDQBhl4k+szaaSNGtAD2c4X4AUOz2plb+xe9kAn\\u002fbOvAK\\u002fs2vBHN1PTBTu8WhK1cqAeCJtkCZbreCXqrg47b7PgCJsXKf6nSY8XAUAIzS1gzARK+qWhDP5RGRtf0i\\u002fKbY7UZmG8VRQOvNVBBoLAprbYJxM9sHM6LYRURHNOtE7LALJqbSlQpgFV32KeaYxrVu4e6Bv\\u002fxgEr2IW1U8xGUNBPq5gbo3drtUswl6bItc+h3w+ouYq1oAQnrvZHYzl3VSHQC2t6lHqa9rb1ctDzu0+SxHNrv54BRn9KHGdaoHOPpcofqyLXVEFROtfQJNBMPM7GZmHqR6AIAdUqNKALKIuITooHaz7RxCNO0RRBwnIhqmi8bNvEXVACD2jw917969+yIifTATdMEMM1wVjybmETEZV9Q74ZhHtCscUYfZNfNPRF+lJVmyz0zU1AqxjV0wz0qDdXzKvzASQEIBMZF6F1u+TbWvtEXvg3TmUYQ98UktBTb7Pt4nosWqGCDTLph0ud1V1TA+BgBSblJmQoI6HuRQ3XMRg7bcoumerNzwXbss26SxumAcg729eWSdXTA4JctVyUObZXxW1QBUUMPRM0Q0taXqBHGW2a3c3pQ9FAMAEVk3ieofNZs+Ftfa7HAnU4U5L+UZZyqjWvXE7m+3\\u002f41NppmTD0AUaWl3TKoHMPA\\u002f3Ni\\u002fYp\\u002fi+nBrJwnEvuxeXa6KAEKzFk1ur+tCwQwiYqVmjpMXvrrK5usDi2WoKgEAztKHqvTTiT1hCSZpLRERlRQu6WE2grnq0q2PgCmkD\\u002fKecY6ups9GN72U+7TI2rWbaaaZe6NrqTd5+w\\u002fA0mr6QtXuSsLEc5sm6ldWv4D2BUzH26mqlqAd1ugFoQAwQLsH1xE67P96hY3YXw6gcvsR1fXSdeGTLSYf0U5bLTY\\u002f2U03OQFARKWIiNzQnsbpeEcCUNGU+1UfANCXiG0y9iFlMkrV\\u002fOTckosi8q6qm4iNn1ld7TlBOi8i1Xl5eXnL846KyGDlLwEAXUuZtWeQAJAvU1QpQKEUicgDqhzEgOU3hZlPAwA6DerUyTsthV4UaxoYAGbTTuUsLYhntRsHLxnHRES6qbKfDw6JSO3ykdbVwvMirt6qCODA7fOWJWOAGHE1McdGF4nIVlX10m61XDv8bLxmCnauvC1iWcEDSGeaZ18xiHHL6OCyaUc54sHuAOwyTdNHLZ16SxUBIC0rG43jT6kyEJ4C3CgHMD4GatIDAOCG3UASf6pRxK7NhMwsHZJuvT0A0McQEdcgzeY9dJ+bk4HXJpQyuzU1FCNidyr4jEtERESXRcNbvFkfCrDQ87Ifp1lWe08Rl3ZB4jaiulP8g\\u002fX+NhEMxt0WsWszblEP6Pw8\\u002fGW1JxxaaD48Ch1LZbPQdSNXbUhK5VOaYNz2wWDQ8OFZNfpgDvHlgarmJzZp0BoWEdlnSpkNIpqO9juoZpmz++ka\\u002f3F4gKZqBgBazJBzlq+Q7ETEDK617\\u002fMARhwUEZkYLE0iAgqIUtGXaGGw4yVGJFXVgnGKnFKaWvTR6iwgkvlhs67g2C8ipodv5vE2JJbzeHQt5aBnBgLEiJg+S+0Dc4C1yuRV2GbSBmAC9v7VrCvQ0VTAnCZ1uwFx97x838WUWpPRNO22e57jia6xdu3JDSJSIqW6UQ2Inp7puWi5V6QxJdjqS9Q3p4aIq2x24TEiajoEADbI6dTO6DX8mMgCSxd+fWNVVfWueO0+PLpI2gAA2s8XkSKT16vOk6zWtV0Atg24b4FI6a5aET75844MPxVJDAFC3qwVcdcpzXFwHhOtHW\\u002fbSJ3F2mCwMMczVlxTjbvwexE5mp9\\u002fVESkLk1178Zh2aFKAADnxIkbRW7qG4Y992\\u002fyfAmRxneSVfOurBHTstI+sf8TcQ5NK0nHGeSXFKrW3Ynb\\u002fNFKVWummWb+WXggt1Gf1vsNeLjuwqK2qvhLyNpworHxO8+caEfrw+VxqgZgsGuR7lDsnon8gq\\u002fv2lUvmu2Ilw49e8a+KKc1z+R1afjqXof5P0z+mPmkqh65Ni8CiK+j6aoDAD1Wf\\u002f11KfPcz917ramPVvsPqicVASJyrnDwQ+nBpI7ZYhARNSo1MND74PgssZy1AMA4Zr79URmzZKkWsMCl2TB56fsDExFZH0SN3n\\u002f5ci3xj0REpGS9BpV4EgkPSoXmW85o4HXzo5BYxVWWdRmcFXtUyU\\u002fkCapcOWgznVa3lQNKiYgovl38oxeJlDdo5W2A8SI5ZgcAFkhpNNB5i9x6RbWAaYFHzi0U0G4AXarr1acJviGihld7A1hOdD5Scb04inXJm+Ri\\u002fqh1hx18bYLqAPj+K1UJsJdGA+hSXa7cyMfqiEofAQDsINJ8oodCXTDONVyWXsrah2lTjG4A+uuPE\\u002fM4s1XCgvwT6rb4a6Lv0gCg7fM13isNztMyQ9UAvMvMwqtjVB3AyiInsq+La4xqAEAlHTxElKHKePZovidtM5moyDaBEy+SDESmTTUP7uOYWXZaDyIBNGbAWZoROuz2E6oD4GQDMdXpaw0AnnaRS9MMAQDOTiNFjq8tvCS1HwbrLbeKyM5gxU+CpCN5BYCl36oWACRncFMHYEz0sqoBIbFD5x8+XCwiYpSWvvmQOa+5lZlZv8pPk24IbQcggVULANCDSVuhAIC33MyW+TVk\\u002fikRkZqyRpHVlo7a4Y\\u002fChz\\u002fgg6oOAEgT7\\u002fwQpg9muJttg3HuYRpjmUi+FtfOJWn9OuKMnLfOPVksk0OzeJ2qAwAGuL0veLrGpPtI57w4VfMS8jLxJ+GqCrdnl+2Yf6vSUi\\u002fof5OfahV3jrWTlr9mHF8uVRwAQLedlda+5CH0U6JxlnoB5JgDQKud4uqvWsBc2QfHq9Wib\\u002fXR5TkA4PjgpG4YbXOJ\\u002fqBqProRmTMaXs7Iuu2zsk\\u002fSD9Z6AeZwviOTr5nyMkGMuZ0TlvRiSZF2slxFG1TJR\\u002fwa0qTdAGC2686dO1t1IwWwinP3Mz+tyn7G3GaumaXOhACAAQ31ton8jUT6um6S8cxybea9rOTirjfYnmsn7KTlll59d9pOrN83QRV\\u002fCiHLKFfV\\u002fMynC\\u002fcQy73zChdobx4AIE3\\u002fa7Bfiz5lM7WNuplm\\u002fqnYVNpkJrCrd82no\\u002f\\u002fP+6XzT6BAzpqT9KHRwSPoeJqm5vB9ZB81+Oh4O7cpJsvbqgQAiLkjYh6+59CEoFIKUeegYhDZeYbBhqGdwGIXN8omVfQTepVdI1URAHqIbDMvFOZQTdCAl6EPJrx\\u002f6S3DOHFSH8zI2iMvTbcuEr04JjJf0i28HRtEvEcPPuYQ3XjIVwg9QKTZiqd\\u002fabBhGC9k64JxTnbNDUcS242pKXZPoS4RSzAjiehLXzYniXTBZHmfbH0xW\\u002feLu9E8FsAbFaruJe4Y8ze6if33DdZgWk4jIt9TCR3Oam5T1jWj\\u002fnyVYdQP1dVMRHWuA4g9bxfMCeabujPFl1xyxBIM2pwhKvSmVpLIGkw6G8ZeZBvGWGiCcZwujgDwvSxQDB\\u002fCPFeRQh8ZueK6yNhO1mCwlogSATjHjv3OGkx2DdfvTUD2kbEOhBZYgnmOUwE8b9S1Vwwv\\u002f+vmb9RcSbczIjdXdXHEaYIZRUQ5ePj1GUREVKzkny4YPBlA5\\u002fYAkG8JZs8pBxBdZfn2Xt4vl8IoVYSze\\u002fdYAHFi\\u002ffUlNjCzt40ys3lISKwO3oHtszRgmQqEFfD7mjwSgD7lrEvqeGl3VbapGpKIiPw\\u002fOVkTbHW\\u002fYBiBkvU2pUkPPH5RzrYxyz5WipzsoIoBCmRcoGAe28+7v6z5H5MCLDF9eIZlXrt2JzcssrHF+3WqAQAYP8qNgVdVNRhrH0wiouojw71XpprJN4wJ\\u002fkJ8tXFObacvfLt5yOU8tYl6iCnmxvdVMZgCTlUlhK3bv8KTQEoiokvB+ez8oP8wEV\\u002fCFWoCDkCL9+70VjUAQOdTzPNV0UeXPn16dMi4kBEV+\\u002fEQ1fOSREQU\\u002fOX3MXsbcOg25nO6+ac\\u002fz1IlD4OZ2ZOcU3F2e\\u002feWiOuaiFTdlHc8omU9UFMZDbw1mvzC7Nxw7HNvL5nU4l+Tb8\\u002f9XLcD3nTV+24qbYH92qx6+8W\\u002fA1DhPlkEAPjY7AZILiei4MPt1GsGG4ZhsLHXf79MPNRo13cvMmeoGgBggojs7G+pCSu9K4P\\u002fKQWAjjPZMAyj\\u002fFN97211rNTmYD6hmqdr0hoA4or2vKRqep4\\u002fNlt59xeLjRMv6G8+kMNq7tTHSBdPUrVfmVOFttV9qVqXRvk1qbyH5EUzP4\\u002f\\u002fB0a7nGAMXcCjAAAAAElFTkSuQmCC\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"MNIST data\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":450,\"width\":500},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8a4ff130-3f5b-4b2d-994d-d130407db97a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display MNIST\n",
        "x = next(iter(DataLoader(trainset_mnist, batch_size=25)))[0]\n",
        "display_data(x, nrows=5, title=\"MNIST data\")\n",
        "\n",
        "# Display CelebA\n",
        "# x = next(iter(DataLoader(trainset_celeb, batch_size=25)))[0]\n",
        "# display_data(x, nrows=5, title=\"CelebA data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scxOXQj2HZiF"
      },
      "source": [
        "### Holdout data\n",
        "\n",
        "Lastly, we'll also get some **holdout data** that we can use during training. The tensor below has shape `(10, 1, 28, 28)`, and contains a single image from each of the 10 MNIST classes. We do this because we want to monitor our autoencoder's reconstructions for each different image type while it trains. We might find that some images are reconstructed better than others!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FduCunu7HZiF",
        "outputId": "0bb2b661-98c0-4da5-ae71-2ee44b7a4c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d9095403-d91e-4e34-b57c-e957ec213079\" class=\"plotly-graph-div\" style=\"height:250px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d9095403-d91e-4e34-b57c-e957ec213079\")) {                    Plotly.newPlot(                        \"d9095403-d91e-4e34-b57c-e957ec213079\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAAcCAAAAACfA\\u002f\\u002fyAAAHg0lEQVR4Xu2Ye0xUVwLGPyPBaMAglkA0EDdVzBKpENdCtmqUNHWND2JbTFtJl7Qx9RFfa1RspVGLoqvGrhpbtcq2rmvBRwVcUbegVkirrgEb62PVMSA+wFR5qBjn++7sHzMwc89cqGu2yW7X319zft\\u002fMZOa7Z845d4BnPOP\\u002fgnhXjameAWBDA\\u002feb7mdlSL7yk035X0d0hVjd27Q\\u002fJ0l3Sf5oWj8fyDPCdACAsJh3F4WaEkB8wlSPJGmfU+oj7fZAU6FrZGRkzsqv+vzV0\\u002fqhmcUXU\\u002fNHmfaJ6XH6Rj\\u002fT\\u002fQQvXhfv1TO1oy+Rdd+t4aYE8KtPqkmuNzUSVl+rpUiS3B7erodPDHgOgIUH7MXE9X97S6EkqWaPmk+MtIUAUkm9acono8\\u002fgwXG\\u002fty6Emb5Tug+7RvHU69QiM\\u002fLxodupmIGf3qeufc\\u002fg615Msq0YvtSuc3YEPAfosvms7ZXJd+XDnfnqqylB7xrvktJNGcAfsr+QfjAtkLjl8GGXtPwrz9fBP8OUjWelOZM2ppgBgB3eb5FVpl1m5OXlO+7v47oZsuen90hejBtADjMizCRv5i7PzS2zF3PFXkxf6wvbOPKyJKnyYGuTzbfxEUv6mq6dEdML3ST5+LyZYJakh5\\u002fXSVamGWHSbarsLKkvzQQYclcqn6u6wekehxTAsDq3+21TIoskL8XCqZiQ2NgYAAivJff4f58uezGl1mLbGOmfTZfO9EDCFrv3UvnwygDTeYk5VlvbRJ0iSQbt5kseaPuqKCTVq968uCGpzSwfFRJ2kJpnRN51tyRsbHYUoBbHjWmrVGY64G\\u002fklV1xwHiHYtrIaCE\\u002fbh+98MBeTKVlzt\\u002fwLlv0luHaSBfznCfMyy6S5MDeA0ddI0vNeLXligH6F1r3p5lRFlkaDmSSNVFmFr9T9dWvex+LO+0hAOA5uRvSTAn0WfLbKAB4t+Ni3igj6V97F1q2YqJvWbGBYwDAapV3MR0AIGKNOB8AZq9ZY0RHSD6YMRTAJvLKc0aKlHP6vEefYt2ZYya54vpwABfIoLUrtJiNo3v7roR4wp4CQL8zcueYMoBtHRUz+VwryX90bxf51sKAGDusixGBYwBAj3K9YjoAQNhRj1KAuXNcksc2c15pJl3ehayYXBsYAQBCt6ou3SXNNIMcte7vDnSb8EBLzQyppP+A4ljMVLcO9TSlj1nZi7JP8Zuupu+3+Pjx4yJ5d0rAd8i3\\u002fBMvPKO41Zrsz9p5vqnmzzMcJs04yjUASfvI5vP6Ni4gOUx+kwYAvd5q9D2ys1aSpS3m9Iy4zf0A+p8kC3oYGVCpcv\\u002fAowr\\u002fwEd6o\\u002ft4tCkBAN2Hlkgeqe55M0l0kd7tuihQ51sZAAYnz\\u002ft4U1NLQ0kTgzZkAJjYKC2IMW3YTNYtQ\\u002fxO1f8leYTOBxbz2ply79OzybNBL4R3V7JK4k0dRcZFLahoEt3jzQzjHnK2fyRu9A+89JOUb0oACHnxOlvqCpvJW\\u002fPNc2GiS5I8kjQmQG\\u002fSj1VVVbIeN1aum9w3pP5xQBZA4hFpk7nMjiFzEF3Mxo2hgy40Bn1MABjfytag5RVA192WZZWYFoi4RZGsvc5bZgRk8GZ7x6F5OhJ0MvzE7XY7XdjQCeTilxBZTZKTzGbi3v\\u002fNoEGDBq0jA4vBgqKioqKid7x70RTrSmAWQEQm9XfDLSCBCnIEUklz9fUicorpAGC3JBWbFkBKgy79MSHmGNeZCZBBV9vD0I9YMzowA4Ckq273HlMCCMkjD0Qg6rRal+4lD6UlO230PY1ibBRYq0zVziM9Gmk3edqHpBuajXiXAuZ4ACs8ksMl7PO+pdOf6TvTtzPcE7wsAxn8k+9R0k7utUUAgAa3uyJoFgFdV7JpWi8M\\u002fY4XRyH8dzua6C84gIzOi0k1lZcXlpVKVcb6m8e9SLrO7bUNJ\\u002fs6fCAgtFSc7rBoZ8rKDsvUdtO3M1oMOsMAGfIdFOfelf187kVut9Nd21Q2vxE5pvA+c7xL\\u002fZsHDvgPpSFjfTv0O81PUczAjTckPT5o6FQydWojqXrnd+w+hdoRYVpg5D2N69bvsjo5cDgXw0frk2Izimvk2mUeQwHke6TADaCNW3xw5iLJD4I2amB4KWMBIDLzHtnS8Z8EBZ7g2wzEzL0qSScnmMGQZu9NqfO6i7AvyVkO8wXLraMImdFgOa3KXjqYMSRvXCBZscyMACTVqXWNeX8BAFUkWTyvf4gZAKgmN6xYsWLFaZFfv2amfgqsLFNFp\\u002f0gSZUTHb7i2DKR22Y7\\u002fyuEX5OXTAcAyFV5SIbuON58eZnmWEzfb0mR9W0rjZ2RbjnvHGGZ67Kjza3IRzV96OZmp1LbKLA220Xk7suSdCLdf1Z+UgZupe1o42ezCo5JwecUP4keORSDmCUU13Zw19phMZ2RvI0kealqfaIZ2Sjw2IpJ2VMrSfeXBx9Cf5qdZAe\\u002fldmSdWdpp1X\\u002fkw6LSOfEHH+KYhD6XgP3vOd0BLWRZZ8xKyWdy8uNCHRPSkIJNzns1ADQa37L0TmmtJPFsgTT\\u002fUJYxasd9PIkhB9i4dNM0\\u002f8B0oL\\u002fNfh3CN\\u002fAX+qU+Q\\u002fyL63HCVafPD22AAAAAElFTkSuQmCC\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"MNIST holdout data\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":250,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d9095403-d91e-4e34-b57c-e957ec213079');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "testset = get_dataset(\"MNIST\", train=False)\n",
        "HOLDOUT_DATA = dict()\n",
        "for data, target in DataLoader(testset, batch_size=1):\n",
        "    if target.item() not in HOLDOUT_DATA:\n",
        "        HOLDOUT_DATA[target.item()] = data.squeeze()\n",
        "        if len(HOLDOUT_DATA) == 10:\n",
        "            break\n",
        "HOLDOUT_DATA = t.stack([HOLDOUT_DATA[i] for i in range(10)]).to(dtype=t.float, device=device).unsqueeze(1)\n",
        "\n",
        "display_data(HOLDOUT_DATA, nrows=1, title=\"MNIST holdout data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUf9W397HZiF"
      },
      "source": [
        "You might be wondering why we do this, rather than just e.g. generating some random noise and seeing what the decoder's reconstruction is. The answer is that **our autoencoder's latent space might not be meaningful**. In other words, it's unclear exactly how to sample from it to get output which will look like an MNIST image. We'll return to this idea when we study VAEs later in these exercises.\n",
        "\n",
        "*For the rest of this section (not including the bonus), we'll assume we're working with the MNIST dataset rather than Celeb-A.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4flXFslSHZiF"
      },
      "source": [
        "## Transposed Convolutions\n",
        "\n",
        "**What are transposed convolutions, and why should we care about them?** One high-level intuition goes something like this: most of the generator's architecture is basically the discriminator architecture in reverse. We need something that performs the reverse of a convolution - not literally the inverse operation, but something reverse in spirit, which uses a kernel of weights to project up to some array of larger size.\n",
        "\n",
        "**Importantly, a transposed convolution isn't literally the inverse of a convolution**. A lot of confusion can come from misunderstanding this!\n",
        "\n",
        "You can describe the difference between convolutions and transposed convolutions as follows:\n",
        "\n",
        "* In convolutions, you slide the kernel around inside the input. At each position of the kernel, you take a sumproduct between the kernel and that section of the input to calculate a single element in the output.\n",
        "* In transposed convolutions, you slide the kernel around what will eventually be your output, and at each position you add some multiple of the kernel to your output.\n",
        "\n",
        "Below is an illustration of both for comparison, in the 1D case (where $*$ stands for the 1D convolution operator, and $*^T$ stands for the transposed convolution operator). Note the difference in size between the output in both cases. With standard convolutions, our output is smaller than our input, because we're having to fit the kernel inside the input in order to produce the output. But in our transposed convolutions, the output is actually larger than the input, because we're fitting the kernel inside the output.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/convtranspose-1.png\" width=\"700\">\n",
        "\n",
        "We won't actually have you implement the transposed convolution operation in these exercises; instead we've pushed it to the bonus section. For now, you can just use the `ConvTranspose2d` module which has already been imported for you from today's solutions file, in place of your own implementation. You can use it exactly the same way you use normal convolutional layers: for instance `ConvTranspose2d(32, 16, 4, stride=2, padding=1)` will define a convolution layer that maps a tensor from `(batch_size, in_channels=32, height, width)` up to shape `(batch_size, out_channels=16, height * 2, width * 2)`.\n",
        "\n",
        "[These visualisations](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md) (linked in the reading material) may also help build intuition for the transposed convolution module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFw_uIhuHZiF"
      },
      "source": [
        "## Autoencoders\n",
        "\n",
        "We'll start by looking at **Autoencoders**, which are much conceptually simpler than VAEs. These are simply systems which learn a compressed representation of the input, and then reconstruct it. There are two parts to this:\n",
        "\n",
        "* The **encoder** learns to compress the output into a latent space which is lower-dimensional than the original image.\n",
        "* The **decoder** learns to uncompress the encoder's output back into a faithful representation of the original image.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/ae-diagram-l.png\" width=\"700\">\n",
        "                \n",
        "Our loss function is simply some metric of the distance between the input and the reconstructed input, e.g. the $l_2$ loss.\n",
        "\n",
        "You'll start by writing your own autoencoder. We've given some guidance on architecture below, although in general because we're working with a fairly simple dataset (MNIST) and a fairly robust architecture (at least compared to GANs in the next section!), you model is still likely to work well even if it deviates slightly from the specification we'll give below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EXaIkXxHZiF"
      },
      "source": [
        "### Exercise - implement autoencoder\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 15-30 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "*Note - for the rest of this section (not including the bonus), we'll assume we're working with the MNIST dataset rather than Celeb-A.*\n",
        "\n",
        "Your encoder should consist of two convolutional blocks (i.e. convolution plus ReLU), followed by two fully connected linear layers with a ReLU in between them. Both convolutions will have kernel size 4, stride 2, padding 1 (recall this halves the size of the image). We'll have 16 and 32 output channels respectively.\n",
        "\n",
        "The decoder will be the exact mirror image of the encoder (with convolutions replaced by transposed convolutions).\n",
        "\n",
        "The only free parameters in your implementation will be `latent_dim_size` and `hidden_dim_size`. The former determines the size of the latent space (otherwise called the bottleneck dimension) between the encoder and decoder, and the latter determines the size of the final linear layer we insert just before the end / just after the start of the encoder / decoder's architecture respectively.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/ae-help-10.png\" width=\"1100\">\n",
        "\n",
        "A few extra notes:\n",
        "\n",
        "* You'll need to reshape between the convolutional blocks and linear layers. For this, you might find the `einops` library helpful - they have a function `einops.layers.torch.Rearrange` (imported for you as `Rearrange`) which works like the standard einops function, except that it takes a string and returns a module which performs the corresponding rearrangement. Just like any other module, it can be used inside things like `Sequential` (this way, the logic inside the `forward` method can be very simple!).\n",
        "\n",
        "```python\n",
        ">>> x = t.randn(100, 3, 4, 5)\n",
        ">>> x.shape\n",
        "torch.Size([100, 3, 4, 5])\n",
        "\n",
        ">>> module = Rearrange(\"b c h w -> b (c h w)\")\n",
        ">>> module(x).shape\n",
        "torch.Size([100, 60])\n",
        "```\n",
        "\n",
        "* Note that we don't include a ReLU in the very last layer of the decoder or generator, we only include them ***between successive convolutions or linear layers*** - can you see why it wouldn't make sense to put ReLUs at the end?\n",
        "* The convolutions don't have biases, although we have included biases in the linear layers (this will be important if you want your parameter count to match the solution, but not really that important for good performance).\n",
        "\n",
        "Now, implement your autoencoder below. We recommend you define `encoder` and `decoder` to help make your code run with the functions we've written for you later.\n",
        "\n",
        "<!-- You can test your answer by comparing the architecture to the solution directly. As this course goes on, we won't always include test functions as the exercises get a bit more open-ended and solutions to them are likely to vary more; it's also good practice to find ways to test your answers when you don't have access to black-box tests that you can trust!\n",
        "\n",
        "from part5_vaes_and_gans.solutions import Autoencoder as SolutionAutoencoder\n",
        "\n",
        "soln_Autoencoder = SolutionAutoencoder(latent_dim_size=5, hidden_dim_size=128)\n",
        "my_Autoencoder = Autoencoder(latent_dim_size=5, hidden_dim_size=128)\n",
        "\n",
        "print_param_count(my_Autoencoder, soln_Autoencoder)\n",
        "# print_param_count(my_Autoencoder, soln_Autoencoder, filename=str(section_dir / \"0503.html\"))\n",
        "\n",
        "<iframe src=\"https://info-arena.github.io/ARENA_img/misc/media-05/0503.html\" width=\"920\" height=\"470\"></iframe> -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BK0atBSBHZiF",
        "outputId": "c6c79772-1b37-4e9c-bc3e-36bdb5b9113b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests in `test_autoencoder` passed!\n"
          ]
        }
      ],
      "source": [
        "# Importing all modules you'll need, from previous solutions (you're encouraged to substitute your own implementations instead, if you want to!)\n",
        "from part2_cnns.solutions import BatchNorm2d, Conv2d, Linear, ReLU, Sequential\n",
        "\n",
        "from part5_vaes_and_gans.solutions import ConvTranspose2d\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim_size: int, hidden_dim_size: int):\n",
        "        \"\"\"Creates the encoder & decoder modules.\"\"\"\n",
        "        super().__init__()\n",
        "        self.latent = latent_dim_size\n",
        "        self.hidden = hidden_dim_size\n",
        "        c2l = Rearrange('b c h w -> b (c h w)')\n",
        "        l2c = Rearrange('b (c h w)-> b c h w', c = 32, h=7, w=7)\n",
        "        self.encoder = Sequential(\n",
        "            #convolutional block\n",
        "            Conv2d(in_channels=1, out_channels=16,kernel_size=4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(num_features=16),\n",
        "            Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(num_features=32),\n",
        "            c2l,\n",
        "            Linear(in_features=32*7*7, out_features=self.hidden),\n",
        "            ReLU(),\n",
        "            Linear(in_features=self.hidden, out_features=self.latent)\n",
        "\n",
        "        )\n",
        "        self.decoder = Sequential(\n",
        "            Linear(self.latent, self.hidden),\n",
        "            ReLU(),\n",
        "            Linear(self.hidden, 32*7*7),\n",
        "            ReLU(),\n",
        "            l2c,\n",
        "            ConvTranspose2d(32, 16, 4, 2, 1),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "            ConvTranspose2d(16, 1, 4, 2, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Returns the reconstruction of the input, after mapping through encoder & decoder.\"\"\"\n",
        "        z = self.encoder(x)\n",
        "        x_prime = self.decoder(z)\n",
        "        return x_prime\n",
        "\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "tests.test_autoencoder(Autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McchIH54HZiF"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, latent_dim_size: int, hidden_dim_size: int):\n",
        "        \"\"\"Creates the encoder & decoder modules.\"\"\"\n",
        "        super().__init__()\n",
        "        self.latent_dim_size = latent_dim_size\n",
        "        self.hidden_dim_size = hidden_dim_size\n",
        "        self.encoder = Sequential(\n",
        "            Conv2d(1, 16, 4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            Conv2d(16, 32, 4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            Rearrange(\"b c h w -> b (c h w)\"),\n",
        "            Linear(7 * 7 * 32, hidden_dim_size),\n",
        "            ReLU(),\n",
        "            Linear(hidden_dim_size, latent_dim_size),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            Linear(latent_dim_size, hidden_dim_size),\n",
        "            ReLU(),\n",
        "            Linear(hidden_dim_size, 7 * 7 * 32),\n",
        "            ReLU(),\n",
        "            Rearrange(\"b (c h w) -> b c h w\", c=32, h=7, w=7),\n",
        "            ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            ConvTranspose2d(16, 1, 4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Returns the reconstruction of the input, after mapping through encoder & decoder.\"\"\"\n",
        "        z = self.encoder(x)\n",
        "        x_prime = self.decoder(z)\n",
        "        return x_prime\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHiPNfwnHZiF"
      },
      "source": [
        "## Training your Autoencoder\n",
        "\n",
        "Once you've got the architecture right, you should write a training loop which works with [MSE loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) between the original and reconstructed data. The standard Adam optimiser with default parameters should suffice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbxpvinyHZiF"
      },
      "source": [
        "### Logging images to `wandb`\n",
        "\n",
        "Weights and biases provides a nice feature allowing you to log images! This requires you to use the function `wandb.Image`. The first argument is `data_or_path`, which can be any of the following:\n",
        "\n",
        "* A numpy array in shape `(height, width)` or `(height, width, 1)` -> interpreted as monochrome image\n",
        "* A numpy array in shape `(height, width, 3)` -> interpreted as RGB image\n",
        "* A PIL image (can be RGB or monochrome)\n",
        "\n",
        "When it comes to logging, you can log a list of images rather than a single image. Example code, and the output it produces from my GAN (you'll create output like this in the next section!):\n",
        "\n",
        "```python\n",
        "# arr is a numpy array of shape (8, 28, 28, 3), i.e. it's an array of 8 RGB images\n",
        "images = [wandb.Image(a) for a in arr]\n",
        "wandb.log({\"images\": images}, step=self.step)\n",
        "```\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/gan_output_2.png\" width=\"750\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-TCkNTHZiG"
      },
      "source": [
        "### Exercise - write autoencoder training loop\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 20-35 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should now implement your training loop below. We've filled in some methods for you (and have given you a dataclass to hold arguments), your job is to complete `training_step` and `train`. The former should perform a single training step by optimizing against the reconstruction loss between the image and target (you might find `nn.MSELoss` suitable for this). The latter should be structured like training code you might have seen before, in other words:\n",
        "\n",
        "- Iterate over `self.args.epochs` epochs\n",
        "- For each epoch, you should:\n",
        "    - Iterate over the training data and perform a training step for each batch. We also recommend using & updating a progress bar, and logging to wandb if this is enabled in your training arguments.\n",
        "    - Evaluate the model on the holdout data via `self.log_samples()`, every `args.log_every_n_steps` total steps.\n",
        "\n",
        "Some last tips before we get started:\n",
        "\n",
        "- Don't use wandb until you've ironed out the bugs in your code, and loss seems to be going down based on the in-notebook logging. This is why we've given you the `use_wandb` argument in your dataclass.\n",
        "- Remember to increment `self.step` as you train (this is necessary if you're passing this argument to `wandb.log`). Note we're using `step` here rather than `examples_seen` like in earlier exercises, because it'll prove more useful for our purposes.\n",
        "- Your wandb logging should take place in `training_step` not `train` (this is better practice in general because often there will be variables only defined in the scope of `training_step` that you might want to log - even though that's not the case here, it will be later).\n",
        "- Iterating through `self.trainloader` will give you tuples of `(img, label)`, but you don't need to use the labels - all you need is the image.\n",
        "\n",
        "If you find yourself disconnecting from runtime when you run the training code, then visit https://wandb.ai/authorize and get a `<LOGIN-KEY>` to use in this code: `wandb.login(key=<LOGIN-KEY>)`. If this fails (i.e. you're still getting disconnected), then we recommend either using VSCode instead or some other IDE, or just setting `wandb=False` and displaying / saving your logged output inline. You can take `imshow` and turn it into saved output via code like `imshow(..., return_fig=True).write_html(\"filename.html\")`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzfhuvinHZiG"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class AutoencoderArgs:\n",
        "    # architecture\n",
        "    latent_dim_size: int = 5\n",
        "    hidden_dim_size: int = 128\n",
        "\n",
        "    # data / training\n",
        "    dataset: Literal[\"MNIST\", \"CELEB\"] = \"MNIST\"\n",
        "    batch_size: int = 512\n",
        "    epochs: int = 10\n",
        "    lr: float = 1e-3\n",
        "    betas: tuple[float, float] = (0.5, 0.999)\n",
        "\n",
        "    # logging\n",
        "    use_wandb: bool = False\n",
        "    wandb_project: str | None = \"day5-autoencoder\"\n",
        "    wandb_name: str | None = None\n",
        "    log_every_n_steps: int = 250\n",
        "\n",
        "\n",
        "class AutoencoderTrainer:\n",
        "    def __init__(self, args: AutoencoderArgs):\n",
        "        self.args = args\n",
        "        self.trainset = get_dataset(args.dataset)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True)\n",
        "        self.model = Autoencoder(\n",
        "            latent_dim_size=args.latent_dim_size,\n",
        "            hidden_dim_size=args.hidden_dim_size,\n",
        "        ).to(device)\n",
        "        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)\n",
        "        self.step = 0\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def training_step(self, img: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Performs a training step on the batch of images in `img`. Returns the loss. Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        # pass imgs to model\n",
        "\n",
        "        pred = self.model.forward(img)\n",
        "        loss = self.loss(pred, img)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        self.step += img.shape[0]\n",
        "\n",
        "        if args.use_wandb:\n",
        "          wandb.log(dict(loss=loss), step=self.step)\n",
        "\n",
        "        return loss\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def log_samples(self) -> None:\n",
        "        \"\"\"\n",
        "        Evaluates model on holdout data, either logging to weights & biases or displaying output.\n",
        "        \"\"\"\n",
        "        assert self.step > 0, \"First call should come after a training step. Remember to increment `self.step`.\"\n",
        "        output = self.model(HOLDOUT_DATA)\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log({\"images\": [wandb.Image(arr) for arr in output.cpu().numpy()]}, step=self.step)\n",
        "        else:\n",
        "            display_data(t.concat([HOLDOUT_DATA, output]), nrows=2, title=\"AE reconstructions\")\n",
        "\n",
        "    def train(self) -> Autoencoder:\n",
        "        \"\"\"Performs a full training run.\"\"\"\n",
        "        self.step = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "            wandb.watch(self.model)\n",
        "\n",
        "        # YOUR CODE HERE - iterate over epochs, and train your model\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "\n",
        "            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)), ascii=True)\n",
        "\n",
        "            for imgs, _ in progress_bar:\n",
        "                imgs = imgs.to(device)\n",
        "                loss = self.training_step(imgs)\n",
        "                progress_bar.set_description(f\"{epoch=:02d}, {loss=:.4f}, step={self.step:05d}\")\n",
        "                #pbar.set_postfix(loss=f\"{loss:.3f}\", ex_seen=f\"{self.step=:06}\")\n",
        "                # log every 250 steps\n",
        "                if self.step % self.args.log_every_n_steps == 0:\n",
        "\n",
        "                  self.log_samples()\n",
        "\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "\n",
        "args = AutoencoderArgs()\n",
        "trainer = AutoencoderTrainer(args)\n",
        "autoencoder = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgPGmMD-HZiG"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class AutoencoderTrainer:\n",
        "    def __init__(self, args: AutoencoderArgs):\n",
        "        self.args = args\n",
        "        self.trainset = get_dataset(args.dataset)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True)\n",
        "        self.model = Autoencoder(\n",
        "            latent_dim_size=args.latent_dim_size,\n",
        "            hidden_dim_size=args.hidden_dim_size,\n",
        "        ).to(device)\n",
        "        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)\n",
        "\n",
        "    def training_step(self, img: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Performs a training step on the batch of images in `img`. Returns the loss. Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        # Compute loss, backprop on it, and perform an optimizer step\n",
        "        img_reconstructed = self.model(img)\n",
        "        loss = nn.MSELoss()(img, img_reconstructed)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Increment step counter and log to wandb if enabled\n",
        "        self.step += img.shape[0]\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log(dict(loss=loss), step=self.step)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def log_samples(self) -> None:\n",
        "        \"\"\"\n",
        "        Evaluates model on holdout data, either logging to weights & biases or displaying output.\n",
        "        \"\"\"\n",
        "        assert self.step > 0, \"First call should come after a training step. Remember to increment `self.step`.\"\n",
        "        output = self.model(HOLDOUT_DATA)\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log({\"images\": [wandb.Image(arr) for arr in output.cpu().numpy()]}, step=self.step)\n",
        "        else:\n",
        "            display_data(t.concat([HOLDOUT_DATA, output]), nrows=2, title=\"AE reconstructions\")\n",
        "\n",
        "    def train(self) -> Autoencoder:\n",
        "        \"\"\"Performs a full training run.\"\"\"\n",
        "        self.step = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "            wandb.watch(self.model)\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            # Iterate over training data, performing a training step for each batch\n",
        "            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)), ascii=True)\n",
        "            for img, label in progress_bar:  # remember that label is not used\n",
        "                img = img.to(device)\n",
        "                loss = self.training_step(img)\n",
        "                progress_bar.set_description(f\"{epoch=:02d}, {loss=:.4f}, step={self.step:05d}\")\n",
        "                if self.step % self.args.log_every_n_steps == 0:\n",
        "                    self.log_samples()\n",
        "\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return self.model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhYpFAD5HZiG"
      },
      "source": [
        "After the first epoch, you should be able to get output of the following quality:\n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ae-epoch-1.png\" width=\"750\">\n",
        "\n",
        "And by the 10th epoch, you should be getting something like this:\n",
        "\n",
        "<br>\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ae-epoch-10.png\" width=\"750\">\n",
        "\n",
        "Note how the reconstructions it's mixing up features for some of the numbers - for instance, the 5 seems to have been partly reconstructed as a 3. But overall, it seems pretty accurate!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sde2dywRHZiG"
      },
      "source": [
        "## Latent space of an autoencoder\n",
        "\n",
        "We'll now return to the issue we mentioned briefly earlier - how to generate output? We might want to generate output by just producing random noise and passing it through our decoder, but this raises a question - how should we interpret the latent space between our encoder and decoder?\n",
        "\n",
        "We can try and plot the outputs produced by the decoder over a range. The code below does this (you might have to adjust it slightly depending on how you've implemented your autoencoder):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePHQZ0SVHZiG"
      },
      "outputs": [],
      "source": [
        "def create_grid_of_latents(\n",
        "    model, interpolation_range=(-1, 1), n_points=11, dims=(0, 1)\n",
        ") -> Float[Tensor, \"rows_x_cols latent_dims\"]:\n",
        "    \"\"\"Create a tensor of zeros which varies along the 2 specified dimensions of the latent space.\"\"\"\n",
        "    grid_latent = t.zeros(n_points, n_points, model.latent_dim_size, device=device)\n",
        "    x = t.linspace(*interpolation_range, n_points)\n",
        "    grid_latent[..., dims[0]] = x.unsqueeze(-1)  # rows vary over dim=0\n",
        "    grid_latent[..., dims[1]] = x  # cols vary over dim=1\n",
        "    return grid_latent.flatten(0, 1)  # flatten over (rows, cols) into a single batch dimension\n",
        "\n",
        "\n",
        "grid_latent = create_grid_of_latents(autoencoder, interpolation_range=(-3, 3))\n",
        "\n",
        "# Map grid latent through the decoder (note we need to flatten (rows, cols) into a single batch dim)\n",
        "output = autoencoder.decoder(grid_latent)\n",
        "\n",
        "# Visualize the output\n",
        "utils.visualise_output(output, grid_latent, title=\"Autoencoder latent space visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT_A8g4cHZiG"
      },
      "source": [
        "This code generates images from a vector in the latent space which is zero in all directions, except for the first two dimensions. (Note, we normalize with `(0.3081, 0.1307)` in the code above because this is the mean and standard deviation of the MNIST dataset - see discussion [here](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457).)\n",
        "\n",
        "This is ... pretty underwhelming. Some of these shapes seem legible in some regions (e.g. in the demo example on Streamlit we have some patchy but still recognisable 9s, 3s, 0s and 4s in the corners of the latent space), but a lot of the space doesn't look like any recognisable number. For example, much of the middle region is just an unrecognisable blob. Using the default interpolation range of `(-1, 1)` makes things look even worse. And this is a problem, since our goal is to be able to randomly sample points inside this latent space and use them to generate output which resembles the MNIST dataset - this is our true goal, not accurate reconstruction.\n",
        "\n",
        "Why does this happen? Well unfortunately, the model has no reason to treat the latent space in any meaningful way. It might be the case that almost all the images are embedded into a particular subspace of the latent space, and so the encoder only gets trained on inputs in this subspace. To further illustrate this, the code below feeds MNIST data into your encoder, and plots the resulting latent vectors (projected along the first two latent dimensions). Note that there are some very high-density spots, and other much lower-density spots. So it stands to reason that we shouldn't expect the decoder to be able to produce good output for all points in the latent space (especially when we're using a 5-dimensional latent space rather than just 2-dimensional as visualised below - we can imagine that 5D latent space would have significantly more \"dead space\").\n",
        "\n",
        "To emphasise, we're not looking for a crisp separation of digits here. We're only plotting 2 of 5 dimensions, it would be a coincidence if they were cleanly separated. We're looking for efficient use of the space, because this is likely to lead to an effective generator when taken out of the context of the discriminator. We don't really see that here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C45u-RkHZiG"
      },
      "outputs": [],
      "source": [
        "# Get a small dataset with 5000 points\n",
        "small_dataset = Subset(get_dataset(\"MNIST\"), indices=range(0, 5000))\n",
        "imgs = t.stack([img for img, label in small_dataset]).to(device)\n",
        "labels = t.tensor([label for img, label in small_dataset]).to(device).int()\n",
        "\n",
        "# Get the latent vectors for this data along first 2 dims, plus for the holdout data\n",
        "latent_vectors = autoencoder.encoder(imgs)[:, :2]\n",
        "holdout_latent_vectors = autoencoder.encoder(HOLDOUT_DATA)[:, :2]\n",
        "\n",
        "# Plot the results\n",
        "utils.visualise_input(latent_vectors, labels, holdout_latent_vectors, HOLDOUT_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO3q1vrjHZiH"
      },
      "source": [
        "## Variational Autoencoders\n",
        "\n",
        "Variational autoencoders try and solve the problem posed by autoencoders: how to actually make the latent space meaningful, such that you can generate output by feeding a $N(0, 1)$ random vector into your model's decoder?\n",
        "\n",
        "The key perspective shift is this: **rather than mapping the input into a fixed vector, we map it into a distribution**. The way we learn a distribution is very similar to the way we learn our fixed inputs for the autoencoder, i.e. we have a bunch of linear or convolutional layers, our input is the original image, and our output is the tuple of parameters $(\\mu(\\boldsymbol{x}), \\Sigma(\\boldsymbol{x}))$ (as a trivial example, our VAE learning a distribution $\\mu(\\boldsymbol{x})=z(\\boldsymbol{x})$, $\\Sigma(\\boldsymbol{x})=0$ is equivalent to our autoencoder learning the function $z(\\boldsymbol{x})$ as its encoder).\n",
        "\n",
        "From this [TowardsDataScience](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73) article:\n",
        "\n",
        "> Due to overfitting, the latent space of an autoencoder can be extremely irregular (close points in latent space can give very *different* decoded data, some point of the latent space can give *meaningless* content once decoded) and, so, we can’t really define a *generative* process that simply consists to sample a point from the *latent space* and make it go through the decoder to get new data. *Variational autoencoders* (VAEs) are autoencoders that tackle the problem of the latent space irregularity by making the encoder return a *distribution over the latent space* instead of a single point and by adding in the loss function a *regularisation* term over that returned distribution in order to ensure a better *organisation* of the latent space.\n",
        "\n",
        "Or, in fewer words:\n",
        "\n",
        "> **A variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.**\n",
        "\n",
        "At first, this idea of mapping to a distribution sounds like a crazy hack - why on earth does it work? This diagram should help convey some of the intuitions:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/vae-scatter-one.png\" width=\"800\">\n",
        "\n",
        "With our encoder, there was nothing incentivising us to make full and meaningful use of the latent space. It's hypothetically possible that our network was mapping all the inputs to some very small subspace and reconstructing them with perfect fidelity. This wouldn't have required numbers with different features to be far apart from each other in the latent space, because even if they are close together no information is lost. See the first image above.\n",
        "\n",
        "But with our variational autoencoder, each MNIST image produces a **sample** from the latent space, with a certain mean and variance. This means that, when two numbers look very different, their latent vectors are forced apart - if the means were close together then the decoder wouldn't be able to reconstruct them.\n",
        "\n",
        "Another nice property of using random latent vectors is that the entire latent space will be meaningful. For instance, with autoencoders there is no reason why we should expect the linear interpolation between two points in the latent space to have meaningful decodings. The decoder output *will* change continuously as we continuously vary the latent vector, but that's about all we can say about it. However, if we use a variational autoencoder, we don't have this problem. The output of a linear interpolation between the cluster of $2$s and cluster of $7$s will be ***\"a symbol which pattern-matches to the family of MNIST digits, but has equal probability to be interpreted as a $2$ or a $7$\"***, and this is indeed what we find.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/vae-scatter-two.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76AJlrkJHZiH"
      },
      "source": [
        "### Reparameterisation trick\n",
        "\n",
        "One question that might have occurred to you - how can we perform backward passes through our network? We know how to differentiate with respect to the inputs to a function, but how can we differentiate wrt the parameters of a probability distribution from which we sample our vector? The solution is to convert our random sampling into a function, by introducing an extra parameter $\\epsilon$. We sample $\\epsilon$ from the standard normal distribution, and then express $\\boldsymbol{z}$ as a deterministic function of $\\mu$, $\\sigma$ and $\\epsilon$:\n",
        "\n",
        "$$\n",
        "z = \\mu + \\sigma \\odot \\epsilon\n",
        "$$\n",
        "\n",
        "where $\\odot$ is a notation meaning pointwise product, i.e. $z_i = \\mu_i + \\sigma_i \\epsilon_i$. Intuitively, we can think of this as follows: when there is randomness in the process that generates the output, there is also randomness in the derivative of the output wrt the input, so **we can get a value for the derivative by sampling from this random distribution**. If we average over enough samples, this will give us a valid gradient for training.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/vae-reparam-l.png\" width=\"800\">\n",
        "\n",
        "Note that if we have $\\sigma_\\theta(\\boldsymbol{x})=0$ for all $\\boldsymbol{x}$, the VAE reduces to an autoencoder (since the latent vector $z = \\mu_\\theta(\\boldsymbol{x})$ is once again a deterministic function of $\\boldsymbol{x}$). This is why it's important to add a KL-divergence term to the loss function, to make sure this doesn't happen. It's also why, if you print out the average value of $\\sigma(\\boldsymbol{x})$ while training, you'll probably see it stay below 1 (it's being pulled towards 1 by the KL-divergence loss, **and** pulled towards 0 by the reconstruction loss).\n",
        "\n",
        "---\n",
        "\n",
        "Before you move on to implementation details, there are a few questions below designed to test your understanding. They are based on material from this section, as well as the [KL divergence LessWrong post](https://www.lesswrong.com/posts/no5jDTut5Byjqb4j5/six-and-a-half-intuitions-for-kl-divergence). You might also find [this post](https://lilianweng.github.io/posts/2018-08-12-vae/#vae-variational-autoencoder) on VAEs from the readings helpful.\n",
        "\n",
        "<details>\n",
        "<summary>State in your own words why we need the reparameterization trick in order to train our network.</summary>\n",
        "\n",
        "The following would work:\n",
        "\n",
        "We can't backpropagate through random processes like $z_i \\sim N(\\mu_i(\\boldsymbol{x}), \\sigma_i(\\boldsymbol{x})^2)$, but if we instead write $\\boldsymbol{z}$ as a deterministic function of $\\mu_i(\\boldsymbol{x})$ and $\\sigma_i(\\boldsymbol{x})$ with the randomness coming from some some auxiliary random variable $\\epsilon$, then we can differentiate our loss function wrt the inputs, and train our network.\n",
        "\n",
        "<!-- Our encoder works by generating parameters and then using those parameters to sample latent vectors $\\boldsymbol{z}$ (i.e. a **stochastic process**). Our decoder is deterministic; it just maps our latent vectors $\\boldsymbol{z}$ to fixed outputs $x'$. The stochastic part is the problem; we can't backpropagate gradients through random functions. However, instead of just writing $\\boldsymbol{z} \\sim N(\\mu_\\theta(\\boldsymbol{x}), \\sigma_\\theta(\\boldsymbol{x})^2I)$, we can write $\\boldsymbol{z}$ as a deterministic function of its inputs: $z = g(\\theta, x, \\epsilon)$, where $\\theta$ are the parameters of the distribution, $\\boldsymbol{x}$ is the input, and $\\epsilon$ is a randomly sampled value. We can then backpropagate through the network. -->\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Summarize in one sentence what concept we're capturing when we measure the KL divergence D(P||Q) between two distributions.</summary>\n",
        "\n",
        "Any of the following would work - $D(P||Q)$ is...\n",
        "\n",
        "* How much information is lost if the distribution $Q$ is used to represent $P$.\n",
        "* The quality of $Q$ as a probabilistic model for $P$ (where lower means $Q$ is a better model).\n",
        "* How close $P$ and $Q$ are, with $P$ as the actual ground truth.\n",
        "* How much evidence you should expect to get for hypothesis $P$ over $Q$, when $P$ is the actual ground truth.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrwzxKR-HZiH"
      },
      "source": [
        "## Building a VAE\n",
        "\n",
        "For your final exercise of this section, you'll build a VAE and run it to produce the same kind of output you did in the previous section. Luckily, this won't require much tweaking from your encoder architecture. The decoder can stay unchanged; there are just two big changes you'll need to make:\n",
        "\n",
        "### Probabilistic encoder\n",
        "\n",
        "Rather than your encode outputting a latent vector $\\boldsymbol{z}$, it should output a mean $\\mu$ and standard deviation $\\sigma$; both vectors of dimension `latent_dim_size`. We then sample our latent vector $\\boldsymbol{z}$ using $z_i = \\mu_i + \\sigma_i \\cdot \\epsilon_i$. Note that this is equivalent to $z = \\mu + \\Sigma \\epsilon$ as shown in the diagram above, but where we assume $\\Sigma$ is a diagonal matrix (i.e. the auxiliary random variables $\\epsilon$ which we're sampling are independent). This is the most common approach taken in situations like these.\n",
        "\n",
        "Note - we actually calculate `mu` and `logsigma` rather than `mu` and `sigma` - we get `sigma = logsigma.exp()` from this. This is a more numerically stable method.\n",
        "\n",
        "How exactly should this work in your model's architecture? You can replace the final linear layer (which previously just gave you the latent vector) with two linear layers returning `mu` and `logsigma`, then you calculate `z` from these (and from a randomly sampled `epsilon`). If you want, you can combine the two linear layers into one linear layer with `out_channels=2*latent_dim_size`, then rearrange to split this up into `mu` and `logsigma` (this is what we do in the solution, and in the diagram below).\n",
        "\n",
        "You should also return the parameters `mu` and `logsigma` in your VAE's forward function, as well as the final output - the reasons for this will become clear below.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/ae-before-after-fixed.png\" width=\"750\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_COL57rHZiH"
      },
      "source": [
        "### Exercise - build your VAE\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 15-25 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Build your VAE. It should be identical to the autoencoder you built above, except for the changes made at the end of the encoder (outputting mean and std rather than a single latent vector; this latent vector needs to get generated via the reparameterisation trick).\n",
        "\n",
        "For consistency with code that will come later, we recommend having your `model.encoder` output be of shape `(2, batch_size, latent_dim_size)`, where `output[0]` are the mean vectors $\\mu$ and `output[1]` are the log standard deviation vectors $\\log \\sigma$. The tests below will check this.\n",
        "\n",
        "We've also given you a `sample_latent_vector` method - this should return the output of your encoder, but in the form of sampled latent vectors $\\mu$ and $\\sigma$ rather than the deterministic output `model.encoder(x)` of shape `(2, batch_size, latent_dim_size)`. This will be a useful method for later (and you can use it in your implementation of `forward`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "035bv8-9HZiH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class VAE(nn.Module):\n",
        "    encoder: nn.Module\n",
        "    decoder: nn.Module\n",
        "\n",
        "    def __init__(self, latent_dim_size: int, hidden_dim_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent, self.hidden = latent_dim_size, hidden_dim_size\n",
        "        c2l = Rearrange('b c h w -> b (c h w)')\n",
        "        l2c = Rearrange('b (c h w)-> b c h w', c = 32, h=7, w=7)\n",
        "        split_v = Rearrange('b (n d) -> n b d', d = self.latent, n =2)\n",
        "        self.encoder = Sequential(\n",
        "            #convolutional block\n",
        "            Conv2d(in_channels=1, out_channels=16,kernel_size=4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(num_features=16),\n",
        "            Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(num_features=32),\n",
        "            c2l,\n",
        "            Linear(in_features=32*7*7, out_features=self.hidden),\n",
        "            ReLU(),\n",
        "            Linear(in_features= self.hidden, out_features=2*self.latent),\n",
        "            split_v\n",
        "        )\n",
        "        self.decoder = Sequential(\n",
        "            Linear(self.latent, self.hidden),\n",
        "            ReLU(),\n",
        "            Linear(self.hidden, 32*7*7),\n",
        "            ReLU(),\n",
        "            l2c,\n",
        "            ConvTranspose2d(32, 16, 4, 2, 1),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "            ConvTranspose2d(16, 1, 4, 2, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def sample_latent_vector(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Passes `x` through the encoder, and returns a tuple of (sampled latent vector, mean, log std dev).\n",
        "        This function can be used in `forward`, but also used on its own to generate samples for\n",
        "        evaluation.\n",
        "        \"\"\"\n",
        "        e = self.encoder(x)\n",
        "        mu, logsigma = e[0], e[1]\n",
        "        epsilon = t.randn_like(logsigma)\n",
        "        z = mu + logsigma.exp() * epsilon\n",
        "        return (z, mu, logsigma)\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Passes `x` through the encoder and decoder. Returns the reconstructed input, as well as mu and logsigma.\n",
        "        \"\"\"\n",
        "        z, mu, logsigma = self.sample_latent_vector(x)\n",
        "        dx = self.decoder(z)\n",
        "        return dx, mu, logsigma\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_vae(VAE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjIGDCiSHZiI"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class VAE(nn.Module):\n",
        "    encoder: nn.Module\n",
        "    decoder: nn.Module\n",
        "\n",
        "    def __init__(self, latent_dim_size: int, hidden_dim_size: int):\n",
        "        super().__init__()\n",
        "        self.latent_dim_size = latent_dim_size\n",
        "        self.hidden_dim_size = hidden_dim_size\n",
        "        self.encoder = Sequential(\n",
        "            Conv2d(1, 16, 4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            Conv2d(16, 32, 4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            Rearrange(\"b c h w -> b (c h w)\"),\n",
        "            Linear(7 * 7 * 32, hidden_dim_size),\n",
        "            ReLU(),\n",
        "            Linear(hidden_dim_size, latent_dim_size * 2),\n",
        "            Rearrange(\"b (n latent_dim) -> n b latent_dim\", n=2),  # makes it easier to separate mu and sigma\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            Linear(latent_dim_size, hidden_dim_size),\n",
        "            ReLU(),\n",
        "            Linear(hidden_dim_size, 7 * 7 * 32),\n",
        "            ReLU(),\n",
        "            Rearrange(\"b (c h w) -> b c h w\", c=32, h=7, w=7),\n",
        "            ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            ConvTranspose2d(16, 1, 4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "    def sample_latent_vector(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Passes `x` through the encoder, and returns a tuple of (sampled latent vector, mean, log std dev).\n",
        "        This function can be used in `forward`, but also used on its own to generate samples for\n",
        "        evaluation.\n",
        "        \"\"\"\n",
        "        mu, logsigma = self.encoder(x)\n",
        "        sigma = t.exp(logsigma)\n",
        "        z = mu + sigma * t.randn_like(sigma)\n",
        "        return z, mu, logsigma\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Passes `x` through the encoder and decoder. Returns the reconstructed input, as well as mu and logsigma.\n",
        "        \"\"\"\n",
        "        z, mu, logsigma = self.sample_latent_vector(x)\n",
        "        x_prime = self.decoder(z)\n",
        "        return x_prime, mu, logsigma\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCQqpNfHZiI"
      },
      "source": [
        "You can also do the previous thing (compare your architecture to the solution), but this might be less informative if your model doesn't implement the 2-variables approach in the same way as the solution does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM7bnhKGHZiI"
      },
      "source": [
        "## New loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOjyjMeIHZiI"
      },
      "source": [
        "We're no longer calculating loss simply as the reconstruction error between the original input $\\boldsymbol{x}$ and our decoder's output $\\boldsymbol{x}'$. Instead, we have a new loss function. For a fixed input $\\boldsymbol{x}$, our loss is:\n",
        "\n",
        "$$\n",
        "L_{\\mathrm{VAE}}(\\boldsymbol{x}, \\boldsymbol{x}') = \\|\\boldsymbol{x} - \\boldsymbol{x}'\\|^2 + D_{\\mathrm{KL}}(N(\\mu, \\sigma^2) || N(0, 1))\n",
        "$$\n",
        "\n",
        "The first term is just the regular reconstruction loss we used for our autoencoder. The second term is the KL divergence between the generator's output distribution and the standard normal distribution, and it's designed to penalize the generator for producing latent vectors which are far from the standard normal distribution.\n",
        "\n",
        "There is a much deeper way to understand this loss function, which also connects to intuitions around diffusion models and other more complicated generative models. For more on this, see the section at the end. For now, we'll move on to the practicalities of implementing this loss function.\n",
        "\n",
        "The KL divergence of these two distributions has a closed form expression, which is given by:\n",
        "\n",
        "$$\n",
        "D_{KL}(N(\\mu, \\sigma^2) || N(0, 1)) = \\frac{\\sigma^2 + \\mu^2 - 1}{2} - \\log{\\sigma}\n",
        "$$\n",
        "\n",
        "This is why it was important to output `mu` and `logsigma` in our forward functions, so we could compute this expression! (It's easier to use `logsigma` than `sigma` when evaluating the expression above, for stability reasons).\n",
        "\n",
        "We won't ask you to derive this formula, because it requires understanding of **differential entropy** which is a topic we don't need to get into here. However, it is worth doing some sanity checks, e.g. plot some graphs and convince yourself that this expression is larger as $\\mu$ is further away from 0, or $\\sigma$ is further away from 1.\n",
        "\n",
        "<details>\n",
        "<summary>Derivation of KL divergence result</summary>\n",
        "\n",
        "If you'd like a derivation, you can find one [here](https://statproofbook.github.io/P/norm-kl), where a slightly more general form of the KL divergence between two normal distributions is derived. Our result is a special case of this, for $\\mu_2 = 0$, $\\sigma_2^2 = 1$.\n",
        "In this derivation, they use $\\langle \\cdot \\rangle_{p(x)}$ to denote the expectation of a function under the distribution $p$, i.e. $\\langle f(x) \\rangle_{p(x)} = \\mathbb{E}_{p} [ f(x) ] = \\int_{\\mathcal{X}} p(x) f(x) \\; dx$.\n",
        "A more general result for multi-variate normal distributions with different means and covariances\n",
        "can be found [here](https://mr-easy.github.io/2020-04-16-kl-divergence-between-2-gaussian-distributions/).\n",
        "</details>\n",
        "\n",
        "One can interpret this as the penalty term to make the latent space meaningful. If all the latent vectors $\\boldsymbol{z}$ you generate have each component $z_i$ normally distributed with mean 0, variance 1 (and we know they're independent because our $\\epsilon_i$ we used to generate them are independent), then there will be no gaps in your latent space where you produce weird-looking output (like we saw in our autoencoder plots from the previous section). You can try training your VAE without this term, and it might do okay at reproducing $\\boldsymbol{x}$, but it will perform much worse when it comes time to use it for generation. Again, you can quantify this by encoding some input data and projecting it onto the first two dimensions. When you include this term you should expect to see a nice regular cloud surrounding the origin, but without this term you might see some irregular patterns or blind spots:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/vae_latent_space.png\" width=\"700\">\n",
        "\n",
        "Once you've computed both of these loss functions, you should add them together and perform gradient descent on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSpuQz-XHZiI"
      },
      "source": [
        "### Beta-VAEs\n",
        "\n",
        "The Beta-VAE is a very simple extension of the VAE, with a different loss function: we multiply the KL Divergence term by a constant $\\beta$. This helps us balance the two different loss terms. Here, we've given you the default value of $\\beta = 0.1$ rather than using $\\beta = 1$ (which is implicit when we use regular VAEs rather than beta-VAEs).\n",
        "\n",
        "As a general comment on tuning hyperparameters like $\\beta$, it's important to sweep over ranges of different values and know how to tell when one of them is dominating the model's behaviour. In this particular case, $\\beta$ being too large means your model will too heavily prioritize having its latent vectors' distribution equal to the standard normal distribution, to the point where it might essentially lose all structure in the data and ignore accurate reconstruction. On the other hand, $\\beta$ being too small means we just reduce to the autoencoder case where the model has no incentive to make meaningful use of the latent space. Weights and biases hyperparameter searches are a good tool for sweeping over ranges and testing the results, but they'll only work if you log the relevant data / output and know how to interpret it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmDPu_hOHZiI"
      },
      "source": [
        "### Exercise - write your VAE training loop\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴⚪⚪\n",
        "> Importance: 🔵🔵🔵⚪⚪\n",
        ">\n",
        "> You should spend up to 15-30 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should write and run your training loop below. This will involve a lot of recycled code from the corresponding `Autoencoder` exercise - in fact, depending on how you implemented the `train` method before, you might literally be able to copy and paste that method here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQOao3fPHZiI"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class VAEArgs(AutoencoderArgs):\n",
        "    wandb_project: str | None = \"day5-vae-mnist\"\n",
        "    beta_kl: float = 0.1\n",
        "\n",
        "\n",
        "class VAETrainer:\n",
        "    def __init__(self, args: VAEArgs):\n",
        "        self.args = args\n",
        "        self.trainset = get_dataset(args.dataset)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
        "        self.model = VAE(\n",
        "            latent_dim_size=args.latent_dim_size,\n",
        "            hidden_dim_size=args.hidden_dim_size,\n",
        "        ).to(device)\n",
        "        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)\n",
        "\n",
        "    def training_step(self, img: Tensor):\n",
        "        \"\"\"\n",
        "        Performs a training step on the batch of images in `img`. Returns the loss. Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        l2 = nn.MSELoss()\n",
        "        x_prime, mu, logsigma = self.model.forward(img)\n",
        "        kl_div_loss = (0.5 * (mu**2 + t.exp(2 * logsigma) - 1) - logsigma).mean() * self.args.beta_kl\n",
        "        rec_loss = l2(img, x_prime)\n",
        "        loss = rec_loss + kl_div_loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        self.step += img.shape[0]\n",
        "\n",
        "        if args.use_wandb:\n",
        "          wandb.log(dict(loss=loss.item(),\n",
        "                         reconstructed_img_loss = rec_loss.item(),\n",
        "                         kernel_div_loss = kl_div_loss.item(),\n",
        "                         mu = mu.mean(),\n",
        "                         std=t.exp(logsigma).mean()), step=self.step)\n",
        "\n",
        "        return loss\n",
        "\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def log_samples(self) -> None:\n",
        "        \"\"\"\n",
        "        Evaluates model on holdout data, either logging to weights & biases or displaying output inline.\n",
        "        \"\"\"\n",
        "        assert self.step > 0, \"First call should come after a training step. Remember to increment `self.step`.\"\n",
        "        output = self.model(HOLDOUT_DATA)[0]\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log({\"images\": [wandb.Image(arr) for arr in output.cpu().numpy()]}, step=self.step)\n",
        "        else:\n",
        "            display_data(t.concat([HOLDOUT_DATA, output]), nrows=2, title=\"VAE reconstructions\")\n",
        "\n",
        "    def train(self) -> VAE:\n",
        "        \"\"\"Performs a full training run.\"\"\"\n",
        "        self.step = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "            wandb.watch(self.model)\n",
        "\n",
        "        # YOUR CODE HERE - iterate over epochs, and train your model\n",
        "        for epoch in range(self.args.epochs):\n",
        "\n",
        "            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)), ascii=True)\n",
        "\n",
        "            for imgs, _ in progress_bar:\n",
        "                imgs = imgs.to(device)\n",
        "                loss = self.training_step(imgs)\n",
        "                progress_bar.set_description(f\"{epoch=:02d}, {loss=:.4f}, step={self.step:05d}\")\n",
        "                # log every 250 steps\n",
        "                if self.step % self.args.log_every_n_steps == 0:\n",
        "\n",
        "                  self.log_samples()\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return self.model\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWrSnD_uHZiI"
      },
      "outputs": [],
      "source": [
        "args = VAEArgs(latent_dim_size=5, hidden_dim_size=100, use_wandb=False)\n",
        "trainer = VAETrainer(args)\n",
        "vae = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrhVBLF3HZiI"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "@dataclass\n",
        "class VAEArgs(AutoencoderArgs):\n",
        "    wandb_project: str | None = \"day5-vae-mnist\"\n",
        "    beta_kl: float = 0.1\n",
        "\n",
        "\n",
        "class VAETrainer:\n",
        "    def __init__(self, args: VAEArgs):\n",
        "        self.args = args\n",
        "        self.trainset = get_dataset(args.dataset)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
        "        self.model = VAE(\n",
        "            latent_dim_size=args.latent_dim_size,\n",
        "            hidden_dim_size=args.hidden_dim_size,\n",
        "        ).to(device)\n",
        "        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)\n",
        "\n",
        "    def training_step(self, img: Tensor):\n",
        "        \"\"\"\n",
        "        Performs a training step on the batch of images in `img`. Returns the loss. Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        # Get the different loss components, as well as the total loss\n",
        "        img = img.to(device)\n",
        "        img_reconstructed, mu, logsigma = self.model(img)\n",
        "        reconstruction_loss = nn.MSELoss()(img, img_reconstructed)\n",
        "        kl_div_loss = (0.5 * (mu**2 + t.exp(2 * logsigma) - 1) - logsigma).mean() * self.args.beta_kl\n",
        "        loss = reconstruction_loss + kl_div_loss\n",
        "\n",
        "        # Backprop on the loss, and step with optimizers\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Log various values, and also increment `self.step`\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log(\n",
        "                dict(\n",
        "                    reconstruction_loss=reconstruction_loss.item(),\n",
        "                    kl_div_loss=kl_div_loss.item(),\n",
        "                    mean=mu.mean(),\n",
        "                    std=t.exp(logsigma).mean(),\n",
        "                    total_loss=loss.item(),\n",
        "                ),\n",
        "                step=self.step,\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def log_samples(self) -> None:\n",
        "        \"\"\"\n",
        "        Evaluates model on holdout data, either logging to weights & biases or displaying output inline.\n",
        "        \"\"\"\n",
        "        assert self.step > 0, \"First call should come after a training step. Remember to increment `self.step`.\"\n",
        "        output = self.model(HOLDOUT_DATA)[0]\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log({\"images\": [wandb.Image(arr) for arr in output.cpu().numpy()]}, step=self.step)\n",
        "        else:\n",
        "            display_data(t.concat([HOLDOUT_DATA, output]), nrows=2, title=\"VAE reconstructions\")\n",
        "\n",
        "    def train(self) -> VAE:\n",
        "        \"\"\"Performs a full training run.\"\"\"\n",
        "        self.step = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "            wandb.watch(self.model)\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            # Iterate over training data, performing a training step for each batch\n",
        "            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)), ascii=True)\n",
        "            for img, label in progress_bar:  # remember that label is not used\n",
        "                img = img.to(device)\n",
        "                loss = self.training_step(img)\n",
        "                self.step += 1\n",
        "                progress_bar.set_description(f\"{epoch=:02d}, {loss=:.4f}, batches={self.step:05d}\")\n",
        "                if self.step % self.args.log_every_n_steps == 0:\n",
        "                    self.log_samples()\n",
        "\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return self.model\n",
        "\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkNbG17dHZiI"
      },
      "source": [
        "You might be disappointed when comparing your VAE's reconstruction to the autoencoder, since it's likely to be worse. However, remember that the focus of VAEs is in better generation, not reconstruction. To test it's generation, let's produce some plots of latent space like we did for the autoencoder.\n",
        "\n",
        "First, we'll visualize the latent space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7yumU55HZiI"
      },
      "outputs": [],
      "source": [
        "grid_latent = create_grid_of_latents(vae, interpolation_range=(-1, 1))\n",
        "output = vae.decoder(grid_latent)\n",
        "utils.visualise_output(output, grid_latent, title=\"VAE latent space visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jILlpeQHZiI"
      },
      "source": [
        "which should look a lot better! The vast majority of images in this region should be recognisable digits. In this case, starting from the top-left and going clockwise, we can see 0s, 6s, 5s, 1s, 9s and 8s. Even the less recognisable regions seem like they're just interpolations between digits which are recognisable (which is something we should expect to happen given our VAE's output is continuous).\n",
        "\n",
        "Now for the scatter plot to visualize our input space:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_QAehNYHZiI"
      },
      "outputs": [],
      "source": [
        "small_dataset = Subset(get_dataset(\"MNIST\"), indices=range(0, 5000))\n",
        "imgs = t.stack([img for img, label in small_dataset]).to(device)\n",
        "labels = t.tensor([label for img, label in small_dataset]).to(device).int()\n",
        "\n",
        "# We're getting the mean vector, which is the [0]-indexed output of the encoder\n",
        "latent_vectors = vae.encoder(imgs)[0, :, :2]\n",
        "holdout_latent_vectors = vae.encoder(HOLDOUT_DATA)[0, :, :2]\n",
        "\n",
        "utils.visualise_input(latent_vectors, labels, holdout_latent_vectors, HOLDOUT_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u1AQ6oqHZiI"
      },
      "source": [
        "The range of the distribution looks much more like a standard normal distribution (most points are in `[-2, 2]` whereas in the previous plot the range was closer to `[-10, 10]`). There are also no longer any large gaps in the latent space that you wouldn't find in the corresponding standard normal distribution.\n",
        "\n",
        "To emphasize - don't be disheartened if your *reconstructions of the original MNIST images* don't look as faithful for your VAE than they did for your encoder. Remember the goal of these architectures isn't to reconstruct images faithfully, it's to generate images from samples in the latent dimension. This is the basis on which you should compare your models to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6bqTqbjHZiI"
      },
      "source": [
        "## A deeper dive into the maths of VAEs\n",
        "\n",
        "If you're happy with the loss function as described in the section above, then you can move on from here. If you'd like to take a deeper dive into the mathematical justifications of this loss function, you can read the following content. I'd consider it pretty essential in laying the groundwork for understanding diffusion models, and most kinds of generative image models (which we might dip into later in this course).\n",
        "\n",
        "> *Note - this section is meant to paint more of an intuitive picture than deal with formal and rigorous mathematics, which is why we'll play fairly fast and loose with things like limit theorems and whether we can swap around integrals and expected values. This can be formalized further, but we'll leave that for other textbooks.*\n",
        "\n",
        "Firstly, let's ignore the encoder, and suppose we're just starting from a latent vector $\\boldsymbol{z} \\sim p(z) = N(0, I)$. We can parameterize a **probabilistic decoder** as $p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$, i.e. a way of mapping from latent vectors $z$ to a distribution over images $\\boldsymbol{x}$. To use it as a decoder, we just sample a latent vector then sample from the resulting probability distribution our decoder gives us. Imagine this as a factory line: the latent vector $\\boldsymbol{z}$ is some kind of compressed blueprint for our image, and our decoder is a way of reconstructing images from this information.\n",
        "\n",
        "On this factory line, the probability of producing any given image $\\boldsymbol{x}$ can be found from integrating over the possible latent vectors $\\boldsymbol{z}$ which could have produced it:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\boldsymbol{x})&=\\int_z p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z}) \\; d \\boldsymbol{z} \\\\\n",
        "&= \\mathbb{E}_{\\boldsymbol{z} \\sim p(\\boldsymbol{z})}\\big[p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})\\big]\n",
        "\\end{aligned}\n",
        "$$\n",
        "What if we estimate this value over random samples $x_i \\sim \\hat{p}(\\boldsymbol{x})$ from our dataset? We could perform gradient ascent on $\\theta$ using this estimate to find a decoder $p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$ that makes this estimate high, in other words a factory line that is *likely to produce images like the ones in our dataset*. Then we're done - right?\n",
        "\n",
        "Unfortunately, it's not that easy. Evaluating this integral would be computationally intractible, because we would have to sample over all possible values for the latent vectors $\\boldsymbol{z}$:\n",
        "$$\n",
        "\\theta^*=\\underset{\\theta}{\\operatorname{argmax}}\\; \\mathbb{E}_{x \\sim \\hat{p}(\\boldsymbol{x}),\\, \\boldsymbol{z} \\sim p(\\boldsymbol{z})}\\big[p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})\\big]\n",
        "$$\n",
        "In our ideal factory line, for any given image $\\boldsymbol{x}$ there's probably only a small cluster of latent vectors $\\boldsymbol{z}$ that could have produced it, and it's that small cluster that we should be concentrating our probability mass over. Without this, it'll take an exponentially long time to get full sampling coverage of the latent space of $\\boldsymbol{z}$.\n",
        "\n",
        "This is where our encoder function $q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})$ comes in! It helps concentrate our guess for $\\boldsymbol{z}$ for any given $\\boldsymbol{x}$, essentially telling us **which latent vectors were likely to have produced the image $\\boldsymbol{x}$**. We can now replace our original integral with the following:\n",
        "\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/vae-graphical.png\" width=\"500\"> -->\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\boldsymbol{x}) &=\\int q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})}{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})} \\;d \\boldsymbol{z}\\\\\n",
        "\\theta^*&=\\underset{\\theta}{\\operatorname{argmax}}\\; \\mathbb{E}_{\\boldsymbol{x} \\sim \\hat{p}(\\boldsymbol{x}), \\boldsymbol{z} \\sim q_\\phi(\\boldsymbol{z}\\mid \\boldsymbol{x})}\\left[\\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})p(\\boldsymbol{z})}{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right] \\\\\n",
        "&=\\underset{\\theta}{\\operatorname{argmax}}\\; \\mathbb{E}_{\\boldsymbol{x} \\sim \\hat{p}(\\boldsymbol{x})}\\left[\\mathbb{E}_{\\boldsymbol{z} \\sim q_\\phi(\\boldsymbol{z}\\mid \\boldsymbol{x})}\\left[\\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})p(\\boldsymbol{z})}{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\\right]\n",
        "\\end{aligned}\n",
        "$$\n",
        "Why is this easier? Well, now that we've rearranged it by introducing this $q_\\phi$ term, our distribution of $\\boldsymbol{z}$ is conditioned on $\\boldsymbol{x}$. So for any given sample $x_i$, we don't need to sample a huge number of latent vectors $\\boldsymbol{z}$ to estimate the inner expected value in the expression above - we can just sample from $q_\\phi(\\boldsymbol{z}\\mid \\boldsymbol{x_i})$, which already concentrates a lot of our probability mass for where $\\boldsymbol{z}$ should be.\n",
        "\n",
        "We now introduce an important quantity, called the **ELBO**, or **evidence lower-bound**. It is defined as the value inside the expectation in the expression above, but using log instead.\n",
        "$$\n",
        "\\operatorname{ELBO}(\\boldsymbol{x})=\\mathbb{E}_{\\boldsymbol{z} \\sim q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log \\frac{p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})p(\\boldsymbol{z})}{q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]\n",
        "$$\n",
        "Why do we call this the ELBO? Answer - because it's a lower bound for the quantity $\\log p(\\boldsymbol{x})$, which we call the **evidence**. The proof for this being a lower bound comes from **Jensen's inequality**, which states that $\\mathbb{E}[f(X)] \\geq f(\\mathbb{E}[X])$ for any convex function $f$ (and $f(\\boldsymbol{x})=-\\log(\\boldsymbol{x})$ is convex).\n",
        "\n",
        "<!-- In fact,  [we can prove](https://lilianweng.github.io/posts/2018-08-12-vae/#loss-function-elbo) that the difference between $\\log p(\\boldsymbol{x})$ and the ELBO is equal to the KL divergence between the distribution $q_\\phi$ and the **posterior distribution** $p_\\theta(\\boldsymbol{z} \\mid \\boldsymbol{x})$ (the order of $\\boldsymbol{z}$ and $\\boldsymbol{x}$ have been swapped). KL divergence is always non-negative, hence the lower bound. -->\n",
        "\n",
        "<!-- $$\n",
        "\\log{p(\\boldsymbol{x})}=\\mathbb{E}_{\\mathbb{z} \\sim q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log \\frac{p(\\boldsymbol{z}) p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right]+D_{\\mathrm{KL}}\\left(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\,\\|\\, p_\\theta(\\boldsymbol{z} \\mid \\boldsymbol{x})\\right)\n",
        "$$ -->\n",
        "\n",
        "It turns out that by looking at log probs rather than probabilities, we can now rewrite the ELBO as something which looks a lot like our VAE loss function! $p_\\theta(\\boldsymbol{x} \\mid \\boldsymbol{z})$ is our decoder, $q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})$ is our encoder, and we train them jointly using gradient ascent on our estimate of the ELBO. For any given sample $x_i$, the value $\\operatorname{ELBO}(x_i)$ is estimated by sampling a latent vector $z_i \\sim q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x_i})$, and then performing gradient ascent on the ELBO, which can be rewritten as:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\operatorname{ELBO}(\\boldsymbol{x}) & =\\mathbb{E}_{\\mathbb{z} \\sim q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z}) + \\log \\frac{p(\\boldsymbol{z})}{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\right] \\\\\n",
        "& =\\underbrace{\\mathbb{E}_{\\boldsymbol{z} \\sim q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})}\\left[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})\\right]}_{\\text {reconstruction loss}}-\\underbrace{D_{\\mathrm{KL}}\\left(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\,\\|\\, p(\\boldsymbol{z})\\right)}_{\\text {regularisation term}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "which if you tilt your head a bit, is just like our VAE loss function! To see this:\n",
        "\n",
        "* **The first term** is playing the role of reconstruction loss, since it's equivalent to the log probability that our image $x$ is perfectly reconstructed via the series of maps $\\boldsymbol{x} \\xrightarrow{\\text{encoder}} \\boldsymbol{z} \\xrightarrow{\\text{decoder}} \\boldsymbol{x}$. If we had perfect reconstruction then this value would be zero (which is its maximum value).\n",
        "* **The second term** is the KL divergence between $q_\\phi(\\boldsymbol{z} \\mid \\boldsymbol{x})$ and $p_{\\theta}(\\boldsymbol{z})$. The former is your own encoder's distribution of latent vectors which you may recall is $N(\\mu(\\boldsymbol{x}), \\sigma(\\boldsymbol{x})^2)$, and the latter was assumed to be the uniform distribution $N(0, I)$. This just reduces to our KL penalty term from earlier.\n",
        "\n",
        "The decoder used in our VAE isn't actually probabilistic $p_\\theta(\\cdot \\mid \\boldsymbol{z})$, it's deterministic (i.e. it's a map from latent vector $\\boldsymbol{z}$ to reconstructed input $\\boldsymbol{x}'$). But we can pretend that the decoder output is actually the mean of a probability distribution, and we're choosing this mean as the value of our reconstruction $\\boldsymbol{x}'$. The reconstruction loss term in the formula above will be smallest when this mean is close to the original value $\\boldsymbol{x}$ (because then $p_\\theta(\\cdot \\mid \\boldsymbol{z})$ will be a probability distribution centered around $\\boldsymbol{x}$). And it turns out that we can just replace this reconstruction loss with something that fulfils basically the same purpose (the $L_2$ penalty) - although we sometimes need to adjust these two terms (see $\\beta$-VAEs above).\n",
        "\n",
        "And that's the math of VAEs in a nutshell!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-jC0YNfHZiJ"
      },
      "source": [
        "## Bonus exercises\n",
        "\n",
        "### PCA\n",
        "\n",
        "In the code earlier, we visualised our autoencoder / VAE output along the first two dimensions of the latent space. If each dimension is perfectly IID then we should expect this to get similar results to varying along any two arbitrarily chosen orthogonal directions. However, in practice you might find it an improvement to choose directions in a more principled way. One way to do this is to use **principal component analysis** (PCA). Can you write code to extract the PCA components from your model's latent space, and plot the data along these components?\n",
        "\n",
        "<details>\n",
        "<summary>Template (code for extracting PCA from the Autoencoder)</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "@t.inference_mode()\n",
        "def get_pca_components(\n",
        "    model: Autoencoder,\n",
        "    dataset: Dataset,\n",
        ") -> tuple[t.Tensor, t.Tensor]:\n",
        "    '''\n",
        "    Gets the first 2 principal components in latent space, from the data.\n",
        "\n",
        "    Returns:\n",
        "        pca_vectors: shape (2, latent_dim_size)\n",
        "            the first 2 principal component vectors in latent space\n",
        "        principal_components: shape (batch_size, 2)\n",
        "            components of data along the first 2 principal components\n",
        "    '''\n",
        "    # Unpack the (small) dataset into a single batch\n",
        "    imgs = t.stack([batch[0] for batch in dataset]).to(device)\n",
        "    labels = t.tensor([batch[1] for batch in dataset])\n",
        "\n",
        "    # Get the latent vectors\n",
        "    latent_vectors = model.encoder(imgs.to(device)).cpu().numpy()\n",
        "    if latent_vectors.ndim == 3: latent_vectors = latent_vectors[0] # useful for VAEs; see later\n",
        "\n",
        "    # Perform PCA, to get the principle component directions (& projections of data in these directions)\n",
        "    pca = PCA(n_components=2)\n",
        "    principal_components = pca.fit_transform(latent_vectors)\n",
        "    pca_vectors = pca.components_\n",
        "    return (\n",
        "        t.from_numpy(pca_vectors).float(),\n",
        "        t.from_numpy(principal_components).float(),\n",
        "    )\n",
        "```\n",
        "\n",
        "And then you can use this function in your `visualise_output` by replacing the code at the start with this:\n",
        "\n",
        "```python\n",
        "pca_vectors, principal_components = get_pca_components(model, dataset)\n",
        "\n",
        "# Constructing latent dim data by making two of the dimensions vary independently in the interpolation range\n",
        "x = t.linspace(*interpolation_range, n_points)\n",
        "grid_latent = t.stack([\n",
        "    einops.repeat(x, \"dim1 -> dim1 dim2\", dim2=n_points),\n",
        "    einops.repeat(x, \"dim2 -> dim1 dim2\", dim1=n_points),\n",
        "], dim=-1)\n",
        "# Map grid to the basis of the PCA components\n",
        "grid_latent = grid_latent @ pca_vectors\n",
        "```\n",
        "\n",
        "Note that this will require adding `dataset` to the arguments of this function.\n",
        "\n",
        "\n",
        "You can do something similar for the `visualise_input` function:\n",
        "\n",
        "```python\n",
        "@t.inference_mode()\n",
        "def visualise_input(\n",
        "    model: Autoencoder,\n",
        "    dataset: Dataset,\n",
        ") -> None:\n",
        "    '''\n",
        "    Visualises (in the form of a scatter plot) the input data in the latent space, along the first two latent dims.\n",
        "    '''\n",
        "    # First get the model images' latent vectors, along first 2 dims\n",
        "    imgs = t.stack([batch for batch, label in dataset]).to(device)\n",
        "    latent_vectors = model.encoder(imgs)\n",
        "    if latent_vectors.ndim == 3: latent_vectors = latent_vectors[0] # useful for VAEs later\n",
        "    latent_vectors = latent_vectors[:, :2].cpu().numpy()\n",
        "    labels = [str(label) for img, label in dataset]\n",
        "\n",
        "    # Make a dataframe for scatter (px.scatter is more convenient to use when supplied with a dataframe)\n",
        "    df = pd.DataFrame({\"dim1\": latent_vectors[:, 0], \"dim2\": latent_vectors[:, 1], \"label\": labels})\n",
        "    df = df.sort_values(by=\"label\")\n",
        "    fig = px.scatter(df, x=\"dim1\", y=\"dim2\", color=\"label\")\n",
        "    fig.update_layout(height=700, width=700, title=\"Scatter plot of latent space dims\", legend_title=\"Digit\")\n",
        "    data_range = df[\"dim1\"].max() - df[\"dim1\"].min()\n",
        "\n",
        "    # Add images to the scatter plot (optional)\n",
        "    output_on_data_to_plot = model.encoder(HOLDOUT_DATA.to(device))\n",
        "    if output_on_data_to_plot.ndim == 3: output_on_data_to_plot = output_on_data_to_plot[0] # useful for VAEs later\n",
        "    output_on_data_to_plot = output_on_data_to_plot[:, :2].cpu()\n",
        "    data_translated = (HOLDOUT_DATA.cpu().numpy() * 0.3081) + 0.1307\n",
        "    data_translated = (255 * data_translated).astype(np.uint8).squeeze()\n",
        "    for i in range(10):\n",
        "        x, y = output_on_data_to_plot[i]\n",
        "        fig.add_layout_image(\n",
        "            source=Image.fromarray(data_translated[i]).convert(\"L\"),\n",
        "            xref=\"x\", yref=\"y\",\n",
        "            x=x, y=y,\n",
        "            xanchor=\"right\", yanchor=\"top\",\n",
        "            sizex=data_range/15, sizey=data_range/15,\n",
        "        )\n",
        "    fig.show()\n",
        "```\n",
        "    \n",
        "</details>\n",
        "\n",
        "### Beta-VAEs\n",
        "\n",
        "Read the section on [Beta-VAEs](https://lilianweng.github.io/posts/2018-08-12-vae/#beta-vae), if you haven't already. Can you choose a better value for $\\beta$?\n",
        "\n",
        "To decide on an appropriate $\\beta$, you can look at the distribution of your latent vector. For instance, if your latent vector looks very different to the standard normal distribution when it's projected onto one of its components (e.g. maybe that component is very sharply spiked around some particular value), this is a sign that you need to use a larger parameter $\\beta$. You can also just use hyperparameter searches to find an optimal $\\beta$. See [the paper](https://openreview.net/pdf?id=Sy2fzU9gl) which introduced Beta-VAEs for more ideas.\n",
        "\n",
        "### CelebA dataset\n",
        "\n",
        "Try to build an autoencoder for the CelebA dataset. You shouldn't need to change the architecture much from your MNIST VAE. You should find the training much easier than with your GAN (as discussed yesterday, GANs are notoriously unstable when it comes to training). Can you get better results than you did for your GAN?\n",
        "\n",
        "### Hierarchical VAEs\n",
        "\n",
        "Hierarchical VAEs are ones which stack multiple layers of parameter-learning and latent-vector-sampling, rather than just doing this once. Read the section of [this paper](https://arxiv.org/pdf/2208.11970.pdf) for a more thorough description.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/vae-and-hvae-final.png\" width=\"1100\">\n",
        "\n",
        "(Note - the KL divergence loss used in HVAEs can sometimes be more complicated than the one presented in this diagram, if you want to implement conditional dependencies between the different layers. However, this isn't necessary for the basic HVAE architecture.)\n",
        "\n",
        "Try to implement your own hierarchical VAE.\n",
        "\n",
        "Note - when you get to the material on **diffusion models** later in the course, you might want to return here, because understanding HVAEs can be a useful step to understanding diffusion models. In fact diffusion models can almost be thought of as a special type of HVAE.\n",
        "\n",
        "### Denoising and sparse autoencoders\n",
        "\n",
        "The reading material on VAEs talks about [denoising](https://lilianweng.github.io/posts/2018-08-12-vae/#denoising-autoencoder) and [sparse](https://lilianweng.github.io/posts/2018-08-12-vae/#sparse-autoencoder) autoencoders. Try changing the architecture of your autoencoder (not your VAE) to test out one of these two techniques. Do does your decoder output change? How about your encoder scatter plot?\n",
        "\n",
        "***Note - sparse autoencoders will play an important role in some later sections of this course (when we study superposition in mechanistic interpretability).***\n",
        "\n",
        "If you're mathematically confident and feeling like a challenge, you can also try to implement [contractive autoencoders](https://lilianweng.github.io/posts/2018-08-12-vae/#contractive-autoencoder)!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HVAES model:\n",
        "\n",
        "class HVAE(nn.Module):\n",
        "    encoder: nn.Module\n",
        "    decoder: nn.Module\n",
        "\n",
        "    def __init__(self, latent_dim_size: int, hidden_dim_size: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.latent, self.hidden = latent_dim_size, hidden_dim_size\n",
        "        c2l = Rearrange('b c h w -> b (c h w)')\n",
        "        l2c = Rearrange('b (c h w)-> b c h w', c = 32, h=7, w=7)\n",
        "        split_v = Rearrange('b (n d) -> n b d', d = self.latent, n =2)\n",
        "        self.encoder = Sequential(\n",
        "            #convolutional block\n",
        "            Conv2d(in_channels=1, out_channels=16,kernel_size=4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(num_features=16),\n",
        "            Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
        "            ReLU(),\n",
        "            BatchNorm2d(num_features=32),\n",
        "            c2l,\n",
        "            Linear(in_features=32*7*7, out_features=self.hidden),\n",
        "            ReLU(),\n",
        "            Linear(in_features= self.hidden, out_features=2*self.latent),\n",
        "            split_v\n",
        "\n",
        "        )\n",
        "        self.encoder2 = Sequential(\n",
        "            ReLU(),\n",
        "            Linear(in_features=self.latent, out_features=2*self.latent),\n",
        "            split_v\n",
        "        )\n",
        "        self.decoder1 = Sequential(\n",
        "            Linear(self.latent, self.latent),\n",
        "            ReLU()\n",
        "        )\n",
        "        self.decoder = Sequential(\n",
        "            Linear(self.latent, self.hidden),\n",
        "            ReLU(),\n",
        "            Linear(self.hidden, 32*7*7),\n",
        "            ReLU(),\n",
        "            l2c,\n",
        "            ConvTranspose2d(32, 16, 4, 2, 1),\n",
        "            BatchNorm2d(16),\n",
        "            ReLU(),\n",
        "            ConvTranspose2d(16, 1, 4, 2, 1)\n",
        "\n",
        "        )\n",
        "\n",
        "    def sample_latent_vector(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Passes `x` through the encoder, and returns a tuple of (sampled latent vector, mean, log std dev).\n",
        "        This function can be used in `forward`, but also used on its own to generate samples for\n",
        "        evaluation.\n",
        "        \"\"\"\n",
        "        # we generate the latent rep:\n",
        "        e1 = self.encoder(x)\n",
        "\n",
        "        mu1, logsigma1 = e1[0], e1[1]\n",
        "        epsilon1 = t.randn_like(logsigma1)\n",
        "        z1 = mu1 + logsigma1.exp() * epsilon1\n",
        "\n",
        "        # we compute z2:\n",
        "        e2 = self.encoder2(z1)\n",
        "        mu2, logsigma2 = e2[0] , e2[1]\n",
        "        epsilon2 = t.randn_like(logsigma2)\n",
        "        z2 = mu2 + logsigma2.exp() * epsilon2\n",
        "\n",
        "        return (z1, mu1, logsigma1), (z2, mu2, logsigma2)\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Passes `x` through the encoder and decoder. Returns the reconstructed input, as well as mu and logsigma.\n",
        "        \"\"\"\n",
        "        first_lat, second_lat = self.sample_latent_vector(x)\n",
        "        z1, mu1, logsigma1 = first_lat\n",
        "        z2, mu2, logsigma2 = second_lat\n",
        "\n",
        "        dz1 = self.decoder1(z2)\n",
        "\n",
        "        dx = self.decoder(dz1)\n",
        "        return dx, (mu2, logsigma2), (mu1, logsigma1) # return values for loss\n",
        "        #raise NotImplementedError()"
      ],
      "metadata": {
        "id": "Wy-U08rHI22u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try a single forward pass\n",
        "trainset = get_dataset('MNIST')\n",
        "trainloader = DataLoader(batch_size= 256, dataset=trainset, shuffle=True )\n",
        "hvae = HVAE(5, 100)\n",
        "for img, label in trainloader:\n",
        "    img.to(device)\n",
        "    img_, p2, p1 = hvae.forward(img)\n",
        "    print(len(p1))\n",
        "    mu, log = p2\n",
        "    break"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-CThuSx5N66F",
        "outputId": "4e0368d8-9e9d-429d-80d1-1f489113a9b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256, 5])\n",
            "torch.Size([256, 5])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AutoencoderArgs:\n",
        "    # architecture\n",
        "    latent_dim_size: int = 5\n",
        "    hidden_dim_size: int = 128\n",
        "\n",
        "    # data / training\n",
        "    dataset: Literal[\"MNIST\", \"CELEB\"] = \"MNIST\"\n",
        "    batch_size: int = 512\n",
        "    epochs: int = 10\n",
        "    lr: float = 1e-3\n",
        "    betas: tuple[float, float] = (0.5, 0.999)\n",
        "\n",
        "    # logging\n",
        "    use_wandb: bool = False\n",
        "    wandb_project: str | None = \"day5-autoencoder\"\n",
        "    wandb_name: str | None = None\n",
        "    log_every_n_steps: int = 250"
      ],
      "metadata": {
        "id": "ZodiP5qASOsf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop :\n",
        "@dataclass\n",
        "class HVAEArgs(AutoencoderArgs):\n",
        "    wandb_project: str | None = \"day5-vae-mnist\"\n",
        "    beta_kl: float = 0.1\n",
        "\n",
        "\n",
        "class HVAETrainer:\n",
        "    def __init__(self, args: HVAEArgs):\n",
        "        self.args = args\n",
        "        self.trainset = get_dataset(args.dataset)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
        "        self.model = HVAE(\n",
        "            latent_dim_size=args.latent_dim_size,\n",
        "            hidden_dim_size=args.hidden_dim_size,\n",
        "        ).to(device)\n",
        "        self.optimizer = t.optim.Adam(self.model.parameters(), lr=args.lr, betas=args.betas)\n",
        "\n",
        "    def training_step(self, img: Tensor):\n",
        "        \"\"\"\n",
        "        Performs a training step on the batch of images in `img`. Returns the loss. Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        l2 = nn.MSELoss()\n",
        "        x_prime, sec_par, first_par = self.model.forward(img)\n",
        "\n",
        "        # unpack them\n",
        "        mu2, logsigma2 = sec_par\n",
        "        mu1, logsigma1 = first_par\n",
        "\n",
        "        kl_div_loss1 = -0.5 * t.sum(1 + 2 * logsigma1 - mu1.pow(2) - (2 * logsigma1).exp(), dim=1).mean()\n",
        "        kl_div_loss2 = -0.5 * t.sum(1 + 2 * logsigma2 - mu2.pow(2) - (2 * logsigma2).exp(), dim=1).mean()\n",
        "        kl_div_loss = kl_div_loss1 + kl_div_loss2\n",
        "\n",
        "        rec_loss = l2(img, x_prime)\n",
        "        loss = rec_loss + (kl_div_loss * self.args.beta_kl)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        self.step += img.shape[0]\n",
        "\n",
        "        if args.use_wandb:\n",
        "          wandb.log(dict(loss=loss.item(),\n",
        "                         reconstructed_img_loss = rec_loss.item(),\n",
        "                         kernel_div_loss = kl_div_loss.item(),\n",
        "                         mu1 = mu1.mean(),\n",
        "                         std1=t.exp(logsigma1).mean()), step=self.step)\n",
        "\n",
        "        return loss\n",
        "\n",
        "        #raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def log_samples(self) -> None:\n",
        "        \"\"\"\n",
        "        Evaluates model on holdout data, either logging to weights & biases or displaying output inline.\n",
        "        \"\"\"\n",
        "        assert self.step > 0, \"First call should come after a training step. Remember to increment `self.step`.\"\n",
        "        output = self.model(HOLDOUT_DATA)[0]\n",
        "        if self.args.use_wandb:\n",
        "            wandb.log({\"images\": [wandb.Image(arr) for arr in output.cpu().numpy()]}, step=self.step)\n",
        "        else:\n",
        "            display_data(t.concat([HOLDOUT_DATA, output]), nrows=2, title=\"VAE reconstructions\")\n",
        "\n",
        "    def train(self) -> VAE:\n",
        "        \"\"\"Performs a full training run.\"\"\"\n",
        "        self.step = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "            wandb.watch(self.model)\n",
        "\n",
        "        # YOUR CODE HERE - iterate over epochs, and train your model\n",
        "        for epoch in range(self.args.epochs):\n",
        "\n",
        "            progress_bar = tqdm(self.trainloader, total=int(len(self.trainloader)), ascii=True)\n",
        "\n",
        "            for imgs, _ in progress_bar:\n",
        "                imgs = imgs.to(device)\n",
        "                loss = self.training_step(imgs)\n",
        "                progress_bar.set_description(f\"{epoch=:02d}, {loss=:.4f}, step={self.step:05d}\")\n",
        "                # log every 250 steps\n",
        "                if self.step % self.args.log_every_n_steps == 0:\n",
        "\n",
        "                  self.log_samples()\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return self.model"
      ],
      "metadata": {
        "id": "fOy7-MVyJHtq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = HVAEArgs(latent_dim_size=5, hidden_dim_size=100, use_wandb=False)\n",
        "trainer = HVAETrainer(args)\n",
        "vae = trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "knPEqpevJPdo",
        "outputId": "e1308200-d687-4afd-d611-242422dbca03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=1.1900, step=03584:   5%|5         | 6/118 [00:00<00:10, 10.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.9903, step=08192:  13%|#2        | 15/118 [00:00<00:04, 23.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.9262, step=13312:  21%|##1       | 25/118 [00:01<00:02, 33.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8864, step=18432:  30%|##9       | 35/118 [00:01<00:02, 39.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8684, step=23040:  38%|###8      | 45/118 [00:01<00:01, 40.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8562, step=28672:  47%|####7     | 56/118 [00:01<00:01, 44.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8477, step=34304:  53%|#####2    | 62/118 [00:02<00:01, 47.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8333, step=40448:  63%|######2   | 74/118 [00:02<00:00, 51.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8188, step=45568:  73%|#######2  | 86/118 [00:02<00:00, 53.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8176, step=50688:  83%|########3 | 98/118 [00:02<00:00, 49.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8110, step=54784:  87%|########7 | 103/118 [00:02<00:00, 45.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8106, step=60000:  97%|#########6| 114/118 [00:03<00:00, 49.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d28ac2ad-0a1a-4f8a-8619-150ef0519cff\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d28ac2ad-0a1a-4f8a-8619-150ef0519cff\")) {                    Plotly.newPlot(                        \"d28ac2ad-0a1a-4f8a-8619-150ef0519cff\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAaB0lEQVR4Xu2Zebju15Tnv2vt4Te8wznnTrlIYro0QaTKVASPJEUoEnMnoYJCk1ItosUQ1SipkiA8mkZHtBSlk9B0ax2UIajE8ERRFNpMqriu3OlM7\\u002fQb9t5r9R\\u002f3nHPPee\\u002ftl6cff7R+6vvP+9ufddZ61m+9e++193uog3\\u002fR8cTT4F90RL+zhdnzv34wjX6r+l0tzFs+d9K3p9lvVdOFOe3qwdX3n2L\\u002f72nXTS\\u002fY9v0XTdPfqqYKc+qNF+gFN25lm\\u002fWKwejh0wwA0N397Ev9NASw597PHw0Gg8HgejdtOqpH3XaPaQReWFh41eU37L52dPiyaduedz4Ir3vl0jT+TVXe8pOTp9kxsltGD7h+Toft9gd\\u002fK2zBG3rmywQ6DQHc+aUPOQU44eXT\\u002fN5\\u002f\\u002fGS+gygAPP4drxiu49O3ba39739zyxAn+j946NyTAGDfW84dfvfLW63AtrOBfbdM099Iu3di+ZG\\u002f9+NfX9TNhSlOe99u4Gdv+8BNl79lE96kk7NpAgD3\\u002fLPzc\\u002frl8F5Pfe+PpyyvP\\u002fvo8zM+cOv64yP2bCkM3+VE2jw+9ZNza09y+fi\\u002f3r7yk81GAHuuJVzwySm4SRe7e52HHz1oGuM+F52MPSfhrfeiXx07uR94wemn4NW3P+yGbxwZby7Mf3w6AJzW+dIj77OJbtIZF+FHTzs4Bft\\u002f+dQe8NMn+m9u3z5lwhfOxqG\\u002fIcWDH7EFP+PvtwxP+JMPbano3qU5APj66iPbGzbzdV1w0mcuvn0aruvhpzz8XIJizzceOG165LOB5kOPehn0umNmzFOv2k637HgDaPtzjoBNhTntbMKXPv2G\\u002fd9ZedSWL3BDD33PHN6+d5qe8xwAt52z7+7TBgDvvRHxAIDe1++AG4+ul6mN7d346Zbx8p8\\u002f7ttvwXfOmdz7uPvr50\\u002f9+auOX5fdf31X9Dv0rdMA8DEn11dfgusOv\\u002f3wqf9jx6GPTZnMA95ZfvlNX80+eBa+tYaOFubUG\\u002fv62ec8\\u002fPUfOPxdeez9j9cLn7kbt1w\\u002fDfEU4Of\\u002f8Jp9uOe0AUDad+TzD+eBfe06ve+u9acj6uMLW8Enbh7d79nvmOAHL97KAQCPf5B+rJqGAIAz3nkiADxgcfsdrr4Tfjht7hS\\u002feP1+3O3SnZMrmynT+e\\u002fGF541xNPOwr7r1tBGYfa8ZO7wgevGn\\u002fkMABQXP2+dH9X2Z8nKVdMQ+LPnfv62QwCm3naznvacAvirjeFjik02YNdd8KstABhigGd\\u002fVKYoAGDudGD5VwBedCJevdX00hOB5jVf\\u002fwmWXnQn\\u002fPz5W43Axx59r8sv6b\\u002fxsUtXvXfK8tpL9ZrLh8DLgUsPr7H1wvgrzh6+4Jv5+p+etP5wVCdfD1x98zQF9l9x5PMhW\\u002fFRnfeyuzngO0cb3T3x\\u002fU1mXLHrJ6PNYwDAG057xBmfn4YAkE5j+QrwYr3oZFx8j81L6qwHAXuffysA3An4xOImEwDgO39\\u002fr0ed+aaTcOXVU4ZXXdre9Joa2Vkn0Zs29vT1wpx2Ns47pi1u0aPvi7979zRc05+WpPfBrV+b5idfcAbwMAUGr\\u002f1MvYkf3W56jz7\\u002fLLxpdZPtiCb\\u002f9ivvvOWb1xx7Nnj46bJ3Cac+9PEY77vndc\\u002fetOVdXOLWK28FMP+Y03HrZ48a1tQOsPt60g9MH9PmXqA3nQ\\u002fc7drfw8fetkHXC3MlfeloXViO3X2fcDm++oLBNAWA4t6XnQ0W7L9oeu7f58PrM++rf73FsADgfnTGHf15XH2jsesb3mb90wuvvuCC8ob9U7h7F+y\\u002f4Wd7XvKEw194R\\u002f9T\\u002fc2ma7evPvcAADzvtfjBsw5sNh3RXgD4zNvX9r0N+e24dOcfP\\u002f6UruqHJht0rTCPPVU\\u002ftcEg+p2jgyM6+Xrgn6c7NQDY+1+\\u002fu9p\\u002f66NLmCe+a\\u002fpcSASABXjcYzZ9g5W+\\u002feXAfSlOfvg33\\u002frSwR8X08cfAMCNt135qNef\\u002fOap\\u002fvPQN+HaN+664uzhxy7b8\\u002fbhzZt75Mc\\u002ffuTzcZchvu84deGHEfDpp09jtId3fF+B24e7F\\u002f\\u002f2KF0rTOEP\\u002fbd15P8cN792fbCufyfAW6chAPfoG3DFzbcufOoU7Hj93hs3Og8A4HuPPf+mBsCz\\u002fnQLfukv\\u002fgDA3k\\u002f88OsA8Cc7\\u002f2mLdUPfu\\u002fCPrn7e3c\\u002fdCu8LvBHXPwjnffnBn8O7pnbfI\\u002fqw4tKtE\\u002fSIPvBExfHO7avnfXTbbZ\\u002f8L8vv3\\u002f2RTXSjKzXrk9a\\u002f8pJ97xiv4zWdehbwiekTKAD77y\\u002fB565e3fHf79O+7ZQnXPvFty1j82Tbe6SLXbG1MDi6kgE8Cmvf9DFaveFd9uGP+NIWNk+fwKl3psu+vOdauuxdW0xr+gsWHGe33H3hk\\u002fQfv3vhzmkO4Bt3AYDTHyH\\u002fvAluFGZ9Oz71JU\\u002f7xDPX4YY+voCvv3AaAvzal4xf95HV33\\u002fr\\u002fX96yS29h5z\\u002fR\\u002f8TvzzOqfkPp8FWTe+Ga7rvkx5g8cOvTFFVQPS+P8x+\\u002fujj7nju\\u002fqIv\\u002f9k0Bc54DV7\\u002fnnMuPOZ0s6FC9KObhmuFITrnFQCAF79i7sP\\u002fZpN9TdsF10zPIgDPfcnk4s8\\u002f6MKz8ys\\u002fuA\\u002fDm256+nl45YbRnnXzkU504Zs32G+ue1x07glA2j+1oX\\u002fykic8+H5dPIMOX3Hc029x\\u002fpn4yIenuwDwiKvw9C+e8Cr8fNqwoZu2DtcKo3rCVR9cfPAF97vT3puu2foXAHA1AxtXwE26DOaSV98deMNbjqTykU2r9GEvP\\u002fOUfQAWzr6yRLW5WU+J9my9OgHACf\\u002f6BXcG8M03b2oJAIB2Ut6kAIYfO7YfA+i+88l45XuOrQvOnPvSp+3j+nTM6WZDU9N6YymZFzxpsAf42s1\\u002ftdl8RKeeKe01x2tJB3Zk98NnvnLjz4+TyltPwUtHAM48TXHLf57xI4EeczbYde+3\\u002fCsAX\\u002f8Pnzwm7j8+58WPBK773rePs40AuNOTcdt\\u002fmoYAoKr2nKuW3z996D2qu24drhXma\\u002f\\u002fwAJywC4sfPbKgpjS3C786bgN4zDn3P\\u002fTB5ekmvaG1NXnoUy+fvpxs0UOu2zJceMepdwVw6ztuOt40O3Jn+T\\u002foni\\u002fGT548DQEAO3D4xtNx0aZ+PK2v8pZvYa0wv3rGc18JvOt9x9m1Zml0w3F\\u002fFwAAvPCiZwLAbdVX3v+9adsWbZ0wD7zkgXcEUL37qqOHrd9Yr3wa3nPM9R8A8CM8mZau+eI03qTv\\u002ffSud12\\u002fKOHoUtp\\u002fxdqV53j68a0Pm0a\\u002fXt+55BuvXbjxC588zlFriz77lC3Dc88Ffvi36e3H3hJ+ve7dx7V\\u002fNw2P6Dr3qm9+6p3TdIuuevfrLj3atKizyfS7rr+8+BdPOc5h6zdU74NnfPyijWk6\\u002fV+C32l9Hq\\u002f6v68Lhhe+94knb4z+v5oxv02xChkiiswiopRARhSkDJACYLGCZMAJRBbEJKJsRIQZSkk5qYAFQsIKJlVaj0kbMXkjpiQoRFWJkcBqEoFrIgYgjgXBKpGHY8DlzhqDIGzbWXlSsNBkG59oAiYyMUuWDBEEs\\u002fwksWiiBDGixOBg1DZkWAGGhYgBJyU2bFiQhJVZSdQIa8yVrIvEjpgS1ICJY2RYgDyzBSnDUDQACEkIqmsxdSOmrMcUsDBllhxJMpGRnCQuICrQrGaDgqO3k8QUS45NsBLJhNbPypO8iOHajy0ymyDBRSREWBiZ6cdibM7GWlEKrDFPilwjgYWVCJoExCxios2MMUItJ6jCJbUhaC2qI6JEnmOtoEAgqEhMiRrDEBusZ6JWkklMAAGSBGCWZKL1azEjAKiVqILKIcYsMClzy5W2lCDdVrgemXFteDm4Sqxph\\u002fBOnJ+ZJylJglMbKIuWet5Ek4SRONpZfqqUGiSaWNWYBcMAN9RKjZapQ2IirCgZjrkpB0Q8NKJqhSDRB2KOxpiqb7IVDxqyQpKDcjKRDAV2iJnLx2RkaEijgY0cbYQV4U0xWVWtkAh8a0BBnYmFy4Zk3Aq1eTAgNOpEHFjVhA4Vi5lmy8TQJgPPyJNj7ISUV1DPgqJYVJ9i1oprLOkMP43qE5mU1HHKfDZgYwcU8kAw1AFDhdkYZ+cKk2E11eOGKiOiRIAJ1pDRvJ\\u002f7AqO40rRISTU6m5DUiSfnuj3jdBwn9ZgDpwQGQZXYGGf7pfUYxKqquDIiasHBRidWM192vdi2GTUjX1MAhbL21Hgfsna+j27hDqXlOLEpcRzNytMoc3DBcJbFeVewO2BXueYBNYXqDD8LDib56JCbsu+VqzCKq1mLlkAdUmUmdn3fX5hzPrbD5dV6OFFKDIUQW2TdvDffs4XUK5NDYVi1RkEsYpmY+\\u002fncXD930gyWV5vBkDkSSMUciTk3fzTmGJRYWJgMI8v8tt686YV2sDSqlxshCKIhCpk1vbLX3Wa6YXwwLtXjSeMEPCPPlMfWqXHU7fU6C1qa4Wh8OK4OEywwy89GMkzs8nyht2C6sRkujurlSpiTVSQTvTVlMbdrfq7DMS46uxpTxUjqxDUeNi97d+zPF5zk4KJUbVIVoyI2ejZlse2EuX5fNB3K7DDIhEgIyUTnTJnP75qf61KKi94OYqwZiRK0KcWWfv5O\\u002ffkC2hbZgTxoCIlTGc1oLnHWz0\\u002fs7OC8NcUyVdGYBJ2VJ0MpepG53sL27i5X+W3Lh6lKTeMiMMOPEqHuJlMWC7vnF\\u002fJEYdkfzIO2qWGrIFg2rtu\\u002fY68\\u002flyUbd7im6dQsRlLkYOFcd9vuYr5fJIm7yhj7oYGNDAMjruj1dvfn5rIIqAtNPiFhSmCyxrju3B17\\u002fblMktlu66ZTsRhRQ+LhynzbHfoLhUum4KxeySa1gXLlKpdcPr9tl9m24GMnndg1Bxduj2IiZuRJLdSJKzsL27sLed5p4q7C7EeXahdm+oFjFrJud353b3tulUvy8ZBnZStsYGCd63QW+vNFapqgrrMtU86QwLAmc67T63V39uO4Cer8vDfkKSlAyXufFzvnF7qxbkP0Za\\u002f0NtcAA1ZnXaezrT9fpKaO6rvbcjUZEryyZpkt5\\u002fpz\\u002fZIlhmCzrst8pmTFc2E573Rs58Qdcawipuhk5IpcdWaeY8R+UXbmegs7+ykMK6OdLJujrElupp8AeZEXvbn5+Q4hhOjyIisyn8hatSkZAhUdT3GYmvkiRso5gGxk29iEjLP5PAyrRhZyZlseBBEnNSqhIHTznNqBNtvLQMYhKJvEVlJyoLLIjsQsQ6SMA8ikoGJaj1j22IzaVvo+BG8QQZRytGySUnlir1lqJml7XlS2GBHXZmJm5Ck9zYfzSTo9QtU07bYmVsgXQ5oLCTP8ItRUfY3dHpthHeNc1rbOIpLRZKFiNFhrnYzHNUm72zSj1EqUZEQAQvQZEw5NgOZEGo+DNkkTAxEafWZdwHBIRnaZ0SpqtIBRjUaCdcan8aQiaXebephaDZIMOICbTg6n9XjFUXOCaQeuDS2glUvWpja3AA6GhHDieDgcueXaTYpZeaqCadgrorX1aNIW+3f6VLUBbcXiZvlxIGo6RbLaDFeNCTs5rJoQa6iwRFACGctmMqk0s6EJw4pdJFZtmBryHoirwxpZOWmbwVizCsamYAyJWCYsr1bM3NShGoBDIpFIpCC2zJPxWszRhG0k1iSGLdS5SCvLLcCpbUcryWiKxkxyOyFvItLSoEp5bzwJg4kropRuZp4SJVkiCE2GY6tW23ZSNTFlks\\u002fyEzHkVLxNWFqpWSm2zWS1JUh01iQrTq0pfVO3ruxnOVa4tsmLCcYqrGap55CYuF90ZIiYr2YcEqshFac9J2103bLopGU0Pjmwkli1ak3p27p1ZT\\u002fPdZlrm5ya4KJNQo6KQkR93sn6cZliuWSbPHHHiCt7w7tRa7m1RdmrBwptCzRqZ+TpWg\\u002fTLBjvm2bis4VOt1mR5GxW5Zj1fjb5JJppmakkXxbFXFyiplwizWHFIpnSlc4luzDXM50wbuErI1EIoGQLs426Nbr9OfR0ueJsoq5WgkPturbjipiVC7nrt6OgtjK+SVCLSB1bOpdcPt813XYc4GsjrZAUbSx8Vvqi9XmnyLpNrMlPfCIVy7HxbO5m+uO6292Gwo\\u002fqfNxQVoNm5ZnymmOn3tlBMruLsu8bMaKJA1Rn+aEIbea5k5WN39Ersm6VEvnVXCMlm4wSTOZSRdYtZF5tw5PYIkE4eCW1CDQG\\u002fIJ3RLUdpmQqBANNRm1p21qcncsJxbLUkihpIjHKZHMXKxg\\u002fnzt1DU9Si6RCKqptL6PYSkK358UvmzrWBlHRFq2hIg3nB57d9tykWPNBa\\u002fyyC25WnkrZMHe5CmnkXYYsVoYsyZhGZvolFQn9gmKbQP3SUrmidaotIog5KSNQqjQ5nzLfrDQNRzYpmcSNMKfMtYEyaFa0w2bsa+uIcmMaBgVojMwuZmV9sAoU1IpY4iTQQKlRcS553yxvxBRU5F0buapjVsBwvTJpUVFUxwYT612T5+NWimRdjhSb3mqmHIyflSdVbQmgmya+mAu+3wwbhGgmETL7\\u002fcaU2xhtU8MWYly1OImouBVvDBsltZ69mVCktlkmodRlIktqKTdKlDyHUGfSrkjltciEPZjESTAeHhNNNlYDUlDBFhmEWVmdN54nlLitV0hIOkzkSC2bVEcnnNdoXZvGKXnTc0RWmG3fkJJksJPh9hQqXcxCVkZiW7Sz8oQT4cLYNFlNKY4GrYTo1YGMn\\u002fl+amMdXCJXSbBtGItYlNaQEbAlAxInk8yMKatX1cambpo0BFQpEbGzSztNDJ3VoIVWKdZxVU2rhgEuMPB+BE8mcAq1SBo4FjGsLD6NMzsmX5Pa2FRtk4ZEqgy1rudGrtuEXEdRZTKWQENrAvMEJudyaccAaMfzdckDE7QeGB6YWXkyCJ6xOJencdmOi7xl66NAQxFm+wkXPTNyvSbmaZQohZoFI8OiLAYEaDNuYrU4mWhtVYZpKQhDEktS28p4onwwVakG4kq1GtskmiCkIcXxpK0Xx0MVG9vFeCi0TCkxCCrtuN6ImYZpKSTWFIwaqZu2GY3DoTCsQaY+FA60LTSNcw3sq3ax1snPMEoULQ6MJqFORmblqbXj1CI0q83w4FB1ZFN90C4F1hhm+bUEk8Z1W62O6sV2HGCk3h\\u002f3Ny1SsNJwxmqLLHOlc51yUgUbRybTRmGEra22FVmZC2tZTOrE0sKYCRFH8gamm7mMvOlm9Th5GjkTWrbSsDfq1mJ2i0kVXBybXBqw1MYZ8AJn7DMqilBJeXCU2xRgspBJU8+7bndubKRvJm0bqQ6OR4Zn5SmOSIus47Nt0fqFrE6aZJhr9MHP8uNovHF+zhbWeCqKUKPQYelChLc2OLXRuLyTcWe+x2lMiaKJCuZIGUlhszIrYr8zz1RxJZUNkkUmtWRhTSfvaH++dJG41WQrNslycLDRuKKTozvf4zhZj0lMBmzUc69rpb+9a5ZSrEmyhsihgXFa1Nt7bIp+ucBFY9s0LFrJGs8z8gTHLkKHu7lxnZ1ZwkFJqaFWLZlZfsmKMWKp23HU217yaky1JFexAVsUUX3WiOGO7+V2VLeLeyuuE1ijt0FdbJP1fq4ss8VUL+0f5ZPWJCPqtfUmJXZl1i3tMDSHfjmhViUhmiJF71thLn2vsKOmWdw7oSDKGtmgsYzW2F7R6XATm0O3D0xtEhL6SapdicedwnWKnltK9eqBYVZVJXUanpEnu3aUZ5TEznlnUU3i7bdP3AStT2Jn+JExkwIUjeuWZY4qNAd\\u002fNXAT00pSG5B47GWZOyFPsrx8oNm72qqiNcElE2nowFQ2WaDDh1eqX6zWVjiY1iUNNFCQ67R5aseHl8Z7VxtWDRR85Ehjp2Q6IU9peengkZjamuAUTTaWRFkvUkqjpUPtvuWk2iZHGqjl\\u002fXegZMuxJ7O0NBjuXarySdmUFc3MEwnjSaqzXms6TRgeHi+PGlHY2s30E658zYyilwixXVyu9i0LEBI5sRSM0HJBLDvHmRuuLI+HVaDaCWeJJ5bCSkcIO9rscL2yOB7UdTAmWB9M5ZRGAsiOymWrqyujYR2oscnmgVqjtFLQkuw4GhONFc4SBi6kCRuRbaPSDFdWhoMmJPIjYwVVtw0HFmQxnnS43IvlpcnBUK9IPwRf8Yw8aeTy9lC5cEjaTpMO1Sur42bFJNPmgWe+34qTNIKmuH2SmcHKcLzStpFs45mok1xwrFmeZ9s4yEqMtEytjbZmIFnl5J3t9gofmkmttJqSb0wgkuQiMRVZUXStxCWNWFFRopZYow+OsB5zOW2KSU3RWovMdoqiaNNQQhqmQJYrQy04ZW3H2N6cLduw2gY7aOFSjJw3M\\u002fJ0ta86InlWdOYp4YBoGpg6a20wmmb4UchaY9ibbqfMGhmkpKupZYfGKnUpkkkwmWbG8qS1iLUJNrCJxJQIpOzZF0hVwzATMarCCjJJXcvIkBljR9FRU\\u002fugajglS5FMJJNpZowZB0ttbSNHNpHIRyGFZtoRx2HkTByWKalhjV5bByV4ch1TpZEjO2ryJqudEM\\u002fI03MbyYhXWxIojUw2DmxDY62CZviRj8kkkKOukA0j5+tRqa1YRrJiVJU0EiWwqRKVk2htawACJ4rJkwSuJpI1VeZHwVKdCyknFrQsaO2YjRtHdKoWJrAqVAxESSPMZC1mFdUGo2ARDSYRa8QIyCap6lZoTHJMyqS+CdYIbLXKZWgoH0w6aK0zyczKc9xJZCPVWRuzVK5ARUE2eh8S6wy\\u002f6FobmTTRgDUfxboz0dbBMRKoH3wgMQoIKwBlgEVNEgvT+OCCETGuVqcCMqpiFG0Gbq2KOAjAIlaEiEjFtmzAwUVK9khMZd0ck6NNQlAjyYnaJIYTi9hgwKYxYCUk8iN0G0qxFxpjmGqX\\u002fKw8W+OCsJISAUnYECQRKZGaGX4ciRKRsCanaqOSEZJkgwP9b4E2iTW086xpAAAAAElFTkSuQmCC\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d28ac2ad-0a1a-4f8a-8619-150ef0519cff');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=00, loss=0.8106, step=60000: 100%|##########| 118/118 [00:03<00:00, 36.95it/s]\n",
            "epoch=01, loss=0.8003, step=64096:   5%|5         | 6/118 [00:00<00:09, 11.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.8016, step=69216:  14%|#3        | 16/118 [00:00<00:03, 25.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7894, step=74848:  23%|##2       | 27/118 [00:01<00:02, 36.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7698, step=79456:  32%|###2      | 38/118 [00:01<00:01, 40.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7832, step=84064:  36%|###6      | 43/118 [00:01<00:01, 39.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7628, step=89184:  45%|####4     | 53/118 [00:01<00:01, 41.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7663, step=94816:  53%|#####3    | 63/118 [00:02<00:01, 43.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7634, step=99936:  63%|######2   | 74/118 [00:02<00:00, 44.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7578, step=105056:  71%|#######1  | 84/118 [00:02<00:00, 44.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7521, step=110176:  81%|########  | 95/118 [00:02<00:00, 46.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.7390, step=115808:  90%|########9 | 106/118 [00:02<00:00, 47.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.6894, step=120000:  96%|#########5| 113/118 [00:03<00:00, 51.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"2bec92eb-294c-4649-9fb7-659296c06e6c\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2bec92eb-294c-4649-9fb7-659296c06e6c\")) {                    Plotly.newPlot(                        \"2bec92eb-294c-4649-9fb7-659296c06e6c\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAXr0lEQVR4Xu15abRlVXXuN+dae+19mtsVBQVFE1RAAyIWUSSPLqbEoFFQInHQaeTBsIkRLGyi6BOHoKIRHHkKKIkGEBQ1Jo9ODCqIDeJTnjiMLQ+BgpRQdavuvafZzVprzvfj3OacfS8Hxhv+eL6R78\\u002fZ65v3W\\u002fPb86y91tznksN\\u002fYi1wnfhPDPB7W5gDf\\u002fHrOvU7xe9rYS67Y7\\u002f76tzvFPXCHH5VftWmGvf\\u002fHjbc+YZ1\\u002f\\u002f76Ovs7Ra0wh916up5+6yg3jHfl5bF1DgAwsddZ70zrJIADD359med5nn9xzC7\\u002fpw8fVKdg1q1bd8HFX9p4TTn\\u002fnnrswCuOwHvePlunnypa3\\u002fvN\\u002fnVuFezI6PlfmNJOtduR91Yj9DJe83aB1kkAT9ty5CHAhi11\\u002fuDXnMwbRQHgZZ9428ISfcy6\\u002f7HyNwD+6N6RIfZL\\u002fvi\\u002fTL8SAB659KTOT749GgV2OwF49M46+5SwcT3mjjv8V09e1OHCNDd9di\\u002fg\\u002fo997o4LLxmih7DfWosCz\\u002fyb0zLa2n3Wqz71y1rkohNWrs\\u002f4p+8tXR57wEhheP99aXj83NumFq\\u002fkwt4Xtu361XAQwIH\\u002fRPjLm2rkEM5zzzoVv3xuncaz37QfDtwPH\\u002f1DenT18j3itGMOxju3HXX9Dwbj4cJ88tUAsKl913HPHmKHsPlN+OUrH6uRUxefMgHc\\u002f7LkJ+vX1wvz9RPw+NWkeMHo83f6PSPDvf7r9SPCh2enAOAHc39SXTfML+H0\\u002fW5786N1cgnHHnLMSQTFAfcdVg8d9zqgvP6Fb4deu2rFnPKx9fSt3T8MWn\\u002fGgBgqzOEvIdx164e3\\u002fXjXC0e+wGUcddUULn2ozp50FoAHXrr1gHoAwKdvgv8tgMl7N+LGHy3TtY3tCtw\\u002fMt75rpfedynue2nv4DeP8Iv41mEPvn3tuux1zdMw1aJ7NwHgZj363i24dsdl2w+7ef32r9RC9nlXNL\\u002f9oe+m1x2PJZcrhTns1kn92pnHvO+z238iL9n0v5b5FZyxF771uTqJvwAe\\u002fNEFW\\u002fHMegBA2Dr4PH4GeLRcYg\\u002fdsHQ1wBS+MUrceGfn0Nd9vIefvWmUBwC8\\u002fAj9SlEnAQCbL98XAA6b3W3jp\\u002ffBz+vhVuOh923DM965e\\u002f\\u002fiuv60K\\u002fH1MxZwyvF4ZOkGl7+8A7dMzf77td2vfmQ7gMZ5S\\u002fQQ1v+V7Fxj73njhzb\\u002fyZkPA7W7HcZfntMA3r88PKExFAM27I\\u002f\\u002fGCGABV3A6+qdxADTRwO7HgHw5kvqZrbsC5TnH\\u002fur2V+dvA8ePKsWxVd+vt9FrY0ffeXOC6+sRd7\\u002fKVx5xgLwt8CW7Yvc0opJLzmhc9a9y473XbpYwf5fAC6\\u002fo84C\\u002f\\u002fGBwecLRukVnPqOpyfAfX6ZOAg\\u002fGwrjwxt+3RkeAwA+sOnYzbfXSQCIm1i+A5yrb\\u002foDnPf04Ufq+BcAD5\\u002f1PQDYB7h5x1AIAHDf95\\u002f1ws0f3Q8XXV4LXPCO6vYLcmQv2pc+tLynLxVm0wl41V1L5Jp48aH45ifq5CL+ukX6bNz9\\u002fTq\\u002f\\u002f2l\\u002fChylwMJ7bsuH+JXtZvLFp70IH5xbCS2i98Z7Lv\\u002fWj65c3Rscc7Q8PIvDjnoZeo8e9Pkzh7a8c5u4++LvAZj5s6Nx920rgUWUHex1A+ln6+fZ9Ov19lcBz7j6cHzlY8vsUmEuobtW6sKyevc98SJ89+z5OgsAzYMvOAEs2HZOrEWe\\u002feX9Fq+++w8jgRkAz+EX7pOcyvn\\u002fLG2tjQEAPHD2Vaef3rpuW42e2B\\u002fbrr\\u002f\\u002fwC0v3\\u002fGNj09+belYBwD84\\u002fr51\\u002f4WAM65ED87\\u002fbfDoQEeAoDbLlvc95bh1mPLHq952SFt1et7Q7RzzrlX9MP5gyvnnIvh8pXBAAfFGK+uk8451zpqa+g88uWFELb9bbsWO\\u002fw3McaoMcZ44hB9ZZz98Y9\\u002fHKWau\\u002fvvX\\u002fv01mPVUGwIh98e45VPq5EvD+H9bt+bw9zlE5t+MXd5Leqcc+6VeSjeUiedc41\\u002fFpFb6qxze2wLMYTw8NawbYhdXDEN9\\u002fiXl0qVvhd3rOrCzxfgo3USgHvxF3HRnd9b97VDsPtFW29cPnkAAD998am3FwBeN3q6vOXhPwaw9aaf3wMAZ+\\u002f+wEh0GT89\\u002fc+vOueAl46ShwIX44Yj8Kq7jvwm\\u002fvs7R4MDfFmxZXSBDvC5VyjW6tvnTvmXdQ\\u002fcdM2ua\\u002ff60jA9qM+p4TdLpZr4YHjoz5cGS3j+\\u002f\\u002fb+K3XSOdf6SAi37uH2\\u002fmHML\\u002fqXEL72Z0ccUf8T59zuIQyvmFF8Sf6uTi2jjOXxo8xH4r+65z8az3eHPBCH1vgQLtEYn10nndv\\u002fv0n84WfiPXV+GZs1njc0XO5jbl78PGzLKTe9eolcxi0z+MHZdRIwF761994b5p532XPv\\u002f5s7J4889WW3YOsa\\u002fczxdWIUN9aJAQ49+Y8sfl5\\u002fUVIFRA+9P3tw85o7njtM9PzRhhEA8ML34X1XnPjaVd3NMjLR4RWzWBiiE88HAJz7rqnPr+oAgN0EV3brJHD2W\\u002ft\\u002f\\u002ffUjXntCdvE1W7Hwb\\u002f\\u002f26lPxtuVg8qI7ByfRX\\u002f3dMvfUcdCbTtoTiNtklL55y8uPfM4EzqAdF63Z\\u002fTZPexFu+HxNBOC4S3HyN\\u002fd892ADXhO15mCxMKp7Xnr1ziNOP3Sfh2\\u002f\\u002f1OhfAMBVDKw6iwG8G2bLe58BfOAjgwPphhtWYke\\u002f40XP3Apg3QmXNJHXe80h0AGrp97z1W\\u002fYH8CPPry0kJdQ9Zt3KIDOP68+jwFMXHEy3nbF6rpg89RdtyYvmaJVL0nLqC3r5UfJvOGVnQOA79+50qEu47DNUl1Zf3kEgMd2T5+D275940P1gxrAZYdgSxfA5k2Kb336znp4Bbqqxd1w8GXPBPCDS29adYv3nnnuccC1P71v7aZr75PxwCfrJACoanLipbs+U296V\\u002fCM0eFiYe754fOw5wbMfnHwQNUwvQGPrnkAbD5x0+NX73qCH2+A1w8+Hr\\u002fl\\u002fDELBnjBNSPDdZ98ztMB3P3x24d7wiV89at1ZgjPPBe\\u002fPrFOAgDWY8etR+OcW+r8Cr7DI9\\u002fCYmEeefXZ7wI+8ek1dq1x6Fy35u8CAIBz3ngmADzQ\\u002f85nflqPjWC0mTxiy\\u002fP2BpB\\u002f4pKhZuup4t2n4Iq1d5Ff4GTaeeU36\\u002fQQfnr\\u002f056+9KKElUdp2wcWX3nWwi\\u002fvPqpOPTl+\\u002fJYfXjhz4zduWqMFHcHX\\u002fmJkeNJJwC9uiZfNjbBPDQdP4h+e4N4\\u002f5979o5v\\u002fvs6O4JIr3\\u002f\\u002fWlUOLVv+W9fuLD5730En1n\\u002fueOiav2\\u002fyv5ywv01Ub3+8zvo53\\u002fN\\u002fXBQunf+oVS+92\\u002f5+tmN8lGEREREbFMgOsLEaNkjArERkiMBGgRAQGiRl8EhEzg5WICASwUmSw8mC+J9SBmGigAxGBlGRRt+RFxLAZeOFFLyBiHsxJpEREDBJe9rKYj1blAxEDYCXwoiMow+gg2xP6ZFaCEqJhRTTMRpxwZHClrCAVkAhZJjYgmEiBCQEKEAQkBMPKTKRWTDRKHjpOp0SkxJFhWIlBsMqRlTyUFFBCtAYUmInFKUcGe4UCEGIRMkxsAJhIkQmBhvINYqM+GWCOzAZKhhQWiCQaGeN8UlOD2ghWBBeJiWwZDEhJAWIVkII4MimYyEcDZQjUQJUUAKkRMBFXwUIJqjRWpxBe1hGDfbAQBhRG46KX6CKYyFTBEHTx+xzMaQIRBnNaFQOp5yMazseAMBSaeGYhI+yjUTUAdJxPygSMKMQgbUJhVFVKAyqtEgiDm2HNIjMHEi0ZpKqsRKIKoywNYZgAQUVEgZ5EBxZVIpg40FFEaYBIykKEKExE0gTURqzywmqksTgnCiJSXStfRMkggZISKIJIbWxEpqQyHkWiKgSs1i37tJF9VoESGG6zS5ve9Hul+KxnhZRiUrImzLZpMtvw1PN9rVxhIguiCyyOyTZswzY89apcvOvbyGN1FE1gtUy2yU2bee5VeQwuN5Ejh7QiWGJumdQ1K5P3C\\u002fFZzyx5gWW2TdMwi\\u002fmqpDAj+Rq2lk9tleZGLbFtcdNknns+Dz4rODLG+KRMwJGMcY12ezqdIbPQ2dmfq7yPAA0WGWdZq7k+nSbkncer2SpEDyKAg2FOs2Z7XTpN1F\\u002fYXu2sog8EPLnOuKzZXpfOEPXnt1c7Kx8DqRGYMPDSms5mYDqd2WJXGQZeYCIzZ2mruT6dMjo057h8IDXREKdZq73OrSPqzz8eZqtSA3Tc\\u002fVkhiDHIWtOT7fWTU4a7Oxu\\u002fDbtsFCVSEcPcaE7MpHtOTFvuzKc7CtFIUFIEy2i02tPNPSanLC\\u002fMpzuqBQUBT0GXtVozzQ2Tk5Y7c+lstQAhkICiZWStqcn27pNThju7ssd9QBSABMEwN5rtdemek1OWO3Pp9nJBI0F5jXwrMY1M3Gi21zf2aM+w6c5l28uOlqCxPq1K4hN23J6YmdzHZiwzpiz7E\\u002fNWRFhtsOSo3ZqZ3JilIMv9ohMXLCIEiWdOuDUxM7nRZaQzJq+6PrhKBGN0CuuZHLcmZyY3upR12uS+U3WtipDaauBl3eTeSUY6Y8uq1x54gQ3GJNSemJnc26VElvOiGxasClRtMFzLtxgDkmCRmon21ORGlxIl3PcLaWkQdJxPJuOdNzJt09aE56pXBNNMG0QVKxOXtjI6bSeaLa9VmcdkotGACVAwldYbnbKt9kQgn+diW40GcQmhcTpCab3FlG23JwJ8notpZw1DFYTA1cBLozURqOrn0TTTbOCFubKeddq2my1PfmVOUhBX1tfyNRdjoNJ4i0mebLUifJVH007abLyOvz\\u002fLYHKaFHtOTCb9Piu31SAoUVRlSuDIlHu0p63sSjxPRIi3UQUEsuRgyw3tCet3ceBJoRiMkAD6JLoEptzQnrR+Fwcz0CkpsOylPZnkfSPcVoMIUIQK7PKccWfieSKqeCsalZVWYkv5ordRRVlhYYnLDe1pG3ZxcO3IWlohhdAYn1YI0XmbJoZ9vzRZs6KeagViDUZJbD7lUke+E9i1Cuqr5sSDIzKawrk04ZB7TpsF5aoFGWXhsTpEV6QuNRQ7gdJGvhgTjiBEVyVpYsj3K86aFfVEC2JotAox+bRLHXwnmKRVIFfNwYKVfMlSPizmIyVFcPl0mjHiQkRKfe4K5wxGpDE+LcRqMIkIhX5eZWhUoag8SqnYCQGakBDFfplnMeMiD8KVBjAUUEeiiHlRpDEln0ukKgawjtEpCHAQXYylscolopKKGGIQrBWh2C\\u002fKgZcQUEZPVlmBBEIkvSrPXMZFHiJVGtUs5+PhfFRpAAsTM6sox25RNGMWylwr5OINjfPJ4MilJZdCmCcmJ10LcSEYQhoVUC6NNAwFlfbMVOooLHgylBAUYkpGZjgqWtPTWQNxzrNhB4zTgZQLQmY5Au3pqUYDYc4bSw4EjlxZcikJ06KXeW+JnACqVLI2LAVoe92UcxQWAhtK+Any8SCfiimgjQQR1F431WjCz1WccApeWzfwaSnYYKqmo1YCpPs4lK6UUkzfEDQm0YQmW5PFZrrRaeG894KKlTUmwfimsUkqlO3ttHQ+eEVJ4DhGJ2IjB2NMkgo19kpRpiF61YpAGmzkqumokeiil0pKNX0GVKyYaNiYLJp0r1Tz1AcvqEjNuHwaEw+xalwabbYho7whIUYNIB13fxYEUFJZsmpmGu3UdwtLHOHKLLAqyEkD1ruk3WxVnQIJq1hJhEUVLmZkomu1m03fLTShKIl3Y3UmCMjFlGx0rXarEbqFJjzQEUEp8QlZte1GK\\u002fPdgolLTarMc1RgZc52uZDDsoqNiZhx+UTATjO1lbPtdsN38+gMqQ0mjvPJhCiEiV6jG0qyscoz8s0s0cyAYKAirXlXFX22yPvOUtZ0NokQJVWg3Umrss9Gy9wZZI3EpqQ6TgdSAdrdrKz6bFDmjpFmiXVQLHrpNvqhICtlnnJoNhI0DDFYVaW54Kqibyz6uTODOQWiY\\u002fIRQVQb3azwPU6ozBNDacMmCRTjfDIYKQvxbD5X5VVZ+K5W\\u002fVZl+7YASXAWRDt7uRbe56Eqql5ScmVzQ4jOCGi215PCV3lVet9NKy5MacbpFDFhBe3odaXwZVEVle+mnitbEhiOBWa2P+fzqiyqXqz6jSrp25ygMUmUaFcvR+597quy6iUFV0lRy1eN5BMEywTM9vpSVmXhq8LPJx7eFDzOJ8NYr7YiWoj9fCHPqx3llO254KJRWOejrVKz4HtVp1\\u002f0H\\u002fMtu5AIe6uwSYzWW+6EbtHp58Xjvp10UzHhyXROxHjLndArOnmePx4m3EAHGBvEVlj0UlQ7qinbd95FI2SS4JOQ8ELoVZ1+2XsstO1CKlwt53ODfPlQvkSRpAorKfdCv+yWee8xmWj0UjU+GeuTVUJoWJsEG9KAMkRTRVuGGI1REpWMnPFJRaXvq0+DOvhonBUSDQ2TJGKqxIdCQlKJkypSwmN1HGWgs6XxIRdxPlopIywLJIRmYpNoQxq05kVVMkqsT0qUvi\\u002fB+ZigiuyMLOazq\\u002fMZoRBjBpt4U3Dpu0KpF5Yiql3Wre2TKEmDWmRk+03mAE16pYpREURFosZIKja2XSKpmn5ZKYkoCcgpjCRqq6a1kqjtl0FYoo7XETsFayJJ1bRWrNq8CsoiICBJo1iksP0mcyRN+iXEaBREIQvDy3NmZIuqVFIBDfKtxIbzKRurhiSTxLeMFSumX1U6+AVgjE\\u002frDZLKBaK0iUa7VRbocicWVLR7FhoTz56pOWGyqcz3tMvz0ufcRRsAV7EnarS5Mdmo+uiaeSmoSryVcToSV5pA7NrUmGxUPXR5XnJUrrKeNSmTSJQ2kbXbAy+hQNHKrdLSnG2bTWW+Jx2a05xyF23UmHgOS7GqP5QvknfeRrBrmcZkw3fR0wX0Kc9KN9Ct7dMmgCjYqWtMNlvUK8vebC+PYktLykzBgOCajYk2FWXR25H31SdRIxFDiAxcs9VuUV4W3R15X9RECMbqjCqINWu22y3kVd7ZkfcFJkAskSjIaTrwUpS97b08qq0MQAaRieBay3MWfdEkalzJtxLL+xqSqFFMQgIy2mg3J9ra9\\u002fnCbL8nSDwN6Vb7tEQhuirvZgVbF2Jn4cEwWxamNDAeJD6rfCdL1ieJl87cQ2G2KtiTQWSWKquqTpqsTxKvnfkH4\\u002fZYmJzgwjidIQlpWXWyZJ21lXbmfyPbw6KOB156acE2WfRSFba\\u002f6CU0vO803G5uaM6SDAUi8VnpO1my21K+Hb7gCgbRGCmysuw1C3Ku0s7CA7Kj6puCDWScT6tKCfkctmqURVEWO7fHblEmJBSVAKu+G9KQtYuY9+Zm4zxECEIEhUXsxTRkeVGVvbnZ2JUgUERgjE4BC+nFNDSKwhe9uZ2xKz5CIRCihHy+0\\u002fisnCqLfNf22C0ri0UvifpuSL3L85j35naGeUQhCBMAq7Eb0pDli\\u002fkWEAUQIo1IBB2xVVbmoezM7fQLECFVGnt\\u002fZK1v9I1Jkql0HWlPZ+O862W5idYnSuKKBDadtrspymreL3CRlSZStEoxzRMy6XSyXrUIc37B9BulCSYYjNEJqcstGzeT7KYowlyYN71mYYMJNlrf6Btr7XS6jrQvO+O87TUKE2ywS17cjN0NWlVzfoHzrEwCRaskLneDWD1fZHUFm8TOuBlFWS34edtt5YlnSWSMT3LRQokMeCL1gULljY\\u002fRGiGFxkQhhhmtVERQVKakMoWyANEqlA1xMwsRWlamQpmpstB4XSIDXSv1KhjoUqgRRbRQgoVpZT5qrLzxEiwrKVSsQgwTtVKJgsKPzLmSL+pIPo1OVQ0Rt11ApMJzSZU1ynFUV\\u002fNJzdJ5YoYBbLARUVBkgWxkUkQOlsQwEg6IxguqRJiUKXI03hLIqDWLMW+FQWChcbpgvSUQw5pAwXhB5aJRAkdbOk+GYEAmmkAxokwjGWESEgqWlFnd0JxGiSjy2vkApkjC0bAwkTWRvKmEyjRaIaJg1taRMkVqRIChUCSeABZhAFASJkAVRAqAVYhVFWbwfzaCQgk0eFIVrAplKKnSk+gUpKwAkSpYlJShUFKmCBApFDYCYFECAAz2EVXCyJxqVUmhvJKPaCkfYymf0GBWIkQmUYAhHAxYx\\u002fj8P3YkIQOmUtR3AAAAAElFTkSuQmCC\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2bec92eb-294c-4649-9fb7-659296c06e6c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=01, loss=0.6894, step=120000: 100%|##########| 118/118 [00:03<00:00, 36.84it/s]\n",
            "epoch=02, loss=0.7357, step=124096:   5%|5         | 6/118 [00:00<00:09, 11.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7369, step=128704:  14%|#3        | 16/118 [00:00<00:03, 25.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7555, step=133824:  22%|##2       | 26/118 [00:01<00:02, 34.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7240, step=138432:  31%|###       | 36/118 [00:01<00:02, 38.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7573, step=143040:  35%|###4      | 41/118 [00:01<00:01, 40.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7444, step=148160:  43%|####3     | 51/118 [00:01<00:01, 41.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7267, step=152768:  53%|#####2    | 62/118 [00:01<00:01, 45.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7315, step=157888:  61%|######1   | 72/118 [00:02<00:01, 45.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7401, step=161984:  69%|######9   | 82/118 [00:02<00:00, 44.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7247, step=167104:  78%|#######7  | 92/118 [00:02<00:00, 44.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7255, step=172736:  87%|########7 | 103/118 [00:02<00:00, 46.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7216, step=178368:  92%|#########1| 108/118 [00:03<00:00, 45.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7527, step=180000:  97%|#########7| 115/118 [00:03<00:00, 50.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"6e07da08-5ef9-418f-aa3e-936e47bdce19\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6e07da08-5ef9-418f-aa3e-936e47bdce19\")) {                    Plotly.newPlot(                        \"6e07da08-5ef9-418f-aa3e-936e47bdce19\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAaFUlEQVR4Xu2ZebRsVXXuvznnWmvv6u85914aUUAj6gCli4ovBI0IDvEhqAQNAjFoiH1UsInNixpRQYfgSwMoiQkPNELU5wMEHxgFNUGNGHQY+47Oy+1OW1W79t5rzfn+qNPUqXu5Ot7wj+cb+f1xaq\\u002fvq12jvllrrzX3PhTwn+wNnhb+kzG\\u002fsYU57Ps\\u002fmpZ+rfymFuayLx5897T2a2W6MMdeVVx1zJT2\\u002fx773f7y2f942bT6a2WqMEfdfLadffNGbZK3FOUJ0xoAoHPAeW\\u002fOpkUAhx3+J2VRFEVx\\u002fT5W+aff+5hpCTIzM\\u002fO2i64\\u002f8H+Ui2+f9g674sl4+xvmpuVflda\\u002f\\u002fuyQaW0P3IbRkz7Rs+Vq83H\\u002fXm2Q1zj3jQqbFgEceuFTjgD2v2BaP\\u002fzcM\\u002fhhagBw6l+\\u002fYWlV\\u002ft3N\\u002f2v9PQCeeNeGIR4R\\u002fsvvbHoeANx\\u002f2enL3\\u002f7SRheYPQV44I5p9VfiwK2Yf9qxP\\u002fjlRZ0sTPPofzgQ+PEHr739nZdMyBMcsrdJgce+5kU53dd\\u002f3O9\\u002f+AdTzrtPWT8+5+\\u002fvXD182qM3FIYPPZgmx0d\\u002frrdypO8cfOIXCz+cNAEcdjXhBTdOiRO8LjzuLPzg6GkZj3\\u002flwTjsYHzgcfQLP+3hSWefcDjevO34j\\u002f3beDxZmL95IQAc0\\u002frS0x4\\u002foU5w4ivxg+fumBK77z2zA\\u002fzo1PDtLZunLPzzKdhxNRmOe+oG+eyvbRge8NKPb6joPbt7APD1hd+rPjapr3L2wbe8+hfT4ionHPHU0wmGR3\\u002frqGnraecB5cef\\u002fkbYNXvMmDM\\u002fuIXu2HoxaMs5Y2GiMMeeQvjSzRdv+9bC0zf8gGv8zt\\u002f2cOm90+rpLwHwk2ff\\u002f+hpA8BHbkC9HUD3mw\\u002fDDd9ck6cWtivx4w3j+bc8++7L8K1nDw5\\u002f9QZ9hTuO+vmb9l6XA655JHot+uYxALg57f63C3DNzst2HXXTlp2fnrLcb1\\u002fR\\u002fPJ7\\u002fzX72MlYvarXC3PUzV373B+e8I6P7vq2nnLMv6\\u002fp65x7IO64dlrEGcDP73rr\\u002fXjstAEg3j9+PXkGuL9cVZ+w\\u002f+rRmB4+v1G44fblI8\\u002f70ADffeVGHQDwnCfbp4ppEQBw4hWPAICjdm058KqH43vTdrNxz58\\u002fiN9689bhRaMp66wP4\\u002fPnLOHMk3H\\u002fNSvS2o932AW93f9xbf+W9+8C0HjdqjzB5j\\u002fSuYunReAV7zvxaefeB+w3bazzgvMbwF+sDZ\\u002fVmPCA\\u002fQ7F9ARYskWcN91JjOkdDyw8AODVF09\\u002fmQsfAZQXPvWHcz884+H4+XlTLj79vYPf0zrw\\u002fc+be+eHp5x3fQRXnrMEvBm4YNeKtjpjsotPWX7JXWvf+BGrB+scch1w+e3TKrDt3ePXp2yU1znrTY\\u002fywLfqNeEx+O6EjUv2\\u002f+Hy5BgAcNGxTz1xah6N0WNZvwy81l55CF7\\u002fyMmKnnQccO95dwLAQcCNuycsAMC3v\\u002fq4pz\\u002fjAwfjosunjLe9qbr1bQXykw6m962t6auFOfoUnPHlVXGvPPMJ+MJfTYsrvKpF9njcuXFNBXDI2ScCxxuw9LbPTU7\\u002fb6wddZ951sl43+KEN2bwiq9dccddV+7ZG5zwu3rvHI46\\u002flQMHnjMJ86ZWPJe18SdF90JYNOzTsCdn1s3ViiXcOB1ZH9\\u002fw5Tee5ndeibwW1cfi09\\u002fcE1dLcz76UvrdWHdc\\u002fU97T34l5eudSKTNA9\\u002f6ylgxbbz05Tz+E8evHL0lb\\u002fbYMwCOJKfflA4i4uvl259WV7np3981dlnt659cEruHIptH\\u002f\\u002fxYRc8Z9c\\u002ff6hz6+q2DgD4uy2Lf7gdAP7knfjui7ZPWmPuBYBbLltZ99YIW3DB1hefekTb7OODCTmEEMJzh\\u002fHC8VEIIaR4+fpgzGEppaunxRBCaB1\\u002fX1y+\\u002f5NLMW77s\\u002faUd+zPUkrJUkrptAn5yrT77rvvTlot3PnfX\\u002fzI1vZqwpvg2NtSuvLQKfE5Mb4rPPzGuHB555jvLVw+5YYQQnheEUevmRZDCI1PqepN02oIW7fFFGO89764bUJdmTGNsOOfVkuVvR1f3KMLf4MCH5gWAYRnXo+LvnjnzK1HYOtF992wtvMAAL5z8otuGwH4o1dtkP\\u002f03qcAuPfG730dAF669acb3DW+86JTrzr\\u002f0c\\u002feKD4BeA+ufzLO+PJxX8Rf\\u002ftlGc8wnDa\\u002ffOEHHXPtcw9769sXf\\u002f8zsT268Zu7aA6+flMf1OSv+bLVUnffGe\\u002f7r6mCVJ\\u002f2krj89LYYQWpfE+Nmt4WHfSMVF\\u002fzPG\\u002f\\u002f3MJz95+i0hhC0xTs6YjfyTfmBaWqNM5UkblUvSZ8KTHkgXhiN+mibm+ASXWEqPnxZDOOTPNX3jo+lr0\\u002foaJ1p63cRwrY+5aeX1qAvOvPGFq+Ian53B1\\u002f94WgTkHRcM3n7d4hMvO\\u002fpHr7mj+5SzTr0Z9+2ln3nmtLCR6dVwhSc8\\u002f7cdvveVKdUMUHvCj\\u002fOfn7jXFS8cpXbBxoYRAHDiO\\u002fCOK0578R7dzRoNtckZs1IYotMuBAC89i29f3zJhL\\u002fCZsWV\\u002fWkReOkFw1fddtyLn5W\\u002f5+r7sXTrrS\\u002f8A7xxzfQn3T7eiV68vtb\\u002f6jzmlacfAKRtulG+6cLnHHdkB+fQroummx8AQPOsk3DdP06dBOCpl+L5X9j\\u002frbhn2ljjto3DlcKYHXDp1buPe9GRD7\\u002f3tis3vgMArmJg7RZwgrdBLnj7o4F3v3+8IV133bp3\\u002fJtPesz9AGZOeX8Txd57VQAAHbbHNo\\u002f9\\u002f+DlhwK46+LVibxKPWzebgCWP7Xnfgygc8Xz8YYr9qwLTup96Wb\\u002f7B6t9m97cvLG4dqlJC9\\u002f3tJhwFdvf9ekPeaoZ2h1xfTNIwBs35odiVu+csM90xs1gA8dgQv7AJ5xjOGOj+zjIYHt0Rvsd\\u002fiHHgvg65feuEfEb5772qcB13zn7r03XQc9Hz\\u002f5m2kRAMzMn3bp\\u002fEenm951HrVxuFKYr37jiThgf+y+fnxBTdHbHw\\u002fsdQN4xmlH7\\u002fyHhYd4eAO8bPyy47MXTt+cbOApq\\u002fcnY2YuP\\u002fJRAO780G17m2a33DKtTPDY1+JHz5kWAQBbsOvm38X5n53W1\\u002fkX3vArrBTmgRec\\u002fxbgr67ay6q1L5Y\\u002fttfnAgCA819xLgD8pPjKR78z7W1g44R50oVPPAhA8deXTDRbvypvPROXT\\u002fTCE3wfz6e5K78wLU\\u002fwnR896lETF9rqpfTgu1duefbGD+48flr65dz9p\\u002f\\u002f2rpkbPn\\u002fjXlrQDXzujA3D008Hvn+TXrrnXcIv5\\u002fAO\\u002fvaL0+KYa8Jb77rpL6fVDVzy4b94\\u002ffqmRft4FPsbx3tef8\\u002fp04\\u002f7fnW6H3vGZ85fm6Z7v7f\\u002fDeXzeNP\\u002ffV2wdPaHn7t6b\\u002ff\\u002f2Yz5dcIGIlM2gCEGAoEJagApzEAgNRhIycAws9Xt1YgAI5iZAURkMCPAxqcZG4yJFQQyAtQM0PF5pERmZmRYOY9gK5\\u002f5kF6KBlUmJnMwAguETU2B8V0QAWYKEBFgRiDD+HxSgxErmTHGHgC1BE1gEDFUiEBgTWZmykKWkJmxU03e2EVWA9RMGQxTMWImIm+QSARAyQAQUmKQMbPzUBmXzwCA2aJ5BTtV9SCJMq6kmYApKRsbETkPiBoU419i7OlePWMgBMAjKnJKLkETj5KSM5AlFYIRsfOWxMwMZDCAGGklA8wb+ZpgBCUzsHCiQImYzIVkHsxJyEAQBqlxbbAosChJvbJCmEiMFJBkpFGIIGKiTMRQqMGI2AhQNpA4ZRViMiiMYOBo0MiAilpQMjgCSG1lHkAFBHbG6saTFCse7dWDKrSExujB0VwUZphTDzCMWRSAUQI7NjIhglGCwUCiqxkcq1chMBQwiySqQEqsymQlK7ER2CRRgKmHQkiz5EERhqSafM0EUhOORAKnwYyjGTSCE9hg6jmCBM6cEUUzpAgxJcBMSMFsIQWDwkw1qYtEIFVPCjgIxIhqUiRVhpHtw0tG5g0Sg+a1sPqRESrTUJJTUvUcAYGDN+OkyjESm65lYIbTTDdkUE7kUxJjCnXGUTQSRiBIAjlODrCgGZo5Z56GVZUGWjM0SWKACeIkZFlGVlaxHsZkACGJEomJkzxknlCWqSpiJBgpjMmYkaGRSy4oyzINUySYURJb+cw8ZIEwqlI1jAkwwj48gzERAZlkjeCISu1bv7IoBomiRKJOfMgyb1SP6rqsI5kKHiIDDESRSYlZkPsmN6KrqrKsKo6JyLFyYgiyVqvbnWkI9eeWlrOBclQyUSOQc203051pSCoXFgZ5f4RoZpyIiJxry2x3JvN1XJgbDvsjrgBWSsws0s43dTc1GYOFpaVGf0jRzESNQeI7MtOdzV1dL8wP8n7BFfbpJae1CFHebszms7mPo6WFvpR9F5FIEhHIh6ab7cz6LNYLc4PhsOAahr1k6Of9EaKaGlSMKeTNXmvWN6tqaW55OBzkKSZnTqHehWavdfCmdidUZW\\u002fn\\u002fM7lkEAGY2UW12l1HzbTbYsOes35nUNRBcgI6oQ7zZn9Nre7bINeY2H7yKlBwcZgCc1e+xGb2l1JxdyubPeyUzOCgUBO2u1N+812Oi4Ne6257YVTALTm9faf9gQQMOf5pt5B7V6HdHmhsSMWwaIxlM2Epd3cdMBsu8M27DXnto8ECqOJDLOdtkvD+bUMysmIJGt0egf1Wh2fisXOju1VZqrkuAY759rtrZu29PIskItUL29aVBBgKhDfbW6e2a+T575icDUYofLRAHXkpNua3XxAJ898dEZlq9TaRUIyYefbrS2bNvfyLIw4UTXo9CsxY1NPIr3mzOb9O42V85qlVZKMHtoDEjnjrNvq7dfbFLhROwqoiqI2YrPkTXynuXnzAZ2Q++RVRp0SlasBU6HJDGJrGXwtouJa7a3d2W7WYg3qh\\u002f2KChi4FheYstBqbJ5taeyjbGRt75k9mcGJ5+A63S29lsVBjHnWcQFmambCwpnvdGe6TUtFjHnWdUHMDImdEw6+3do809K0bFUjtHxGxAQ1IeZcOr3ZblNjUdd53g6BzHRfniWGMjJpdHrdBrjQOkiWNR0cmcGxk+A73dlug7So60boSoOhIIUTN5mhzrOOy2CmFsWIXO47nf27TYpVPWqg7RpKRuw8pVEA59lMPipt3uc+kLZ2MWoYk0XnQsO3sjLpgjSCWN2YhxorhKIX1\\u002fCtRqVpSfKMrc5hyQiOtPaQPHTzstYHfO4Ca2MXWQTAqINzTd9s1BaXpBHEYg5LYNunBxfZSSO0mzqsHnBNR1I3VKOqMlF0TrLQzMqkiy4LlGK+YDWJTmSopjOQ+siQEHKupH7QZY59as77ApxcZBLnJDnmYRr1s+GmqtRBtlyZEoyIHDQ4VINiEEY9iq5iQwKZgcFsPmg5KIYu20TRRUpIoERg8aJeeFSvfCaKYFENMBIIaQha9ouhG\\u002fW4lpoUEQQjAU97UEQQw5iypE1fW1ktZP0ZV1CZxVqFxm2vkA8obDh0YYYSKyUkGOlahrLemAEEmHgjF2hUDAsXtsQhEpe1Mjk2RnTS8j7p7jq1vTkr6pqVnBqUodwMojZXohmIMSgrSgSAzFxClrHV8yVyz6D+qBr3ACBKym3vos5V1g7mMKxKSiSqMFCiPIimuRHlnkGDUUXGBoKBdNqrydgAM9OUNc3paGGoeUMhtjQgcDJmNVJkGSebL5FnougPShCn8e3IWgYaZxhVlAhkrJTAWdAa8yOb8agwWijN1VBHYsqe0KSURGd9Ly\\u002fquqojRVIxqAHsPCry7ayTp+WqhnIkZTMkIseOKnNd32loP5VQqCiTKjFRU5DEd10vq2JVa02ajJWggHOORuq7oZunfixNYQSseLKnZ8nBqPYShGLkA1ynUS4uoQYnE1JDDOTY8chcJ7QbaZBqNo5esZaBKvKd0MnTchXHGSS5qGDmICVlndBu+IVojpMKOcCUzGUaquhm83aQ5dFIUgpcw1iVhBlSmduat4IslkOKycigLppjIebask1ZO\\u002fDyqM81GSuMoI7EpVCq25y3gttVjCSZo2jKCoYQUW3ZTGjnvFQOOBJYDSse78VzlEwCVFyZ8s1uEwuXhZk6kMJF88xgqazR8+2Ml0YDqig5NYwzCKRUvzVvZbyeIXEyMWNwaa1Oms14fnlgI2ZJ5GDqVWGhcC7b6jr1ANFUsiHASHBq3mell2zWt22Y1Cn5mgyIxBE+hNqFfCbr6EDVGQkrjNS8KlEoHDqbfTsWppQ41AY2BRv54GuXNWazlo40irGwAtiHZ6SSLHOudC6fbWRWDonVZckAqpkTheBrT1mv0UkDU6ck404bTs27cYbQTsV6BlF1QCa+aibpNNv1AGpkCnMVm5GJmFsaDgNZ1CgDITjiuhaDGZPxsCqZzTTqkhkzkRqD1YSUhtWIBRartGgmAks6boAE0h8WgSxZomUiElBKTCsLybAqiE1TXS\\u002fBhM2S7ssjVmagptFwQKxJR2WfEDgmqDM2CCUuqpFzpHWVFtREgGSMtQwVs6mmiQyJxFhUYjnyQeqkWFJiR6LRMTkGfLBcuC9lNiwGubVEmSiriMXEebjIIyu5KMqGNkkgJsYQExfMR1dpyaOqzrVlogwBnAN7ZxnRAGUYFoNmajqwkEsEUgnBXAyVVlRUMdOWOWOI7cODEps0ggr8QDSWyVqxBWFSSkycyGXqay615LKsGmhDIhMBaxloZCUPi7I5zgA2MTORRgXVAUrXHxaeeuKNnahDTOLh6sJpY8kIZkUqPWKURDBlEhQkS1liOLWiHjGlyKYMZTiUfV7OaoFLNqiH4zYNFJUFrhpImfeJWG2oI0KsyZQoEQuqgSyFWswlG9QFq42fij20R9F8pqTLrm7ullZVyXIcIdXjxUmZnJXLm\\u002fohCrmIQTVkI4UZ7ZFhGEeMFAmqnIQFNpSh373JpOZRPUKdaiTi2sBkjKLkUWkltQrn6iIlrZVgzIAQKsRYR8tGHLQs2TSyGYkpk0YtYxmRlc5bXZmZISkYxqhLriutqFmIi6NoiIlNyWkSWNQ6lpGyyodU1zBT7M3TFS86dqrMqDGisrLWyElKJhXUzDD+LlWq6zJaXjinVWVmSma0miHFOiEbsdeyItPIZCSU1NQMqU6pOWpmdZlg0ZiDOFMkyxpZnsOHkQz7VVVURApikGgkCT5zIrnKcFhVZamAEdicKTvnGiG4DKEoqlFVGyu8MMgifO6zQOIrKfplXVTKBiIyB2XnfBaCC+YHw3pUVuPndnt61aisjI0cQ2uO5DLvAnwWfVHGoigiAQQxSeq9c5n3HCwvympU1crj5WQlQ3DC2UqGaryecyJoLsQchPOULQ+LYTlUgroYIkOKzc0sy7JW22i5HtYD0sgJykrKFvI8a4R2mwYaB3VhNUgMnNiQQqMRWq7bokFdDarSkpqj6GuYq\\u002fNGnmdZq22jWA3igJKyApQAxJDnedN3mtyPsV+NNJox2UN4akyWVHPU1HCNNm9qkMZysRik5E2NWAlkvtkMjdBt0aCs+lWJqOZNWUllIkNaz0ChFhW4Vp5njU6Dy+W4WNeGCDEOFeqkaQDirLmJ42B5bmm5HEUyD1JKKRUlhazR6VI1XNq1uBwrKBGZUqxjMYTPQreLeri8a7EfKzOCuZpSneqCRPJmj+JgMLe0VFcJKmRKqU5FQSEL3S5XxdLO5X6szEC27oU9vJREo9YlZY28N8uxXNrdH2hUNWZTxJSKZfNZs9u1OOzvWFiuSxAAUooprmToUTVcXs+giS3W1WIKzVZrlrQ\\u002f2Da\\u002fFEvnCOosUVb1O0tt33AN7fd37dhWzscRlIyQKMSBX8xCIEmjxd07tw+WqhKmbEgcrMwXmz5jqavFnbt3DBfqEmYAEoXU58WmzyTTwdLundtGi1VJCgMS+zTKFhouY66rxV27dhQLdQVT2qdHllwqfH\\u002fR58kV9fz89geHc3EEM4AihTgKg2WfQ2K5uGv3zuFiPYICRuMMS3kIJHFjBqnJoa6K5WZbsrKan5+b6y9XtQJmzjwliYWfg+lQ+6Od89u1qhRCSVksOhtYRlXsy2K1fWFnqio1ISNjJIpDnbOqXg4L1faFnTFGUwZIvalY4eah2qelcsf8jlRVpgI1EiQXBzZnder7xerB+V2pjqYCI9vTi6seJWjtljR3cWbY3F1u37XbNEV2yUiJok9Dc6i29P1CtWP3Lo01qSgkiUWHgQVUccAbM9SEJKkgT3FLocPBrt07VBXwpXMkQimo51an0WyOsDRarAsolAgGEyVharea7azCoFgqK1RkIAOpREfMrW7eaFbUHy5VJZcAjJGEVOC42Wk2mqUtj5bLgtQUPG5ghVhanbyZ1+iPFquKKgMA7MuLTCyJXafT6GRDWR7Nl0kroigwTpKEhNq9LG9VNiwWRzVVABmtZWi1W628XM9gbDAxY\\u002fLW3JyF9lBHxUI9otrI1ZIotyjkoqeGBeI4Sko6IqIEJpMaIoklh2NfVRUUNUjJiEwihJQ5h3hX1iWpqlFio\\u002fFTZhcdGhQgcRTNUI77YTJO5khZMhLxZazM0up52IenDGVidT4nx1VZmymganAKiWBWlsyc82VVkiVSjB\\u002fm7yNDDEmZGM45IRdHyWREiUyVhTIoVIiM2eAiJTNIgrGRgRPAZERiTMnUTJJJ4sRkXLMJyNgpsapCWU3SeIMwSg4EYiMXTc2IEoGUyCiRibGJS0xqyYzVOLGR0YRHvNFTImUyV2Zi7Cqu1GkiGFF04EqMiRN7MzaLRlCwSiKAE0zGGYiTmapTXclAChUyhpcyJKthREnUJxUKZsxQUnalo1pUPRLBJBHIAFaCESeCio3\\u002f72o8vtCw8gdQIowf0YwfaCuIoTB2lSCJqpACKolBZkQGMiIjxcbz9uElKUP0ypUXmBmUAKMkdWbji96MbPyvvPE\\u002fns2MVr4iK2iPDInJOJkxlAGBGYEsBU0AQf8P7Ola0qxJv0QAAAAASUVORK5CYII=\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6e07da08-5ef9-418f-aa3e-936e47bdce19');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=02, loss=0.7527, step=180000: 100%|##########| 118/118 [00:03<00:00, 36.55it/s]\n",
            "epoch=03, loss=0.7327, step=183584:   4%|4         | 5/118 [00:00<00:11, 10.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7344, step=188704:  14%|#3        | 16/118 [00:00<00:03, 27.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7106, step=193312:  22%|##2       | 26/118 [00:01<00:02, 34.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7170, step=198432:  27%|##7       | 32/118 [00:01<00:02, 38.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7179, step=203040:  36%|###5      | 42/118 [00:01<00:01, 42.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7175, step=207648:  44%|####4     | 52/118 [00:01<00:01, 38.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7297, step=211744:  53%|#####2    | 62/118 [00:01<00:01, 40.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7340, step=216864:  61%|######1   | 72/118 [00:02<00:01, 43.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7186, step=221472:  65%|######5   | 77/118 [00:02<00:00, 42.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7246, step=226080:  75%|#######4  | 88/118 [00:02<00:00, 43.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7108, step=231200:  83%|########3 | 98/118 [00:02<00:00, 42.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.7242, step=236832:  92%|#########1| 108/118 [00:03<00:00, 43.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.6927, step=240000:  97%|#########6| 114/118 [00:03<00:00, 47.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"db8ab21c-aeb5-478d-be70-315222696b28\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"db8ab21c-aeb5-478d-be70-315222696b28\")) {                    Plotly.newPlot(                        \"db8ab21c-aeb5-478d-be70-315222696b28\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAXLElEQVR4Xu15abhtVXXlmHOttbtzzr3v3ncfPEQgRrCheYIESaRJ0QkqoFgUnYrYQQgV4AXpYpMQisYGjRiITysSEUExVVjyRDSAIgaxiBCi0aAEPzoF3nu3O+3ee60568e5zTn73nfkq88fZX0Zf85eY5w1x9xzr72acyjFf2A1cJX4D\\u002fTxW1uYPX7yaJX6jeK3tTAfv2uXR6rcbxTVwuy3qbVp3wr3\\u002fx52uOfMyX89u8r+RlEpzIbNp+lpm4e5QVzS6hxS5QAAjfVnXBRXSQB77PneTqvVarW+FFWlZRz2i5dVKZjJyclLL\\u002f\\u002fyTn\\u002fXmf6zqrbHdQfgQxdvq9IvFLX7Htutyq2AHWr93i3j2izWHvhwMUQv4e0XCLRKAvidjQfuBex4QZXf821v4Z1EAeCN1140v0gfPPm15e8A2P+hoSZ2iX7\\u002fD9a8GQCeueb45o++N6wCk8cAz9xbZV8QdprC7B+++mfTVX4FBguT7fu59cC\\u002ff\\u002fzGu\\u002f\\u002fyIwP0AHZZbVDg5eecmtDTzVe85TPV+fCyY5av3\\u002fr57y9eHrL7UGF4t11osP2qO8YXruSy9pd\\u002fNfuzQRHAHjcQThkxrM+LXn4KHt2\\u002fSmPvP9oVe+yCj72Sfrly+B5w2kF74tJfHXTzg\\u002f32YGE+dRIA7Fu\\u002f79C9BtgBHH42Hv3Pz1fI8ctPbACPHR89PLW2IuGeY\\u002fD8jaQ4cPj9e+sPhprr33XLUEWfmh4HgAdn\\u002f7C4eZBfxKm73HnuL6vkIg7Z85DjCYrdH3p1VTr0DCC\\u002f5bD3QW9a8Rqe+NEp+u7UlaC1p\\u002feJgcLsdwzhvm9c+ew\\u002fz\\u002fynoQe4hNd+Zhx\\u002f9WSVPf6dAB4\\u002f9undqwKAz96O8jkAY\\u002f+0E25ffl8qE9v1eGyoPf1nr3\\u002fkGjxybHvPPx7iF\\u002fDtDU9csnpd1n\\u002f+JRir0cP7AuBaVX3\\u002fRty09a+2bvja1JbbKpLd\\u002f7rse1ffH3\\u002fhSCxmuVyYDZvH9JtnHHzZDVt\\u002fJMfs+89L\\u002fDLeuh7f\\u002fWKVxAnAEz\\u002f8wNNYOX8C\\u002fun+55FrgGfyRXafHRav+hjHPcPE7fc29znj2jZ+8l+HeQDAsQfo\\u002f+xWSQDA4de9GABevW3tTpt2xk+rci198i+exUsvWte5sleRTr0ed58+jxOPxDM3LVBLhdlj4\\u002fjWZ7\\u002fYuvNOAEjPe+civ4ypd8jsKnPPOe+6+9+3AKjc7SD+yztT4PKl5tHDx5AddkN1AMxjHmd8RSosAGDNQcDsMwDOeTEuHZY2vhjIP\\u002fDgz7DtnJ3xxHuGReC2173i8vPHrn799Ec+U1H+\\u002fEL9zGXzwEXABVsXuMXCxFce03zvQ8niV3dZvFjGbjcDf\\u002fOdKgv86or+54HD9DJOed\\u002fvOuCRconYY\\u002fhpXrXjz1uDbQDAFfsdcvhdVRIAwn4s3wPO1bN3xXm7D1b0yNcAT737+wDwImDz4h0u4V9+8IrDDv\\u002fILrjybyrCpRcWd32gi+SIXejqpTl9sTD7HoOT7lskV8VR++Db11fJBfxxjXRvPDA8pwLY7dTDgdcqMP\\u002fBbw4O\\u002fx8uXY0ddcqRuHp2WVpA+5z7r7v3oU0r9wYHHyRPTWPDa9+I9jMvu\\u002fn0gSnv3AwPXPl9ABOvOxgPfHNZWEDexPpbSP\\u002fu9gq\\u002f5ky96yTgpTfsh9s+scQuFuZqum+5LiwrZ9\\u002fjLsf9752rsgCQvfLSY8CCZ88KFWXvWxdH3j9+bkiYALAPH7ZzdDJ3H8ztw0NiH4+fuem002pffLZCN34Hz9782B4bj916zyfHvjE2KH1u7fwZzwHAez6En7ztuUGpjycB4M5PLsx7S3BTuGDd6W\\u002fcs656S3uJXSjM6zfo15c4iP7LcqOP3W4BflFdqQHAveqW9d1nHzgqA7\\u002fpuuq+kAgAC\\u002fCGoweeYE+vvRDYh3zn32586L7nf55Wtz8AgK89ftVhl+364cr08wcfwd9etcOVxzRvu2T3a5v3Dq6RX\\u002f1q\\u002f\\u002fMNl8L\\u002f7Sp1MQcR8I0TqzTKLet+qsCvmuu33rHMLhQmjZ7\\u002f+0Uqfj++88HFxiL+VIBrqiSA6Kgv4Yp7vz95x15Y95dP3b608gAAfnz0qf+QA3jH2UP0eU8eCOCpr\\u002f\\u002f0fwPAu9b9Ykhdwo\\u002ff\\u002foZN737pscPkPsBV+NIBOOm+A+\\u002fCX1dm3z5uVVwwPED7uPFNitX27bMn\\u002f4\\u002fJxzffNH3j+qUSYGBVKhYHbXzxxmeurc6GG44ENld3oADcBzbiW5+enbpt7+Ljex17wz2fmMHgoffJDwMArhguzHCFD8NXB5sDmL35envwod8d4sZpMzbsSpfct8cNdMlfD0kLuIwF36uSwE6nv1kf\\u002ftHpq62dD+4KAAcfIoNPaKkwi2\\u002fSho0nbj51kVzC7RN48MwqCZgPnd\\u002f+0Fdm9\\u002f\\u002f4vo+dd+\\u002fY75\\u002f8xtvx9CurXwKOrBLDqM6GC9jnzftb\\u002fLR6j6qA6t4\\u002fi584cunsNYjoVaLvG94wAgAO+yAu+\\u002fRxp6\\u002fY3SwhFf3KQHOhMETHvQ8AcO7F4196z4C+gLWCTdVRBOBd53f+5O4DTj86ufILT2P+W9866WRcvCS6I+7tr0TvWGX382vxsrOPWw+EZyubma9vPPbAfep4K229orr5AQBkpxyBW7+8cgd06Mdw4rd3vBRPVIUl\\u002fMNwc6Ewqjte8\\u002fnp15y6Yeen7qpufwBsYuCBKgngUpjz3\\u002f9S4IqP9hekW29d1g668IhXPg1g8uirMnSre80B0O4rlnnsePJZuwF46MMDSwIAoOhkdyuA5m0r12MAjetOwEWfXlkXHD5+3zfc68doxSFpCUcNN5deJXPmm5q7Az\\u002f4zvIOdQkbjpBi02pL0nPr4g248x9vf6K6UAO4Zi\\u002f8aRPA4fspvvvZET8SaOXoBOyw5zUvB\\u002fDgJzavuMWH33HuocBN\\u002f\\u002frI6puunU\\u002fA46tutlTVHfexmRtWeeoLeMlwc6EwP\\u002fjh\\u002fli\\u002fI7b9ff+FqmDNDvjlqgvA647bd8vnZ6uL9BLe2\\u002f94\\u002fo4LRwwY4DVfGGpOfmrDSwA88Mm7VjsS9c8s28HL\\u002fwQ\\u002ff3OVBABMYevmg3DWwHpcxf089BQWCvPMKe++BLjuv68ya41C8+abq9QSzvqjtwHA4537b\\u002fhxVRvC8GbygI2\\u002f9yIA3es+urzZesG45ER8esXxHwDwKE6g6U3frtID+PFjL\\u002fndgWPE4qv07BULR57V8OgDr61Svx6PnP9Pfz5x+92bV9lqDeFbbxlqHn888G93hE\\u002fODrEvDHs28LnvVMk+vhhd8tDXP1Vlh\\u002fDR6\\u002f\\u002figuVFi9IB6bcd\\u002f+28J09YZbP1AjH2hcP\\u002f11lLw3TFxPfbjLtx8f99XTD\\u002f9s++adel1v9XI+Y3CQaBQMRQQFmIiKBEBAIpMQRkRJjIeEtEBgRVIgWBAGJVkLIwAcBCP1KogogFQVkNCFAmUuq7bc9vpCYgIVj1xMTBAUqwUCEiAASQkUBEJIwlp8W424lZ0TCgcf8mVJRJoFAxEKgSRNWoEBvy7KTwSiCiUlUZIgCgIBUY8qqQYCHa7wciJahoaQkcfBBigSgxVEb5jdKYJIAQKNbSkxYECl6ISIMSVEHq2SJ4qIB4IRcZGbOq6aCfsBFm8swgshlFHLFhK9awqqOSA7wBkVBcknUK2GCdg8IEYiqZlNimFHPMhkywxojCCTMFpwSN6pRRRBE7cWwwym+kZlxpwZ6IGVRjdk4cnEaRA8gEJgogJdgM1kZgssH8upira8EaVkoZCvbqKGhKcGrKAupZSKDECi6RIEdiNCltEYIoEBhgVVrolzCcGp9DhARQNcKAiKWA2Grk4yJn9cKBhFixPT+MysUIgT0bU2hsJIIpO1ZF4A3ACjUFOfKaWHViixLqWRQjYy5qEYKmrMsaFJSCTGE0CuxcmqVRGZq+F7rGC1RcnuUU6jmZJEvTLHRbhS+6VlQLQ2oLo04ocmkticrQLHPftT4QyFPUdTASWZNF9SgU7SLXNknJpNv3wwhNSSUSoSgwpXGS1sq8GfLQtVKQgsSVLHEgE6dZmhS+WRa+Z7yQYETMUZoFqYc4ieJ6tma8FhdFa3auDQ8AprQFJCojNxlPjKeZtGba8y0qBIEBeIjTKGpka8ZrcZm3ZudbVIIhsFqwOGPjsawxkaW+3Z2ZawfpWsgIPx2hQZVzA6YomnATjXqi863m\\u002fCykhIJApcBp5MayNWP12Heb83NtLpUUo2Ku1Jpzi5olcJ6qkXR8fHz9eAPUno8t2VYQEjgtYlikE7Wp9eM1cHdsa0SABg7L\\u002fdaMrV9TB7fnEkuuKZ6UYPKad1ofa6zZYaJO3JmOyNh2VLCYlX6JJdsKSp5Xaku5MAhWI0kajXVT4+Niulu2GZ7rWGFPxHmiBtlYY2rH8Rq4PbPVMostSUfd30q\\u002fuX4uQmKVxYrhRn1i\\u002fdrxsYhsxh66hZnVaACL5Xpt3dTk5FjEnJO4EHoIRCCxygv9GhHbjEvVLcwkDJSRt5zUJ6emxuo1wAYJ1DUM5VX8FvqpkZXaci4cTHBcH5\\u002fYYaIxGRtJBEZ7hQgxVIwYU6tNrZucaETMKQtr2WMWrOK3FHPAb3LNWEQm47CgsYWoC2STxvjYmCmLqGM4cc5QQDA+8kmhJq2nk+uo6CWilLWdMcGVpFAni\\u002f18GXUMpe3IcoAY9cyWKKvVJyaMtjggcrFLOh6BVQb69f3S9oLfyFwMLDPX4mxs0oV5B4kSsmm7tAHKiIRN0qhPTBpfOGFKrLNUQlhHxMRyLuOmLKJgOHHOUkAwFqTeOGvjRqLdTqkcmeALCqIsVrRn2Zp0Ki5z7hFFiUheBpQIrOSNMzZqpNrtlGKdEd9DKcqBjQbOGEndlT0tOSSh9IUvfP8AsuCXrvAbmQsHb53VeCrzOXqGrPGh6PlAAoZ6Zmfi8ch3KQdHTnyuAUJCI2IO+Mmyhr4f4PKIWFxqm9oWjTNXSt4LBKh3UR4ziUuSOXQLcnXJy0JK7wKBOY\\u002fISJyalrRFkiwqQtlb2BFHnTQ4cUna5nlBpBx8kTOUBGQX\\u002fMygn4Cgo3IRdp5Zg4uTjja94QYXWniB2gByZWRJoiRqaSsgqodSyiKANYyKOaC1qhpDQiSBspgLdLtdihJIYUiJRNkjzoNNneuh0+qaOIYURLAERgiRBKo5zrXb6XKciJREgSEK8YkHNSLbok6vizhjzYnLiEWX\\u002fMyQn\\u002fT7bT8XFMEEdY04aoZmp6eJYy0Y3rIahOBCMPXI9rTTzTmJKRSkSpCR90fLGlU02P65gh0Tha7W1mbUDb0iRyCCOCGKDMdebEeSqczOh27ZISFRJmXAWEvku6itzbjte2XOORMR+whqHJMJLa2vzTiEIuSmUAJIiYgdY8GPO0t+o3KBUzVi46R0oSvpuiyeDe2iZ1txacAKEBsLUA\\u002fJutS2pVOWFAx0VEwlZe1rvqvZVMad0C1yCkRqSSGwkXOqZTQ2VocGQNiWJcMECqCkwcQtOzWeGQZILdkAAhCQWOcUPhqvN6CeEdiYklWJBIaNC7ZI640aUY8QIp8LQApBOujnadFvVC4U2GtmmYjztD6eGmvViutxTsJKgayLGZpHY40aUQ4WJlPS6JgKT0OaJwgbLhksxnlHGTGVSNeuaaxNQmDjhC0QrPUxxYWVvExetKa+JuacjVEYw2qcd1xTohLZ5ER9bRZKYyNhA2VKg5NYrQkUT47X10TeR2y8JaYFPyz5pcEv+o3KRWOnRuMAzntmqlEft+KNNWoYRo0NFpk4LX28bry+NtFgjBUypKNiinHBUUZmWfNmQWMib8R466XQepZa3+14o3CsCoaPwEWUF2U0FmWRFE1x3kTWF8TwRoyPgpRSr6XWdzqBVSxDWbhrkCAOWvixLLKSF95AHVSI+v2cl0IbWWp9p+ONwrIqRuUCKQxZH4duYddkaRS6HY9AlpVB6g1s6UofTCPO4tBrFRw0NgozKuZCLrZc1HrLmiUFCYmhtrU+Z9eZ93GwIE9EwSqCUcOFxGhKrbUV8axrcm7IM4BAwaDlXJkjbs\\u002f5OBgiT0QgQ3lDuGwaV3ZFus1O1rSAN0EJy35lseTHnoh0RC5CxIURSz2Oy45yazok81GLghEKTkgoGHQ49l3W1owmMxE4mIBR96cDuficXWduWbMaoA5xj8R4mg2Ue2nmLfHqQdwh43zWs4HREc4LRa8s1ZMEQgBZSnukZcQzY51ekGbR1qAepB7kQlSQgbOUsoidLjsAvBoJ0GjRb6Yx5IcRuWgQiXzSIjHBzEgnKHLfFTK5elMSW0k6pBTsPLq5SqvswEuA0IiYg35c0SyI1Tp48lxQ1GMYFEHL0ohKSILENhQ5RW1Nepp1NS9z3+OSgiGGserJ27Ibd5kYhdeiZIJYtcGlpqyVNrRj41Sp09OiIBYFsZpV\\u002fJhVeEQuJLZ0EYJ4V7aT3ESkrW5ZdIXZK5PaSDyKpGhHPThoUfq8ICWlETH7furJc9F2w7lYgoBYiRLj6g1r4nlVJSEC2ChpWQfbNNY1mSMbtQrP3sCLRb+f4Zhdo25sPA9VUmICC6koi9PYuHrNIm23iACoKi34cdWPAR6VCxMTqRibRdRoEEcOIIu4LEpDEFLAcGptvW5M7BHUKDPpyJh9DQMalnKxyoHhQ2QzNxbVHLp5q9f2OZWk7F2wqu2JOE2TrJZI4du9bu6pZ3sMCowQnE3jepJZ6hatbifk5DkwENh4TyZzWVaz6rvzRdv3FGQEfT9nM9eIaw7dotVr+4JKDiwjcildyYR8rUmiLI3TkOezvVZeUJdZxYAUPrZZlGVprL2y1WuXhXrjadT9Dfgt5NLs50LKFgB8LxFQHDVi6XZac\\u002fPtVgiqkLhEsL1GLwaltZp227Mznc5sUJRWQQTfiYOyS+px6HaaM81WqxRRFRjvfDchorSRZr7XbE3Ptrq+EAoM0IJfFDdi6XSac\\u002fPtVikCyKhcHKkpbFrWENfTlPNma36+3SpAymoAlW7sQVGtHkunOzfbbLdKCUT9w9l2Yg5ocSOWbj8XH1QhVkzUs0Uxn\\u002fUaPuTdmenpuZlObkqjJN5qUXbj1ng+Hqyfm5vZMjvbUtu2RiBserYomrW8UYZeb3pm2\\u002fxMJze5UVYi17OSz6Xe5EmvMz0725xrFRQ4RF6MW80vN0KiI3KBk17ay1tjgTyXrdlt03PN+dK0IxUWy7nJ81Y9rxehaE\\u002fPbJub7RQmN8Iy6v7Umu1pahkSaejCFMV4l6db7Zn5tnBpBGCIcVR2+blxGctlbr47PZ33ghoBvFGNBB1wnvf7TTc7gUtWKAIjQTlP2\\u002fKy7sNMu7ul3Q3qDcjTdvxYANbVNSMAh5JS0g6e64Z67mebvS1zuYpyCQhDIpUOTJGPd3lbuz0z3ylC4MCK7fgZAVhVq1onUGkEIEqJbO4ocaltWO6V89JSz6URUfZObFCXuMzWY+r2mtqRgr0JChaCLQ0nLnV1Sz0\\u002fL02UpjCiapQlyi3VOI5rznTLZmhSYUuQKCmRKbbjpyNyUWhUsk2pHmUOvbztOyjZGy9gYbjccmIzV3fU9fPaREmBBDI65ohcKAGrGALHyoaDL0FBglFAYQo2wQmbSK1R9MQnvZwsCZRUGcv9fChBXoUAKAjKwTCQCJP1wZN4JRIojIzwwwiNlckbIqRiScWXGvUUJBwYUEJwSpyIZfalVxYvRklAOiLm9vxIlSgjBEMarIG3hF5cmF6kUAYUZEoGvLNaOic9V3K7pkEjqO\\u002f\\u002fH6PBWJQRtJcUphcpBQMIDLwFyhiAJSmi3LZThRpPBEJgXtVPeEQuLK5kUm+NiDPSqXVcJ1UVp6oE9kzkrVHvWHtxbvJY+v1oREyi\\u002fj1YA++AXlyYPFrsl5EqAYACLKzg\\u002fm9G1P+zkcGBFLYXUeAAo0oK60kYisV+Jiz3g4JA\\u002fR9DSFitKIE9sagSARjh1++2HQ1KRAGKqCSAvfWkoL6+kIuCvQFBjIBUwZ5Hxxyh\\u002fR+qeax2t3QkbwAAAABJRU5ErkJggg==\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('db8ab21c-aeb5-478d-be70-315222696b28');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=03, loss=0.6927, step=240000: 100%|##########| 118/118 [00:03<00:00, 35.72it/s]\n",
            "epoch=04, loss=0.7178, step=243584:   4%|4         | 5/118 [00:00<00:11, 10.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7272, step=248704:  14%|#3        | 16/118 [00:00<00:03, 27.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7075, step=253824:  22%|##2       | 26/118 [00:01<00:02, 36.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7158, step=259456:  32%|###2      | 38/118 [00:01<00:01, 44.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7219, step=264576:  37%|###7      | 44/118 [00:01<00:01, 46.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7172, step=269184:  47%|####6     | 55/118 [00:01<00:01, 47.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7160, step=274304:  55%|#####5    | 65/118 [00:01<00:01, 44.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7064, step=279424:  64%|######4   | 76/118 [00:02<00:00, 45.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7039, step=285056:  73%|#######2  | 86/118 [00:02<00:00, 46.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7098, step=289664:  81%|########1 | 96/118 [00:02<00:00, 43.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7152, step=293760:  86%|########5 | 101/118 [00:02<00:00, 43.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.7113, step=299392:  95%|#########4| 112/118 [00:03<00:00, 43.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.6966, step=300000:  95%|#########4| 112/118 [00:03<00:00, 43.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"9dbea865-58d5-462e-bd02-92295b4a0182\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"9dbea865-58d5-462e-bd02-92295b4a0182\")) {                    Plotly.newPlot(                        \"9dbea865-58d5-462e-bd02-92295b4a0182\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAaL0lEQVR4Xu16Z5SsV3XlPufe+31fhe7q1ntPwSAEBoFGAiEBRh5ElAgiCDDJILDIYMCDLAFCJBMskRGwCDIaY5IQyYNZiCADEkFgRA4eogU2QqDwnt7r7gpfuvfs+VGdqt4zsGbxY5jl\\u002fafq7lN796lTN5xb1VLgv3Ag6DzxX5jiD7YwR\\u002f7oJ\\u002fPU7xV\\u002fqIU5\\u002f7LDvzfP\\u002fV4xX5jjLxxfeNwc9\\u002f8eDr786Qf94C\\u002fn2d8r5gpz7CdP42mfnOW245xxebd5DgDQP\\u002fSJZ+fzJIAjj35qOR6Px+MPZvOhLdzrP249T8EtLy+\\u002f8G8\\u002fdNi7y30vmo8d+fY74yVn752nf1f0vvyzI+a5\\u002feBnRnf6wIDDZscJ32lm6E38xXMNnCcB3PysE44BDnnOPH\\u002f04x4uf2QEgAe95bnDDfquB3186zUA7vjtmSEOz\\u002f70LoM\\u002fA4Br3vCQ4fevmI0CB50C\\u002fOqL8+zvhMN2YuUed\\u002fjpjfP8fthemO5x7zoMuOr8913+8tduo7fhZgeaFLjNXz2mkGuGRz38wvn98BWnbD1\\u002f7Lu\\u002fuvH07reaKYze\\u002fHDZPr79pwfrz+zl4w9du\\u002fLT7UEAR75b8OefmCO34YzsqEfjJ3eYp3HMM2+GIw\\u002fH64+SX+8\\u002fff\\u002fktLsejXOuPfHib0zH2wvz1kcBwPH9K+5+zDZ2G056Bn7ysBvmyMXzHrEAXHVq9t2dO+ZCuOwU3PBeIU6YXX+P\\u002fdrM8NAnfWCmolfvHQDA11fv0Vy8nd\\u002fAaYdf+j9+PU9u4G5H3+0hAuJW3zl+PnSPJwD1B+75XPB9+y3DR7x+p3xp56sgO06fEtsKc\\u002fwpgis+9aprv7vvnjMf4CbucuEAb7x6nn3IEwH8\\u002fIHX3Go+AOB\\u002fXoL2egAL3\\u002fojfHxrvcxtbBfgqpnxvhc84Lvn43sPGh\\u002f9rBl+HZ+\\u002f\\u002fS+ef+C6HPrem2PQk28fD0C789EXn4mLdr9pz7GX7Nz9T3Mhf8e3d7\\u002f8qn\\u002fJL7o3NrLcKsyxn1zkPz\\u002f+ri97155\\u002ftfsf991NfguPOwxfev88iYcBv\\u002fjmi6\\u002fB\\u002fvsnEK+ZPt5nGfhVvcHe9uCNZ1Ms4rJZ4pIvjI59wpvH+OGBCvOgO\\u002fN\\u002flfMkAOCkt98UAI7fs\\u002fOwC2+CH8+Hu52rX3odbnn2rsl51VzoMRfgsscN8Yh745qL1qnNwhx55mDPdReNLr0UADpnPHGD38KOx9vKa+ZJ4JlP\\u002ftzPdgM4ZD6whUc+qQO8YnN4SmdbDDj45pifAEOs4okfsTkWADA4EVj5FYBn3RQvmA2ddVOgftE3foq9z7oJfvGk2SDwT\\u002fc76twzFl9z\\u002f72vuXAu8rLn8R0vGwLPB56zZ53bKEz+qlOGT\\u002fnWZsaHbzzZwhEfAC74wjwLXHvu9PGEWXoLj37eHwfge3GTuDV+uC2MVx\\u002fyb5vn1SbOu8PdTvrcPAkA6Q5qXwaezWfeDH99y+0VvfedgV8+6asAcBPgkv0Onu9\\u002f7ah7nvS6w3HeBXOBFz6v+eyLSxQnHy6v3tzTNwpz3Cl45H7H4gzuczt8\\u002fq3z5Dqe2RMegytn91QAR5x2EnAXAmsvuXT79P\\u002fW5rOF+z7m3njV6rbYFONnfvVtX\\u002frWO\\u002fbvDe52ov1yL469y4Mw\\u002ftWtLz5925Z3RhdXnvdVAEv3uyuu\\u002fOetwDrqNRz2QeG7L5njB0\\u002fjZx8F3PJdd8BHz99kNwrzarliqy5q++++p56Lf3nK2jwLAN3\\u002f9sJToIZrn5bmIsf848bM+8o7ZwLLAG6n97pJ9mgtv1H778wEp\\u002fj5Uy887bTe+6+bo\\u002fs3x7UXX3XkmafuufxNi5duHOsAgHfuWHv89QDw1Jfih4+9fntoiqsB4NI3re97m8h24jm7Tn\\u002fgMX3yA+NNdr0w9789NycRYPz+1mCKIz4I\\u002fPv8SQ0A4fYfOKy89sr7duEe8rb5vlAEgBrwgPtt+wRLvuVs4HYSJz96z7evuOGqznz7AwD4+M9ec69X3Ow1c9vPia\\u002fDP7zy4FedMvzoObd8y\\u002fAL28\\u002fIj31s+viAFyL+\\u002fQHq4k4U4NMPn6fR7N71YwK\\u002fXjtsz6e22PXCdLIb\\u002fnGDyl+EL7xkY7CB5xjw+nkSQHafD+G8L3x1+dJjsOtvf3nJ5skDAPjBfR\\u002fz2QrAE545Q59x9Z8C+OUnfvR1AHjyrn+fiW7iB4994IVPudUDZ8nbAq\\u002fEB++MR15xwmV4y9zuO8VHiOfMTtAp3vtQ4kB9++qjPnrQzy+5aO\\u002f7DvvINnbzVGo2Jm1+zlnXvHm0Qa\\u002fj2HsDl8x3oADCi8\\u002fCZ96+uvNjt23OP\\u002frUd19+\\u002fj5sv\\u002fRePT3Fzp0tDN6wfXBPzHcVG1i9+AJ\\u002f17t\\u002faYYbyCU49gh5\\u002fhVHvluef8At7+VqOMBuedjpf8bv\\u002fOvpu+Z5AN84HADuejfb\\u002fgltFmZjJR175iMvecwGuYlPLOPrT5snAfc3Z45f8uHVO55\\u002f3FXP\\u002fuLCf3\\u002f0Az+Ba46afxFwn3liFvO74Tpu+7A7evzoy3MsCRhv99PiP04+4I6XHWd8zmzDCAC419\\u002fgZRc8+PT9uptNdIwHmDEipz4XAPDscwYffPK2+Dp2GN4xP4sAPOnMyV997oS\\u002fOKV45XuvwfAzn3nUo3H2ZjCc\\u002fMXpSfT4121yvztu\\u002fYwHHwqk6+aamU+edeoJt1vAY2XPefPNDwCg+5iT8eEP7t8B3f0NePjnD3khfjEf2MRnZ4frhSEPfcN79t75tNvd9JeffcfsKwDgQgWunCcBvBDuzBffEjj3ddMD6cMf3oqdePbJR10DYPmUV3dRzvea2yC32u+YxyF\\u002f\\u002fpdHAPjWa+a\\u002fAmkm3csJYPjR\\u002fc9jAP23PwzP+7v964KTB1d8Otx\\u002fUTb6t\\u002f0xN603l5J7+kOHtwKu\\u002fOJWh7qJY0+y5u8OdCRdvys\\u002fFpd++ZJfzB\\u002fUAM4\\u002fBmcNAZx8PPGlC3\\u002fDlwScuzoBBx99\\u002fm0AfP2Nn9jvLX7n9DPuDlz0v793gG0EwE0fhp+\\u002ffZ4EAJLh1Dfse9d807uFW8wO1wvztW\\u002feCYceghs\\u002fMl1Qc1g6BL8+4AFw7wcft\\u002fs9K\\u002fOH9CbWd6UbPvXc3zBhgBPeNzNcfuvtbwHgq2\\u002f+3IGuRNM7y3+C2zwb\\u002f\\u002faQeRIAsBO7P3kinrbtPJ7HV3TmU1gvzK8e\\u002fZRzgLf+\\u002fQF2rd+E0cUXz1ObeNozHgcAP5985R9+MB+bwWwz+Sdn3ukmAMq3vXar2fqdcc4jccF+138AwI\\u002fxMNn7js\\u002fP09vwg6tu8cfbFtrGUrru3HO3yHn85Mq7zFO\\u002fHd8745svXf745ZccoNWawWdmW64HPwT48SftjfvfEn47jl7EO78wT05xUfaCb3\\u002fiLfPsDF57wcvP+tHmSIptoT90nPvXVz\\u002f0AM3W74iF95\\u002f0sadvTtP9Nr4\\u002fZFyGs\\u002f\\u002fv64LhYy986M02R\\u002f9fzZjfJxQCGEATyPpGaDCBgABII0AVIQQQkoQAhHqxRhOTwkOgcAAQTKIkCAABKcBUB07\\u002fAAGCJECR6auE5MZt\\u002fjfpBNzKU0HQYAohCSEMAEWUkOndNQlEQAhkwxMbnhQIACjMQBJOKSoCAElEhFShiIeII0RUgAgxUAgVUFShqgbnFSqNU0IIEbG29bnkDBBCpdFJm8wan9RBKAqKCkWdCtACBIQQFUAhqkpxqgJpnKznKdt0bksnQojCRDfzdIC0EIMpRRWEqDhRExWBoBWagxjhxEQ2PHXDkyIEFAnOaxAvdNOikHQGo6oajATFROicOngKVZTTe6gQBEVNvDrxoAoUSejMC2JCijQvLrOQg0GpjoHC6dRQQJ0GDdCpp3B9koCKpE6dekIEAhNu6KDghg4KJUGKkcQ0T1XHAKGKTPNUMCWhR\\u002fRenDqoCAUJRp1OU6jQbfPE9BcyRkvaOklOVTl978JE+qlM1FQdHayTZJKzFToh1aAGsSAePjATqxWNwoFRNapJ1gSXt95PioljMYmBlDTNVsVUFA7MiDJH46BiUFMBxJw4BlCASqVWcWLC6TuI23TZhs4EFCidOCisE3WapwoBQimSnAuWgUprnTQQD4KQTU9PZmSVoXF0IKnmTAGKp3MmhjqzVh2FkosmqFhwuS\\u002fyzNs4rUqDWphEoIkO9K4nRVYEjtKaNRZJU\\u002fPRCYUuy7RfMGtL7g0tRt4YvazrfMflWR7SJA7ZsAFMMPWEc10tssLbJK2ysbi+LBIUcAfQUcSgYkGLrTxb1kASgWtFQe+6Ls8zz0laZc0WnPHUPF\\u002f3bG3qCdc6upi7jg+dzHNs+2QikaRIQXUxMMs7nV6v3xc3WhnVq2WqJJmaqjlked7t9xb6DsPhWr06ZqkJrXhXZxK6nYW8382RTYbDdneFElbmgE+eedbpdPsLfZHx6rBaK1NtBExdchLyotPrL\\u002fQdRqvDem1spRg4rxutbepIdTGzLO90+t1+H360MmpWNvIUnzxCkXd6vUHPYbw6rIZDq6fr5T\\u002f1NKpPHlmed3vdxb7DaG01roxSS6MXIQU+z3f0Fwf9fsB4ae8eN5w0pqCCYFHkO4rlxUE\\u002fWLlv3w1eqyaJZVJ2NBUL3JktLS50c05WV6495MbYtsynB0jIih29wWChHzhe2Xej85MGalAQCEWxo1geLPaDTVb27vFaNklserjM6Pbu1aluM8+dvc08d7u1SZt0ekghFMWO7tJSfyGk8eLePV4mbZx6crvnaGXfXnVla2rOkAw+yw\\u002fqD5YW+56j1d4edeNoEE9IEsmzftbfubQYXAq+ZaRNd0ghnHedYmHXUr+QFCQ2e2gUkk1uErpYDjt29ha6iLmm7BpfpeTNRCiahV6+MPX0vk21GZSgwKDBdYrFXcu9QmLQ2Eab\\u002fj0BRbPQP5AOkoR51g\\u002f9nUsLmZr3La+f5gKBwWW+01nceVAv05T72LakOSME1O2eLsRUw6iEKcxJyHu9xZ07u8FZ5qPtTjRI8hCQLuRFb9dyh1UjSme5F6EBFBPviqJz6KDjq0oBQVGUGinqE5FJR3o7lp2vx5kK2iCUvE0CmLhQdHq7Bl3WtQgD8qwUSxBqEu87nc4hS4WvSxUq8k4p007DUMzqZF0HIelDXvQOXu6gakSp0zwJoSY43+l2Dl7uSFOpJJUir1zc5tndOeiyqaaeQcQSxMSYh7xT7FoutKmcJmGeNS5SPChOINoZdHXSVCI+ZDJMiEoIJQDKXj9gsq\\u002fOYvBeh60mQAw+OThdXFrM2j2VU\\u002fb7o9K5KK2IqROKFguFmzS1MAtOxglp2l4FQNDpB4xXqixmzrlRq3HahTmhaD6rkwQITVUg2l3quElTiros12FCVCOIAAOLnseoqUMKzrlhQhRw4\\u002f3li51NT2x4UjMTYXeQyaipPLz3ukZGUvy0GYzwLo4nVZl3KI0xiUtKEZqzpMg4bss2y9Ub2QoopmYZpOz5ok4raaXbycpmbKkm8gZCVSRxIY2qqs4KkUhr1bUbnqaScdxMYparM7IRIQBs6cp6UwfXKtfzpHNpPKkmeZFrk2CipoSYKMy5nGtt2eSFOBoamXajVGUSF+KorOs8V0SkdU81UVARbFSXbejQWWJS0NFjeob6EGK92qAbOmwQm5ERpgombxqkkVFjeeikNpoNndHUSaNd5xAwmZh3feqwU5bM2tZUmQSmuY8c1uyEDtvEODLQVEnzSTO34WlTz0gTZQJMch852tKNDTQRmAAh+FivtOxmBRppqxEJU0GiS5JJ5LBlJ3SsiSmNYGKilgBK7lM7qtkJBduEdU9Im6XoMk3NqEHhC7a0eiiazJOORp+55Oq0tJh1rKyGNbRRAskRDLkwNGmxnxV+XJeVaQ2RKiiRFrMsFm2zOHAdjMdrmiyKCaJnCiFD8uM46Odda+rxBBKVQFImhgypV6XFXl74ST0pKS0ESI6GkB9It5mnaZ0Gg9CxslqrZTNPg89AX9piLy+kbiaVTC9tyZnB5Uyubgf9vMN1T2eUlFn0WVDzjS0MQkerclw6RHX0AigsSJ5L1h\\u002fsCBpb3wqcCpViCjotCtXewo7MWenMSRZJzZNTSPADl3xncUfBJvq6Dm0rXqh0lrwWmWS9xZ3Bta02DqEVUyoV9NopxPUWdgaXSm8OeYIpsKkLs7qkBBXMpMg19Ac7gsTWtQLnQKWawoIrCsmwuNP7ttHkLERJjqDjumd\\u002fcWembauNR2hhjj6pmpNOIRkGO72vax9dCq2aU4ACja6RFAYHLSwFia33QbwpAFVl0qSWLSwt7siQknOaxBSUWr00NraU7dy1MPA2Ct5nSdP09qpIrpUUBjv6y5nGNngHT8X0qp5cVAsLyws7cknROUdNiumtN7pWUjbY0V\\u002fOZEPnAEAgrWsQw+CghUHQNnofZNrSq4iYi6AfLC0elLmUnFNoUjMKgOSjWDbYsbCUSWyDW\\u002fc0SJIUGqFfWOovB9dG51UzU6oAMUlWa1nGLKCprSEKigYjxJoUQsJkHEOWqqotI4M6F0iBbyoLVV6P2+BQTjLUodc613WEWkwua6QuUxZQV6kmcqg6M8AifYgoJ5ZlqZy0ZWSAajBCLZrLG9STqc62dLqep5RlDBma2lqioLhgBBjpQouqZJaletKUEZmoDwZRi+bzGvUkZZ51ZdWm57StyBqZTCwPVo5jRQRAQqLSCIFaWWtbj5syVowGNTY0JHFMoa6iNm1ptZWgiCaCRjOnrvArja9iWVZaxYpOciutRSKEkspaYj1py1gikt4YQZiomW\\u002fqVppmYg1LMRGXYOCGrmp0VpcEZhSBs7LWth3XVarYGtSsYYKRllzTNGyqMpU2EQocSRgSIDb1rMq2jKVErHvSTBi1qRtpm3FsOHEt4CitmFeIuCwOUHmKF7Fxm2dj8ebM05to1i7LROmdE07a4MxCMgY2YoWvlnTobA2tt9p6ceQnUFVTUrLUkYkjnTkbNXkgXVQL5g2atUuYKJ1T4aTNnDEkoeeGbqykM8cNndATEJe1i6g8xVFs0hblSDwdlSqqeVyyyjfiRKxsvJJKMlCAqaeA3hxHdeaMLgodxJLvpqVUBoM42KQpSqqLmoImGr06bTCxqh5bK4IUG06bI1jwziIqlu3EahHWMdJM0RSMFnK1sC+NYpMIdZSxS0xmyRjUocXY6rpkI4IYW5jAYLDgHTY8G4E1sbVEgSXO6OptukTSq9cWk1Q1Y0YRpNSgFZJmDE7RWml1M7EWwhgjCMCMDM6hxcTqqmQtIim1MAUTKQHijGNrmhFNwKatpldWOIWoSzld8vACxKZqYjQojAJxmWQEvfMUSXXbWDQVavK5iaO4IvXrEJrUlk1Tj02RqKrqvWV05tVBJNV1EyNFYCIqmiGj0KuHirVty0gVo8qsDqmu26kOTgVOUwZnnmEjz9agoIoTyZADVFWIWts0qTWZeopb93TbPCGAqmoIKRM1VU+oNU0LS87Di0GjKXy2LN3Ffh3bajSuLcEMIqLWqmmvj\\u002f6gP051uVq2nPbTlrUFknNc0oWlzjhbHU\\u002f2VS5JlOQBiVHgimX2FvtVmlTjSZPMDBQILKq57gL6g\\u002f4k1ZO1SWMJUSjrOt3QxWo8qdd1BmdJ4fyydAf9MrblaFynRBqgpJmj7w7YG\\u002fQrq8rhpGFiAgXU1B7AMwkpdCk602KA3qBXVfVoNGqYmIyeEIfEEPq9TrGoTRqtDauyNROIiQUm5nnW6xY9l42Ge8fVKNEkCRFzxJiyhaU8W3I2GY9XbE2SuehBcTRmWa9XFH3XxPHasJrEaGJioGdinhW9btFzYThcGZbjZElMSHga8\\u002fyAOigTQ1jodfNFbdJoOKzKaBQIDR6WQuh0u3lfNY5Xh+UkJk49Hck87\\u002fXyTt81cTwcVpMYqSYSRa1lnne7vbwv2o7X1qpJomlyXoTRt8Ni3E+iabR2\\u002fY17ViYxGQx0RPJpNSsWSMTR8Ncre1YmbaLRGcWavKmylX7Ty5q0csONv77RWHtz5ggm3w6LcS+K2GTtur27VycxrnvSfFrNJwu27rl7pWynX+ABEkM7LPJeFLHxcLtuI8\\u002fJgkHTePWGPXtWypgMFDojpV0rJgtGpNHadSvXr5RNMk6\\u002fY97w7Gz3JEHVmLK41pn0EyVWw+tu3L1St4lIWfKAaYsxzLVVV8ZrN6yOh21KhKNFRRSZ7CHrxU6crO5dHU8qM6gBLmpMnZXaunHSD2sr165VbRNZU9REzFpOdpu2VUcmq3tWR2sxJULJJIjAZHdiNeikycq+lcm4SQYBATGJLHebtFVHxmvbdaBpK2OYa8utPKNASQMi\\u002fXi3oVrssNy3b2U8bhKppPA3eDKSdTa+PrKuOjLet29tNGojCJ9EciEgmRXLvQ4pa3XZVEkkKikACZcxX+4WmrDStpMqqiRnMPN1DjpbXM6Xa\\u002fMr46pt2XQaIxSAwWWps9ztULjWVHWVIEmnvzgZXEC+3C2EttY0ZZVEkjMx2V9X2rrOlADy1FnudkkZVmVTR5EkpIBC84HFUq8Qcq1px3UUSS5pcoTBh6mn2ownIYRmlu3odpB0rYplGUVMQZMCIuYo6jMvtBQjzUBnYkKBmIOo90ESW0sGIyAGYQxoM4lBu863mDBpSi666f9SiJg3leCD0FJMadpHigEiMAdocJmYtZaSGAEhhfvrkCiAGACR5IFpnowzeUIRA+idz4SsLRmYpr\\u002fJgOuePkw9o214iqkyeYqEkCG6OlkikURhIh2KUKjJiQda12L6K4wJIQAgVFPJW21doxQDnIHSWje6ZKHqmBc3LiY+lOLaThMaDxOhmpoTBybXCEGIJAXXPV1UDSZRG2egQA0UWddR53UEoACoyYkTtL6WmTyFQpfEOSJJO\\u002f0qVEwowuk72PBsQbF1TyU0OfooXonWN7r+ASghBaY\\u002felLUBJguTEBMKQCSigGiJhQKaJ6gqSBmlcQcaL1vnIAQMgUTC9PGUAFAQCEAIXW6EWLdUyjTf9QRkM4AigCE6fRl++n42\\u002fMUiBinSwE6PcgASjqQ57Q5SBRngMAwzYdK0Jzg\\u002fwBcCzeiJ9b6AgAAAABJRU5ErkJggg==\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9dbea865-58d5-462e-bd02-92295b4a0182');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=04, loss=0.6966, step=300000: 100%|##########| 118/118 [00:03<00:00, 37.06it/s]\n",
            "epoch=05, loss=0.7193, step=303584:   3%|3         | 4/118 [00:00<00:13,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7153, step=307680:  11%|#1        | 13/118 [00:00<00:04, 23.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7094, step=312288:  19%|#9        | 23/118 [00:01<00:02, 32.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7083, step=316896:  27%|##7       | 32/118 [00:01<00:02, 38.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.6997, step=322528:  36%|###6      | 43/118 [00:01<00:01, 42.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.6994, step=327648:  45%|####4     | 53/118 [00:01<00:01, 45.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7166, step=332768:  54%|#####4    | 64/118 [00:01<00:01, 47.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7011, step=337888:  63%|######2   | 74/118 [00:02<00:00, 45.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7056, step=343520:  72%|#######2  | 85/118 [00:02<00:00, 47.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7122, step=347616:  76%|#######6  | 90/118 [00:02<00:00, 42.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7128, step=351712:  85%|########4 | 100/118 [00:02<00:00, 42.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7048, step=357344:  95%|#########4| 112/118 [00:03<00:00, 46.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7218, step=360000:  95%|#########4| 112/118 [00:03<00:00, 46.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"26ce73dc-2c43-47e2-aa4b-3b93c7248238\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"26ce73dc-2c43-47e2-aa4b-3b93c7248238\")) {                    Plotly.newPlot(                        \"26ce73dc-2c43-47e2-aa4b-3b93c7248238\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAVpUlEQVR4Xu15a7hdVXn1eN95WXvtvc8tF0gUA1KiFjASC8IjAgXBNla5ipVEalFoLeqnJFZUtGIBC\\u002fhIbH3kUtRqQVqo2gpoLEoRtF+USo39iiiKYgiEYBJyzr6s25zz\\u002fX6svffZe50LPH38Uft0\\u002fMhZc4zzjjXeeeZac60VsvhfzAeuEv+LEr+2E7P6Rz+pUr9S\\u002fLpOzOZ7Vm2rcr9SVCdm7Y3JjUdUuP9+2O+bb13y4B9X2V8pKhOzZssG2bBllBvG+5LsuCoHAGiuOO\\u002fi+e7jqw\\u002f9oyxJkiS51VSlWZy4\\u002fQVVCjw1NXXJ5bet\\u002fNts+gNVbfV1L8MH3r23Sj9b1P\\u002fvzw+scnOgR0ZH3johrXzp0f9ejNADnPunAVIlARy06ZjDgP03VvlDzz2LnxMEAF77yU2tPv2KpV+e\\u002fR0ARz4wMsTz7DHHTp4BADuuOb31H\\u002feNqsCSdcCOe6vss8LKZdh3wkt\\u002fvKfKz8HwxMRrP7sS+OnHbv7mpVcN0UM4MKoyAPDCd6yv0WPtF73uhh9XlMvWzR6\\u002f8W+29g9POGRkYuigVTQ8fsk\\u002fT\\u002fSOwoe6tz6x7+FhEcDqzxFef0eFHMK77IvOwY\\u002fWVmkc9rZVWL0KV\\u002f8mPTF3cR+14bhDcfETr\\u002fj8v5Xj4Ym59vcBYG3jvhMOH2KHcNKF+PHpuyrk+EfOHgN+8hr7H8uWViTcvQ5PfZYExxw\\u002fQm\\u002f47shw5VtuGZnR7XsmAOD+fb+d3zLM97F+1Za3P1El+zjusONPIwhWbzuiKv32eUB2y4nvgfztnMvwddcso3uXXwla9saSGJqYtesI9331yp3b9p048gcc4OWfmsDHtlfZ094M4JFX7zikKgC44XYUuwCMff85+PLs9VK5sV2Pn46Mn37fq7dtxg\\u002fWdQ99+wjfw70vefQ988\\u002fLipuej4kG\\u002fftaANyoqh\\u002fciJt+uXn3mq8s++WXKpI68vr6tz7yr9Etp6CfcnZi1mwZl6+de\\u002fyHPrP7\\u002f4V1R2wb8LM4dyXu\\u002fXyVxFnAow+8fwdeWBUA+B3lz1dNAY\\u002fnffbw\\u002fftHJSbwjVHi9nvaa87b3MUPLxzlAQCvfZl8MamSAICTrnseALxk97KVNx6Ah6pyPf7Fnz2J33jv8u7lWUVafwO+saGFs0\\u002fBjpt61OCPt3rTxJ4Hb+5suXo3gPiiPj2EpX8Y9l5ZJYE\\u002f+YuTTjj3MWC\\u002fqjCLsy+IgQ8PhuviIQ3Y7yBUF0BLpvHm+ZftxLHAvscBvP3KaphNzwOyTcc9vPfhsw7Ao+dVVHzpoVVX1FdefcbeS2+oKJf+Na7f0ALeC2zc3eP6K8Zeua715gcGiZ\\u002fXP5jFgbcC136zygI7Lyt\\u002fHjNKz+INFx9sgB\\u002fMbnQvwA+HZFy1\\u002f8OD\\u002fWqAy156\\u002fCsr66iEfymHbwHvlAsPxEXPH57Rk48Gtp+3FQCeC9wxZ+P5wXdedOLJH12Fy6+tCJdcnN\\u002f1\\u002fhTRKavoI4N7en9i1q7DWd\\u002fqk\\u002fPiVS\\u002fGv3yiSvbwtgbJ4dj6nSp\\u002f4IaTgGMFmLnka+kQ\\u002f73B0dir1p+Cj0wPaSW6f\\u002fLd6+594Pq5zwbHvyJs34s1L38tOjte+PdvHLrlvauOrZdvBTD5O8dh69dmhR7yGay8leRvbq\\u002fwE38sd50N\\u002fMbnXoovfWzA9ifmarpvdl44zF3Gp16Bf33LTJUFgPjQS9aBA3ZeECrKYV9c1Tv69qdHhCUAXswnPdeew8n9mf7+iFjiZ+ffuGFD4+YnK3TzIOz8\\u002fE9Xbzp1992bx+8aH5Y+vWz6D3YBwB9dih+ur26eALYDwJbNvfveAHYZNi5\\u002f02sOa4rc0h2irbXWnt51m8oja6317trZQYnV3vvPVUlrra2\\u002f\\u002fDHX2vGFGed2vrdR0db+3HvvxXvvTx2ir\\u002fd7tm3b5kO+b+tf\\u002fsHz67vyIW0Ia7\\u002fu\\u002ffUHVcjXOvdhe8Adbt+1zSMe2ndtRbXWWntG4tJ3VElrbe0LIYQ7q6y1y3c675zb\\u002fpjbOcT2Vkxsn\\u002fqHwVR9EPdc0h\\u002f08e4AfLRKAjC\\u002fcxsuv2fr1F2HYfnl228f7DwAgAdPWX9XCuC8t43Q\\u002f2f7MQC23\\u002fHQ\\u002fQDwluU\\u002fG1EHeHD9733qgkNePUq+GLgCt70MZ33r6HvwV+8dFUt8QXDR6AItcfMZgvme26df909LHrnjpr03r7xtmC7n5xz38\\u002f5UNa9wv\\u002fi9\\u002fqCPIx8pii9WSWtt\\u002fSrnvrLcPud7PrnsS87986uOOqr6K9baZc4Nr5hR\\u002fEP4aJUaIPPZyaPMVf4f7ZGP+032sJ\\u002f5oTU+hCvF+8OrpLUH\\u002flnw3\\u002fuM\\u002f26VH+Ak8e8aGg6eY+7s\\u002fVyz6ezb39AnB\\u002fjqFO4\\u002fv0oCfOnGzgdunf6tjx\\u002fxk3fcO\\u002faZc17zVTw2z\\u002fPMKVViFNW7YQ+Hn3mkxkPVHUEECPLin9YePWnuLRuAOSLIxnk+1Zz4IXzo2lPfNOfpZoA4yPCK6U8MnboJAPDO90383Ztn5T6WBlzXqZLA+Ru7F37j6Df9bu2Kz+1A6667Xn8O\\u002fnQg6lPuKXeiN83e6589XnDhaSsAv7Oy9u\\u002fcdOrRa8bwRtp9efXhBwAQrz8Zt\\u002f7d3Avm+M048+4Vl+AXVWGAr48O+xMjK6757N6j1685YPvXrx\\u002f5BQDAjQzM2YsBXAK16YOHAJddVW5Itw3N+bEXn\\u002fyCHQCmfvfqOpL5n1UBAHTI6KsTAOz\\u002fhrceBOCBK\\u002fsLuY+8W\\u002f+mAGh9ce5+DKB5\\u002fZl497Vz5wWvnLjvq\\u002frVE9R\\u002ffpuLyrIeXErqrWfMrAa+c8+fD8sl1rwy5NfNs\\u002f9h1\\u002fJoDbZ8+\\u002fZHqxs1gI8fhk0tACevFdz714t8JJDKqxOw36GbXwTg\\u002fmtun9Pi98995wnATf+5rXqJlTjgTDzyySoJACKiT7vm6c9UH3pncfDosDcx3\\u002fnekVixP\\u002fbcVl5QFUzuj8fn3QBOOnXtLz\\u002f79AIfb4DeN7anvrKx+nIygmP67yclpj75koMBbP34XcPPhH1sWeQzGl74Tjx8apUEACzH7i2vwAVfqfKz+DaP\\u002fHV7E\\u002fP46y94H\\u002fCJG0dfc58R7Vvm\\u002fS4AADj\\u002fwnMB4JHk259+sKqNYPRh8qiNRz0XQPKJq4Yetp4t3n82rht6Fh7Cj3Am7b3u7io9hAd\\u002fcvDBQxda\\u002f1J68rLeK898+PHWY6vUM+MH7\\u002fi3D099+e475rsEh\\u002fG1s0aGp50O\\u002fOhOv3neLecZcOgYPnVPlSxxk33\\u002fA3f+VZUdwVU3\\u002fPlFs5sWzf2W9euLKy76xWnVz33PHmO3vPKfzh8s0zk3vl9nfAPv+a\\u002fPC1rrbzj9wMHof9SK+VWCAQiI0LsJUrmGBgtpMS0IgQQMElC56TIAggAioP6YS2oeT1nIs\\u002fxn2BOCICCCMEGIQTK80y+Ys5KFFsxSqSMjjMAkgRQ8iUAAb0iEIRjWgBGNAlFQCBCDggXwRF4TBCQAifTqHItAQM4KBnUVzWsS0EAbeEoY9gSCYvGkxQsJCaiwJKGas9oDaKA5FshQlsXqNBQ8AhNT0EFREBLORXvyGqLILagxBfLMgb0mRUEYJKI9hEiIAgITcVDE7IVEibceQUGYBudj5iAAi2iPvmdgFvKamIOoYU\\u002f25MFKAolBAAkHIZQ5F+mByvPNm2WROjKEQEoCFKCUTSLxXa0LT0oCeproBTVPWsDGJFZ8onUeiBGERUiJJwUiZdMoSEcr56FnPaEFrG0SBZ8YVdZVPUc0DsIcRJQwa9utoUiULRzzkOc8PQj3PBfLMk+dBpGAPbOpOztuxCGLnOKESISIAtiT7muSDzQmACRK6ZqYMYsCadsr7hIHIYYX9qxM7O24kYIyHVQ+7Mls4mBKz5ZXnBCHUc9xM+LJ0EVQRGQahR233lM+4zQFoYBFe2BIJYvxKk+IgtAidVqEKQSl9FhdNyfHjc9b0502IwlU\\u002fo0W0IRYgihlmnVbn5zQIW3NtDuMRIhDUCSi2IzFpjE5qUPWmmm3WdJAHEB9T9OYnNA+b023OwPPIEOePpv1FBCHoMk06qY5ORb5vLWv22Z0hUNYrIegSEIvy9SEDvnMdKetJBXGYnUaFDhAm0Z9mV0xPh501n5qL8hpD4ACe2jTiJfZFePj3mTtXU+DSw1CgIoa9aX1\\u002fcbGvEnbT+0hFNoREQkH0aYZ72dWjE84k7d37SF2LoCAwAHKNuNl0Yqxnif1zgdhDxU14mXx\\u002fmNj3qSdXXt7nkJBCZRuxkujFeMThc3bu\\u002faAC+NAi\\u002fVAJBygqlkKFxav00QhkLX18aVTzaVLjaOmTousw9AeRCFwX1tiHDd16voaQiAbxWPLJ8aWLLEODZO6oq3IuIBKnYJOXN5WUE765xtbNtlcusR4aurEZW0m7WXguWyyubT0TFzeVmR8AAVBFMXN5VPNpZPWMamkcG0F4\\u002f1iPfSzjC2bai5dYgtFc7PMV6cBk9ZsZOtjY5O1Lkgsx9pa6wpdACaNe1qcJCNa0DqLImPrjfHJOEkAQzVlo9x4UwhMam1k47GxyTjpskQUqyjKo8K4AJPVImPrzbGyzlJ9xtZy66jnaW29OfCMS08qBDatWWMbjbGpOM8EEUfaxoUrjMgz9GAHPfSz2H6WBes0xBlfNGzcGNMucT6ORcRpcTkDAQOtSJyP4zDQWLxxWcPWmw1VJEWo1YWk0OIzkn5dvTFmisS5ug+QQgeXExBgXF63caPZ8xQJTsFlDBr2TPNQqwv6nhSc8UVd15sNztNCIh9ICiVFpvFsehjJkpsyyyJ1WkTBB0eksjzNUyOGU5dnhQcgRHO1otQCM3kpCFy4NE0jsZz6InWeSIRBTnlizoskTwwMJ65IvSu3JQTviFRepHlqxXDq87Q8nzBCWMCTCD4ERcq1ijTVE5Yz5xLnSGTRHpjglBvN4npZFqlj4lAwMbTWRVKw1CLfFQ0HzUIspaZ0PtBUT6OQMysorYquUxJHoSsKXlgBJLlSBNacJQUjjnwXhFxYCXEoWDGU0kVSUKhFoQtFTrQSolCwUlBa5aWnDDzLnIBSXHQcI7Y+AVFOWve0+XsAhVwpBmvOuqNZwmJ1TCASFq0RAIqXGItoJnMIEBBAgUVrCjRHI6JAgQ0JM8dLTCTRvsxDyINBLBBtSIioscREYqdzR0KBCERCokqtvsRaifZlDkKh76kMiWKOp0wUBp4EokBiDAlBoildQzRdBIJI6blAD0NZmKmxRNtBlkV71+JZBW+M1YrqUbPeoFbXO83kGeKZpac1bLPexLCmOHhdM5oljprxGM0k4jXIqyCBlQ\\u002fGWM1ct41GgzjxTjF5JfCKxevIaqY4atab1EqC10yu9BSvo75nk2cS8ZrIqxCIWMRoY5kbplkfF+4GxywQLNpDoH4WKrO0ellYsEidFqUcgyg48XaqbmuJcsYHVb4kKscA+lrUVc6EUhNmr0AiTrydiq1NlTdBFHqaJoJ34uxUw9QSXZggihxTUOwYJMFnPpqq2yhRhQmB+55Ms55RqpwpH+pJiIMCiS+Cs5NjNkrYGR+Yc4PFe2CviYezuNksC9cxsQ8SKZHgycSxziUJNmZPmoXIB6lplFpN5egGW5vVYLXABTJxbAokzkYEpRUI3gfLQt7D1Gs6l24wMTk2BCIfJNIQ76Hjmu55CpuBJ3qeukDibUSijAIgAREzec+2UeM0dJ2JSMgQFuuB4X2wJOQ99bL4QZZF6jQEZLSiLFHadcC5z+rTpDULlAfIKEVZV\\u002fe0vD7T0xyDlFaURE65tqjMF\\u002fVppRSXby+stebEauU74Kzn6XrfJLRWlCZauQ7KOlKaQOwZpFTFk5XmABISYraSJUpHHUZe5I0WK8UCksV7WDjLInUagQ0FRXlmKM9JSZIm6DrJIQSvSi2XgSazmqLAVOTBZhlpJGmKrpdcAHilODDluVCWEYc0TdH1yEWoPB9TngWbZ6TQTRN0vRS9OgqMEc+k9BTPmoU4rRnOoi6HNE1Cx6OAU8\\u002fQg+JeD\\u002fNnmb+OQSBigJ2keZHkgHUu5EFDATzQsqpGLKSUEDvkuU8yiC6cyz0LAQRiJrBDlrtuTmQK7\\u002fOgRUmpAcohy12SCxnnQu6VEIhRehbIqp5EBFIQVSDPfTcjmNz53KlgsGgPvfMtlGWhOgaJQ8Sgmq5ZbcnqLAs2+MAiQ1pU1YjhxCiQVVaTgVZ5GqLggiKBEicRQUU6MmxgVZ6FSLxXQOmpQJGKjDJkVV56KhJiKT0jFc3xZPEwilVNWa0MIp0XPkLwaiTn3B6eMcv8dZrAgYPoWmSbrGu200ldK4AAhoAD9TUzqgVRngg6rtm60XXbSdLQ9kREYBEOFKBrNdtQtlcnIAAc+uer2YYyddPpJr4ViEDgIMrzQp5QjoV0LdYNrWPTaSd5JwQGFGSxHubLEsosi9VpCMhlefBo2Lhm22m3SDt5mjklgQTkh7XOrMbiEdJIHDVqURy10naetPM0d4oEIuSz3Bdo1GqxbqedPGu7NHMKgWbPF8U1287aRdLJ09wzAouHX9DTs8+y2Ek9rtV0krazvF2kmVdh2HOhHnyBRhTHup12580yp04HzSHXSdvUmiZk6Z7WU529WSHCQphfg7AwvBGknLZ00jCSZ7vbu9pP5wU8C4kYhEwn1iRNHRLa3X6yu7dwCEoYwUCygWe2u72rvTcvEEgY3kDm8QwsJFCCnLvWxs3CprKn9cvWXleIp6Bo\\u002fpy9HgyFTA1l2dWZzbJInSYKBfm2gwrxRNzdmbvuTAonQiKEiub7WgCJOJKOIxVqk3HnycwlM90gCEqEBI58y4mW2kS9szMP3ZkUBQIFkKCg0HZQUpusdXfmPplJxUtQQeb1lCBBiUBQUGi5wL42WUseL4pkJoEnYZFFexApKLQctNQm6t0nSq2XZZE6TQCRZJnr1KJarbMvYRccg4Lqfe+XLJ\\u002fVfF\\u002fzECLyLi1acRTXWjNdXQRPIGFhoUDkfe66sa3V2tNd5aRQBFHwBBAF1ztfezpRRXDl3iilp0+LVhzVau2Kp7AEX7huTddr7emuKaRgQBieF+kBJETB564Tm7jWnk64kmWBOjJC5BWBIwaTUKrFpcy+MGXQniaKBJkWlyoqNYCcAbjGQgzJdHCpJucViwiTVyQqUoEZSLW4VCFUPEEMyRRmzweU2jyeAMFpEm1ZmAMygzzV5FzFs9rDaBahVEuR9XtYpI4MKQ\\u002fPLBpBBfbapJ6EvRBBSHl4RfNr2klQBCWiPQetsgChIMQSWHkJzMEg6EBBmdSxkCewCCmHwAw18AwAeYBQaoqgELTnoHU64gmv2BsEBSp0lDgu\\u002f4ugl3P+HgLp4Sxel1kCyiwL1pFlONECIXZM7KjMISh3rYU1goeCBNI5K\\u002fJgD1EIEBZi8aIFAuWYVEEkgVhEGADDQUFKre+JAID6ngKdsyJHfc9+FhPIsyqIlCu\\u002f8YsIPUPOOVlKrZ9lobr\\u002fD15IpVTYMNrmAAAAAElFTkSuQmCC\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('26ce73dc-2c43-47e2-aa4b-3b93c7248238');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=05, loss=0.7218, step=360000: 100%|##########| 118/118 [00:03<00:00, 36.43it/s]\n",
            "epoch=06, loss=0.6874, step=364096:   4%|4         | 5/118 [00:00<00:10, 10.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7094, step=368192:  11%|#1        | 13/118 [00:00<00:04, 22.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7125, step=372800:  19%|#9        | 23/118 [00:01<00:02, 31.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.6962, step=378432:  28%|##7       | 33/118 [00:01<00:02, 39.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7202, step=383040:  36%|###6      | 43/118 [00:01<00:01, 41.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7144, step=388160:  45%|####4     | 53/118 [00:01<00:01, 42.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7225, step=392768:  53%|#####3    | 63/118 [00:01<00:01, 43.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7001, step=398400:  63%|######2   | 74/118 [00:02<00:00, 46.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7077, step=403520:  71%|#######1  | 84/118 [00:02<00:00, 47.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7037, step=408128:  80%|#######9  | 94/118 [00:02<00:00, 45.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7176, step=412736:  84%|########3 | 99/118 [00:02<00:00, 43.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.7057, step=419392:  93%|#########3| 110/118 [00:03<00:00, 47.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.6997, step=420000:  99%|#########9| 117/118 [00:03<00:00, 52.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"7d33b365-ef6d-4dd4-b25f-946d9c862ed0\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"7d33b365-ef6d-4dd4-b25f-946d9c862ed0\")) {                    Plotly.newPlot(                        \"7d33b365-ef6d-4dd4-b25f-946d9c862ed0\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAASj0lEQVR4Xu1aaZQkZZW9772I3Corqxe7m0VBONNsIi0NKIioLHrckMUFZVEP6kHGhU0UGYXBEURHHDiOoqMoIoMCMtrdDCAeVhVFRXDkIAjSAzQiNHStmZUR8d735kculRldlVW0\\u002fBg93upzKr57897M9yLii+\\u002fLLhL8HbOB88Tf0cJfbWNWPvhwnnpO8dfamItu3+6ePPecIt+Y1Zekl+yZ4\\u002f7\\u002fYfntJy659\\u002f159jlFrjGrfnSsH\\u002fujfq4XZ6b6yjwHABje6vgzinkSwMrdTtA0TdP0+4W8NIODHt8pT0GWLFnyyfOu2fpynfpUXlv5tZfhzNOeydMLxdAvHt0+z22GqG+0z9UjPpku3fc3aR\\u002fdxbs\\u002fHuB5EsALT993d2Crk\\u002fP8bu9+K28THAAO\\u002fcqpEx36gKU\\u002f7L4EAPb+dd8QLyi8fP9FRwLAhosOn\\u002fzt7f0qsPQNwOO35tkFYetlGD1wrwc25fnN0NuYyp6XbQ08+IUrbj\\u002fr\\u002fB66B9vNdlFg55OOKdFjk7u+7eIHcsq5b5g5Pu6bd3QOX7XyhzM8wDtsR73jl\\u002fx4pH0Uzqp\\u002f90+jf+gVAaz8DuGta3NkD04p7Ho07t8jT2P3D22HnbbD53ajxze\\u002ffF967AEvwulPvOLyX7YJmcHlqmqqx99k3+the\\u002fDap7Pf7TCUI5d8bVRV799hZ9VX5iT5iOqfzj3v3HNvUlU9oEs\\u002fdHnPa0ReEL7TN172oJmZ3XHd9Hgf38FndN0L8lwXB37o6kxVNb0vr8hJZta4bINZeFdeknf+We2m\\u002f1G1K9tET2P22WR282m2Yc8jvKP245Ubsuw9eVKOV1V9YHuZrTHF7bffRkRk8aOq11S69MP9jbk+nNU3liMv+ZDZXTV58df7+RbuaDy0c55rYdtbH310XO2Xqqr6SF49p27f+vwKWf2kPZk\\u002fucX9J\\u002fTmg0sj16md3qZmGrN6k+q6kUPPXCFik3t36R58w+ymPCfy36oPfe+FIofN0pgOjppUvag7ekm9vzF3hJf3jUUWR1+3Y3NcB0eYnr9dnhQRkdeuV1XVXZfvevD\\u002fql6fl78Q1m8rstPVYeqDeel41esXi7xL9ZEVbarbmF2usCfveXvr2PSKDt2DFZZtPCRPijz\\u002fnFdsJSLy\\u002frkbc\\u002fRNqrq4O\\u002fxE6GvM1k+E7XvHIiLyBbs5znMiIrL0AtOPi4icfMEFOelGVa1\\u002f+GUicrHqQ50Ku9jvXrus9vy19vSpeeVc0y8tFpHfqx7R4TqNqazVsdcvb58J05909BnseJdlZ+fJHnxzrsYcd++0qv662iUuDZ\\u002fokeXycP\\u002fS3rGIiNRuttflORERWXSL28tFTjv1YTPvm2peN6H6cGsiW6v6xV5JREQq37ANR643+0heONum11RFhg6v26e7ZKcx+6se2CVnbcyJmd2wJE+2cdKZ\\u002f3TmL\\u002fX2Qp7f8azbbrvNVHXTCT1X\\u002f6XhNd3jxe9Y2wzHzWhdrBx\\u002f5NsfjvKsyJvV1u8sq3+gOnGf\\u002fXyHHuVHqre\\u002fRkTkeceOtY\\u002f68UUzC\\u002fb1\\u002fOW59M+6RkR2ulP1qlqX7TTmDru5y4nbT2cGbRw5lt22dZ4UEZHhl60zc7MNK\\u002fPKqodVVU1V1\\u002fTSl4ajRGTPvU6\\u002f6OLxyafWjetuvWoHbxkzO2PbPLvoI7rhX2SXK+zJ\\u002f9zrQLuvtzFvu+vmbURE5EzV37aO+nGSmYV1u+TprVRfuNUZPxs3zQ6bYduNeXNDT5khTb88M2hhRzO7NE+KiJT2fUwnN1w9ofrEx8o5bdV6MzM3M3tTD32xPXP33XdbSMfuuPC47UpPpj1aD1bdaHZx\\u002frn8RtWzZeu1Ovblyh6\\u002fH9vsY4qIHDatzc2mVxEpfD+EsC7Piix9Qk1VH31Mn+hh2405Sv\\u002fUPTmV8+3Gkc6gg69mWbZrnhSR8uGqZx0gy+5RVX3nzBO5hR0++dI99thjjwtVexsjZ6xZs2bNmve1nkUnhId6tR4sfbfaj3PcGaoiP1M9UPZXzc++LZjqCXlOROT7ZmZr86yI7LfRHvjX3be5VS\\u002fsIbuNWd9hKp\\u002fRR17fGXSw+o9Zdk2eFJHS+arXLpUVv7LpT\\u002f+X6g2H7LVX\\u002fiUisiTXmD5cFT6fp7pILDmonznffiCrH7dTZJeHreca78Fn3WyWU\\u002fj8Twb71SX2izzfxau9b1rubgnWtX+v+ujb176tQ3Zxw2LceXyeBOSc0+pnXjm290V7PvjBW2v7HX3oDXjsH\\u002fIvAl6bJ\\u002fqxJk+08OK37B3hvp\\u002fkWHcg+B7rS+tf1d179aLwkuAnP5RngYPPwae+cth7fp\\u002fnuygHv7J33OrPO6y9UDxtk32n27UZWJYdk+dE5B914uhlb7x6Ss9uTfXHXHvtzKK0dGj7Cf2+icFXzP55SkRk1y8\\u002fbmbpdTl6f9X9TxxTtSdnTxw+Qe3ypXlW5KBRe\\u002fPQjg\\u002fagAWHae\\u002fSp3srJV9avf1Rax+x9d\\u002fLL0NF5FI3630AdPCE1u+6X1U\\u002ftdmDWuRV1+v2IiLL3jWqOnlwTp3BVb75NkO2Oe2PZmZ3Hp4X9plQU1Wdfd6VRVeqnjTbyvC8cIuUPvxUmG1WbuH1\\u002fY3p3kpy4pETK4Gf3\\u002fLPPZdTG6sOCelXnsqzAP68rLgK1\\u002f1kzSOWV4CLdsfpkwAOWe249Wu35uUZeP7LMix\\u002f0UW7ALjzgrUhL\\u002f3mmJNfDVz2u3s2+y4CALDtW\\u002fDHf8+TAODu8WEXjl7y1bzQxY79w3Zjfv6rfbDVCjxz5Sn9cguLVuDxj+VJADjwsD2f+tbYHF\\u002feAB9o\\u002fXrq2lOa\\u002fUI\\u002f9v1233DJxat2BHDHv9043ce3cN11eaYHO5+CP7wxTwIAluHpGw7Ae6\\u002fN8zP4KfefhfaVs+05avrFOXatB2U21yN1buz1TVVVfeDuL63KS324yv+jd7jfNY+amU2dN7MIXTiuUJ3jXjnFLDz96ZltySz4g\\u002fZOIjO76wHY5rYtaIxUPrBRr\\u002fnAbEvQPhwf+hrzOTO79\\u002fxzl\\u002fZyC8WL1+nFszypRUSe9\\u002fHJWzbbPPbjeL1p95kR\\u002fS39h9tnT33kTfmv+xaO2ncP+cF7653RZhPfXzN+jI9ueV8w8c6vHrFdd\\u002fQ3dcU8l2AnAHAA7qDgTsGdHJ2fPg1tzR0DfYM0dP9trs3jc3cgcAAD7h7amncDHU4IaHkdLWHLamAEdwQiAI5AcCfiAFBAK9WJCAACwUHEofUxBvoGa0AgIvJW5ozmg3wU4AwGiROyEAI5U3CQM+CtLIbDnTyEts+93dVnXwNJq21EAezEFLICghHInRAYFBhwsDNRyApkRiAH2u2e3TdICwxyAgJxYKaQFX0hPlJxAgEBEpjEVNgVRMGZjEGByWfLnKmBwEQhLSIoz1tDa4HnBCPyEkhA6gq0rhdycjg5yItgAUKAU\\u002fBO5By+gRoBDicn8hKRAOaK9nkd4AMFpkDu4FCEMODBAkBo9dPJHUQoEYlXZjJ7akARLKi46UJqYDZ2ghC4VBouxEEmG9NZAoMDbBK6WjFqaU1SABjsm1ujwAAxEZWLw4XYoonphfgIgQhE4lws1YrsMlVPktQNBMDEqZVZqBVik4npZppuXkOt0K3BfHANEQKcIRyXKtXF1RpTvTE+OsYKA3lAYLAUipXhRdURpnp9fHScMxh8Ht+cGrytlStDLa0+PrYAn5MzgblQLlcXDVdjqjfGRsc5I4OTkxOxFEqVoSVDNeJ6fXxsTP6iGiJyJiAqVqqLqstrIwWdmip6mEqCB+fABMSlcnXx0PLaooJOThXdJ+EeCAN882dGpXJ1pLasNhLb5FQRYZKCBwz2ReSISpXq4qHlI7U4TE3G7qDEHeRM5HGpMtTK1Ml6EWESwX2La4icAjNHxXJtyYpF1RKZR5pZlgKg0NIK5drirUaqJbIQaWZp2rnn5\\u002fINynR2tDKXrhgZLpGGWNXSjBaQSRSVKiOLly8eKrGZmFmSsQWAHCRRoVRbunxRtUyZdzLd25+lUB5+djVEICaKotJQbclIkevmQFQsRA5yEDGxRKVKbclIiesGR1QsRAA5+QDfoEwwmKJ2JtUV7lExjt3n8xGIJS5WhpcsLnHDEMCFQkwOZ+9kDi9ZVOJ6RgFRoZ3ZrWHk2dUQITAJx1KqDkeN8dQjrggIIGV3Z2KJpDxck\\u002fp44jGVIwKclHygbx6NOZbK8LA0JpqIuByBHGTzZQpzHHGlVpUpzUJMJQFTYGMgMDHFUq5V25mViNDK3NIaIjImi4gLZU6bmwLHQ9OaNd2NAiM4W8xSLHLWHDUqDDWzNAGMwD7IN0gLcAdx3MrkeKipWRMwzJPp7haighQlTUaN4wpZOg3KHEQBcCYuFCVr+RpZlgAGEIKzRc++hghMTgFxgby5qUHFUsxZZpmTeyBqaTFb2tIoyzQJrSXiAN8gjQA4CgX25mgdpZaWBsB9oI\\u002fJObjEFLLRhpdKMSWaNQ1sAoJToELMIRtttDJTSwMcBCIn76shzbIk+Dw1RE5GESGOSkE5rgxVozRJNG2vcgJFTlFccuVCpTIcNZuJKTlAg32DtMAAJCoFpbgyNBwlSVMztBZ9c\\u002fuAQEyI4pIbx0PlapwkqSuREQUKxHCJS25UKA8NR8000Qze2kvka0iSoNSa0Od+v4gCMxCzBGJ6Xq0cm3Jm7OwEBGZQxBJIeOlwuWAqmcLIyQf5BmsCIJIZLWM1ds4G+wBiByMyMC2tDYmGOMsQQNzJjCQQ0dJaqdCTuaU1RCA4OIojuCyuLhJPVCMGOTtRgIPjSOC8qLqIPc00EgKIMMg3SCNyco6jPk0AnyeTAjtRocAAjdRGBM3pVKR1pxA5O8cSAbKkWhO0MwMRZq+B562htZsSDpY1Q6EcxdpMQExMhNZfFUbkIW2GQjkqWDMBESJibz1C5\\u002fAN0hwOEm5nSqzNphORzJ8ZGAUC0tSLxbiQpikTgcFwhztF7CFteqEUFTRpOoFEGN5fQ1zQZgIin7eGyIkc4lHaKIumRGmSJgR28vZml0OURM6aEiVJmhBxIAcG+wZo7IB4OxOcNLO0dRmRD\\u002fIFBsg9StgrqlOcJDZNIIfDXRxgj5qRU5aCk+ksBZPD2zvFdg2W1hdcQwRAxCTNEmgShqIslVLdOUoc5A6RECWlBJaFoShLo1IdEmeByAf5BmkOFpOklLilNhRnmZTrLlHihIGfJbjEFDULwiELlUKWyNA0R3HmjNCbqUPFNJNyAxKlToBDxNo12MJriBhwxI4QjCgpsmskZg7iAHYCxCmYEcVFchUx9fY0OMA3rxYohKCIS+wakVkAkTMG+hyBA1yNNSlaUCJVCkyBGADFgYKZIg6KTCjT9rdYTqBoC2rwEEDETkSRRHHExdA0WBqI3N0DMTsxiUgcczE0AyxTZgz0DdacIA6QiEQxF0JisCwQhYE+R2AmkIhQVIiiEpqAhUwADyHA2UHEwlFMRU8MpoEouHsAU6sGXngNEcHhhigqVyqFoaqZTYxPJo0suIPg7GFG0zAxPpE0EqB1Bgf45tRAzh48iiuVcrEybGoTY5PJdBrQekjM6WMtuLNIpVIpDA9lIRsfH0sazfZUwe6I4vJQqdTOnEqm00CdGiiKn2UNkTsAywCpVkvF2KefGqs3GmngjFoLK1VAqtVyIQ7NJ8fq09Opk4EG+QZpcDhMCTI0XCrE3tw4Vm80ssBZ+xEyh88NAZkRSXW4VBBtbBxtTDcykIEoIMCUEHUyxxuNRuasMzX4s6whArEHbTTKqZOgPrFx46bmRDPAOBCBPWT1cjl1Yp\\u002fRAoh8gG9QJog9ZI1GKXUSqk9s3LipMTmtCPP4iDxYVi+VM0fkjYmNG59JJqczD2itSXoyxzuZTvkaGuMLrSECHBqglnl9KEyMTYyOp5m6weEEh4YpNe3RWt99YX7fXFqAhrpa5o0hmxifGBtL0wzmoMG+QFEWpjLVUK+GsfGJsbGsmSE4mTi8m1nVifGJsVFNUphTJzNXQwoDfHANQsYIEZeGiyOiWT3LmhYyYxUH2BgWc3G4VBPVepZNu2XGKnAa4FuARuVqqRZpVs902kJmbDzYB9bYNZLCcKUWpTaVZU3zzDiL4M49mZm2MtPAxn9BDQyAABKJOfLMuQly89aKsaUhkpjioM5NABbgAPn8vrk1cpIo4iho4ATw4A4fnEmAE5w5ipk9GKeB4BkAsLtTO1MkaODEyUMA3NmdZq+BfPD7RU6B4AxE7IHJjFlb9wqorTlFhMAIKptpg3yzaRwoEJwIwjCBK0u2AB\\u002fInQORs4g5katwxsHJneEUCABBGIHhKpx1bVtYgzgBTk6gQOj545nWqpHgIIACuXQ1p44+l28hGtuMRt56rwE+gA3sAJm4WMtDLWXWTLR2kVtYw\\u002f8BjKwZy+WEouYAAAAASUVORK5CYII=\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('7d33b365-ef6d-4dd4-b25f-946d9c862ed0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=06, loss=0.6997, step=420000: 100%|##########| 118/118 [00:03<00:00, 36.57it/s]\n",
            "epoch=07, loss=0.7232, step=423072:   3%|3         | 4/118 [00:00<00:14,  7.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7127, step=427680:  11%|#1        | 13/118 [00:00<00:04, 21.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.6916, step=432288:  19%|#9        | 23/118 [00:01<00:02, 32.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.6987, step=436384:  24%|##3       | 28/118 [00:01<00:02, 36.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7162, step=440480:  31%|###1      | 37/118 [00:01<00:02, 36.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7166, step=445088:  40%|###9      | 47/118 [00:01<00:01, 39.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7160, step=450720:  49%|####9     | 58/118 [00:01<00:01, 45.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7181, step=455328:  58%|#####8    | 69/118 [00:02<00:01, 45.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.6970, step=460448:  67%|######6   | 79/118 [00:02<00:00, 45.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7135, step=465568:  75%|#######5  | 89/118 [00:02<00:00, 46.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7103, step=471200:  84%|########3 | 99/118 [00:02<00:00, 47.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.7031, step=475808:  92%|#########2| 109/118 [00:02<00:00, 46.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.6953, step=480000:  97%|#########7| 115/118 [00:03<00:00, 50.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 96, 5])\n",
            "torch.Size([96, 5])\n",
            "torch.Size([2, 10, 5])\n",
            "torch.Size([10, 5])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"45f13933-6e8d-4873-9f46-5d704cfa8bda\" class=\"plotly-graph-div\" style=\"height:300px; width:750px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"45f13933-6e8d-4873-9f46-5d704cfa8bda\")) {                    Plotly.newPlot(                        \"45f13933-6e8d-4873-9f46-5d704cfa8bda\",                        [{\"name\":\"0\",\"source\":\"data:image\\u002fpng;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAAA4CAAAAAADPrjSAAAYTklEQVR4Xu2Ze7BtVXXmvzHGXGvtx3ncB3ABFVFbRIwC4lt8GytWoiiJ0fiKhekujB1FoEyUqNEOatJqtCyDaaMxxtg+2wioRLtQNCEam9J0Wz6CQkAEudzLPefs13qMR\\u002f+x9jlnn8XltJXyjzaVr07VWfP3rTHW3GPNPedca5Pg33U0cRf8u1r93BbmlOtv6KKfqX5eC\\u002fOua076Vpf9TNUtzFnvr9\\u002f\\u002f0A77\\u002f08HvvKyfd\\u002f+j136M1WnMGdc9cJ44VU72aJeW+vjuwwAsHzCS19TdCGAUx50vtZ1XdefzLvWtp784wd0EWTfvn2\\u002f\\u002f+ZPnfhhHb+u653yZ4\\u002fEJRcd7uKfVsOv3Xxyl91FaUfrER9fjVG9\\u002f9HX1Tvwln7zdx3RhQDuc\\u002fGjfgE4cEGXP+jFv8YnegDAM\\u002f70wo1N\\u002fLj9f7N1CgA8\\u002fLodTZyUP\\u002fqxe84FgFve+azRP31lpwvsfzpwy5e79KfSicfiyBPP+v7\\u002fu6iLhRmc+aETgOvf9pFr3vCWBbygk442KHDqK17Qox+NHvic936v41z69O3jF33g2s3DJ9z\\u002fb7Y5wCffixbbZ35hdX7kr5\\u002f899uOfH\\u002fRBHDKhwi\\u002fdnkHLujC\\u002fNTn43sP6WI8+OX3xv1Pwh+dRrfedfg+8gWPexBefevZf\\u002f31OZBtfVhVTfW8q+2jC3RBTzvU\\u002fJ\\u002f7Djtw358dUdXv3fdU1cd3LHml6q2XvvnSS69WVX3cFv7BhxfOETnJ\\u002f2pH+9jrzczs2s\\u002fP1nfwTf2hXnlSl23pSb\\u002fziUZVtf5O15ELzGz6oVvM\\u002fMVdS37jJ2pX\\u002f29V+9gcLBTmEXeaXX2x3fLQZ8emu1OPv6VpXtKFcp6q6vdPlqMVpjj55HuIiOy9WfVTgy18w87CXOWv39GWc9\\u002f\\u002fO2bXrchD3reTt7p2+oNTu6zVPb98883rav+oqqo3dd03Tuwv\\u002fvh4Oet2u717c4uzN\\u002fTqp\\u002fRWP6\\u002f26jnaLsxZd6peufqMS44XsdHDt\\u002fCC\\u002ftzs6i4T+ZzqDz56H5FzjlKYTT13pPqurdaZk52FudYfu6Mtsje9z17YYZt6tulb792FIiLytBtVVfW04057yr+oXtW13+Y33lPkAZ\\u002fw8X\\u002fuWuepXrVX5MWqNx0\\u002fR1uFeeBH7PZv\\u002fXp7bPqRTbygA9bc8dQuFLnXG88+QUTkP919YZ5\\u002ftaru3Wq+1ncU5sTb\\u002fOTFtoiIvM2uzrpMRET2v93090REXvX2t3esL6rq5BWPEpHLVH9woOPKY75tH1q51xV26KKuc6npu\\u002feKyHdVn73JNgszuELXnn7c\\u002fE6YfnXT39b9rrPmDV24oA\\u002fcXWFe9O2Zqv6vpS3wQX\\u002ftgi0f9u\\u002ftX2yLiMjK1fZLXSYiInu+FPZYkYsvusEsdkw1v7ShekM7kV2h+o5FS0REBn9ut5x7o9kru8YbbPaZJZHhsyb2pi24WZizVZ+0BY9amN9u7G\\u002f3deFcF1zy+5f8o34l7\\u002fL7vf6aa64xVb3z\\u002fIXR\\u002f0H\\u002fxa3jvc+7ovQXbXtbOmX9pr98RepSkWeq3XiqnPVp1Y3v2D\\u002fcd8H5gupXflFE5JgXrs2PduodZub2vu7w3P8T\\u002fYyIPODrqh9f2aKbhbl2cfoI+7vtxlznrjXXnNiFIiKy\\u002fKgrzcLsllO6zhk3qKqaql6+iD\\u002fozxWRhz7s1e+6bH108Mp1fdCiu6lfXTN7zT27dM8r9Zb\\u002fIg\\u002f8iN3+1w97kn1nsTDPue7qe4iIyCWq\\u002f9Qe7dQFZuZXPrCLT1C9zwmv+ft10+acbTovzDOneuE2NH3PdqPV\\u002fczsg10oItJ79I90dMsnNlRv+91+xzvjRjOzMDP7lQV8mR3+5je\\u002faV6vXfvOF927d3u94C3ojC+aXdZdl39Z9Q1y4hW69p7B6d9du0s3RUTOmWl5l+lVRPJPuvuVXSqy\\u002fzY1Vb35R3rbAp0X5rl669bNGbzVvri62djUe5umOa0LRaT\\u002fLNXXP06O\\u002fZaq6m9sr8it7vu6R55++umnv1N1sTDymssvv\\u002fzyy3+rXYvO9x8segva\\u002f5tq\\u002f7PDXqMq8veqT5KzVbuzbytTPb\\u002fLREQ+aWZ2RZeKyGPusO\\u002f\\u002f1wff48v6zgW4VZgbN8ngD\\u002fWmp282NnXWD5vmU10oIr23qn52vxz\\u002fDZu96X+o\\u002fu1TH\\u002faw7ikisq9TmB36uP9xF22psurJO8lb7dNy1o\\u002ftQnngDbYwxhf0ljA7yi281+vcvvF++1qXb+mJsWNa3nokuGL+\\u002f4yLfv3y52zCLX1+L75+XhcC8saLJpd8dO3h7zzz+pd\\u002feeXRz3\\u002fGVfjRf+ieBDytC3bqbnb3Dzn34Qnf+WqHRgAeD76xd+MT1zsWACA\\u002f0+NV13cp8OQ\\u002fwOvfc85LvtvlW+p7fGyx3dbneTbfKF58p\\u002f3VVtW2ZU3zgi4TkZfrxvOP\\u002feVPjPUN7VT\\u002fgs9+dntT2nvGfIX+rY3dR8zZXSQictp7fmxm9ec7+GzVs397TdVuP3rG5fPVPry\\u002fS0WefMSeObzf9bbLhsN0c3MnsvBVqt591snPveImu\\u002fGj3W2oiHwwzBYXgE3dppPrvqeqr7vLQi3yhKv0ZBGRY198RHX0lI67rY\\u002fHS7pI5B4X\\u002f9DM7OvP6hqP2FBTVT36vCt7PqZ6wdF2hm\\u002f2L0nvFQf9aLNyq6fvLMzWV0ledu7G\\u002fYF\\u002f+NIfLAynuc54qtd\\u002fenuXAvjJscXp+PxXP\\u002fMv1nWAd\\u002f0CLh4BeOpDA9e898tde1vRfVmGA6e961QAX3\\u002f75d61rnvBq54AfOjb37zLuwgAwD1\\u002fFT98dxcCQERk5\\u002fzJkfdf1jW2dL+dzXlhvvaNh+P4Azj8sVfttFutHsCPX92FAPCkc8684y+O3M3LG+D89t\\u002fBK19V7jR26lF\\u002fuaO577LT7wvg2j\\u002f5wmwHb\\u002fW5z3XJgk69AP\\u002f8K10IADgWh656HF56ZZdv6+94512Yj5x7vlFN33E3T61PbuzultS718M+oKqq3\\u002f\\u002fmu8\\u002foWjv08fhvi83HfOpmM7Pxm7c3oT+9PqJ6N9+VC8380Jvmk97R9c+6OIlsFmZX3eOaf0VhZPCyO\\u002fRTLzvaFnSHzvMdhfkjM\\u002fv2Wy\\u002fdv8h+Wj3kSr3sKCu1iMgxvzf60kVduFPn6dUP3m7Rv6Uf3N5y4U3P6L7u++m18tGnfPqlk83WXSa+n2d9ERf\\u002f6+uCjee999n33mr9mxoxP0sxQETA1h8ICCICEVEQQMQABTEoCAhmAm2e3MZvxmEet1vOXT1qhSCAgSBCMAGMFgCEYBCCAqB5OgDEoDnejJP5qaB53PwEQtveig0ChMBzg2h+tbZzm50KcmCeOsAgRFAQCQWcFYCDCBTziDYMiHaxCwIQu+VsP0fbgbt4AQQCYCIKBLsDAaagIAoiJnD7wYkdEXCeX86BQHsGQOaIAHNs9hNgJgqmYANAQdT+DsQsQsLBAYAi4ORt5xOCA0GAC4jIoRLBFN7WmiKIHBJEHOaMIIIBhOAgJyA4GOxQCYBg1Hp3m\\u002fMo3vyjUaDNGZ4FEXs0mYMojEBwBhAM4yCmoEacQORwDhAQYLccYHbTzEGAAQyX9nqWnBlOKk4gtLsWCgry5AQwTOp2ZnEOEgI52MESWSTnKoUGGYICFOxgJ+JIlILqpBHUsHP78UFBxEhIzrUsxO2Wc6dH9abHTgCc2EHJ89ZzC1YAARAZUXAkMHJDI9BAeLAzAGNRmsehSW6ghoKNCGREQUiRkCsaCWvjAsGIECeSSJE7VZm5UURQkIAQFExcZPmwJ1Y1OivNGwoiJ3IQIyuk6A\\u002fYp9bMKrOGQAAFACbJs3ypYC+1KWfuDQVhl5xzT8BFng17YqUuehEcTNxL+aAvWmlTlmZKQFBwBHKnosj7\\u002fWQzbcqZmxGcguFiQtzL8n6frTSd1mrN9por4Dzfyjlzb6gdMHAmTlmRlvNkM6vLSURDACUEgShlvf7y8uoyUTlem47GM2oQDCeAk\\u002fSHw5XVJebp5MhkPK5CAWzGFYPl5dUlpmp0ZDoeT0kR7Ux39JxH8Y5Mx3MvQMGUUn8wXFldYionRybjUUkNgh0EQsp7w+HKyjLJdGN9Mp7MyDzYQcEkWX\\u002fYX11dIq7Ga+PxqEJNwREUjCwv+kurK8vEs4316Wgyg0W0twEpL4bLS6srxLPRneXGZEqKSAgQKMuL\\u002fcuDAyt7WNbHvcNWW03txAfPpb9\\u002fuHzc6iql9Y3eYdReU0Q7H2R5b9\\u002fS0nEre5g3xr3DsRW3W85Nb3jcpudzDyAgy3r7l\\u002fsHVldZNsbFYa+9Zkewc1DWS8cMl49bXWUebRw6HGWvaudvArLUO2a5f2B1VXg06h3yOipxBMBBWS87ZtDGbYzvOBy11wBRe4+KYv\\u002fy0nEreygdmfQORZ3XbEgOBmUFLxX7jlsdLqfacqsmWU1g96CIosCw2H\\u002fsnsFqquosynGmauQICWQFD4v9x+3pL2e1Z1ZOs5rAFrvkjBBQlstSsfe4PcPlVGtu1SRrGrA7GJQXtFTsO3bPPK6c5I0aRTgHsgJLvWOO29tfyWYmUU8HIxgZhYDygpbbnNJobtNJoY1RREhEkWNYHHPs3v5KVlpm1bRoADICg1Oelop9x+7rLefVSmH1LG+0IU8ETwQaFCsrg0yqEQxFUSR2tEseEQbFyspynpU1nPIiz0qLAMgSgYa9lZVBluoxHHmRJ\\u002fYIwi452zgeFCurw0zKBh5FUWTkcACWEdGgWF0eZlw2cBS9IpHBEewJQcNiaXU5STki45TnEkHBQZYx0aC3urqUcVWRR9HrCSwiiFw4aNhbXlnKUjWCUVYUGebXSwD3i+XVYcrrCcxTlicxBCcwmBKo3+tzXUZ4P5ydPQggJqIEGvT6VJUB7asSYCENB7EgIXr9HlezCOuHs3EEK+2WEwwmAfX7rWeDcHaKYCPiYErgfq\\u002fPVQn3vhsFtQsjMREDw0EPVWmwPpMEpSAP4mBOIb1ejzbjOMiRlEBgYsKg36Nq5uF9AjtxEAjEAokYDHpSlR7RMxJlCzJKBESYcNHLfDKNIkdys0BWU3BQoJFUFIWXkyjyWQpvglLNQUROHqkoMptMIy\\u002fKzM0cqeHYLScBAQ8uitR6s+RmEVnd9jUalqLIfTqLIkcKbxyppgAIBJOiV2g59V5eJm8U4HbJIm8KLorCymkURcnhtUeqERSEgElRFDaZRFGUKVQDqSFnRIQjy\\u002fPMp9MoMi+0ieCkmTEiIoIlCcrxyBCguqxrs2g\\u002fgREzJy\\u002fHY0MQNWXTaBCAiHBjToJyPDYOoKmaWi1a7+5yIiI8OCWhcjwy6noG4SQoJyNDAHWpjVkEEITGs4wlqsnYGEFVWdeNBYMCriGcBNV4ZIigqlR1jUBEhHnKRKIaj50Bqqq6UnXiADQCIikm47ERwpu6nFXmESmA4Eh5StT4cE\\u002fRo0pLDWl36gEJyVOixnp7+7nMtFRPdYACQYyUJyHz\\u002fp5ewZWWDUkA2C3n3MtSIo3haq9Hlc62vCAiyaWN6xWotWy8zclOQknyBI\\u002fhyqDgqZa1E0KDYWCWjDOoD\\u002ff2iqitbEzaPUU4IaUskWFppV\\u002fITGcNGORBLkFepExIfbCvyLlsykZSheBEwYjEWSaQ\\u002fuD4JH5ENdrdHQhw5ixjSL9\\u002fQuJQtxAGBSgISJxlGfGgP48DAHLCbjlbL88SuDf3rB3wRM4Ac5ZLez2xNdVon2EJiBDpZxmj1z+QEsomgsHhxMaAUNEj4n7\\u002fhMR+xKyNYwSDhHtZTtTrH0gp6sbbZwQiigihPM8pDYYHRNxU5ws8I5wgkpgb9Ff6+3peIzEJtw94RMKJWb230tvXt9oyppB2NHGIJOIGveU2TphYiGK3nHNPtr2m9ZgCEQBLxtxEf09\\u002fXy\\u002fqSEzMhIgIgjAENfVXB\\u002fsKM5BERtS6oJSR1N7f09vTs8qFwcwAEBTCxGjQX+3vK1yJGYmJgHD2jBNIqVga7CuiChFGTuQMhkdkQK2W8lwnM9MQeHjAKRzIwI1GXhQ2malaRuERAMEIGahSy4pcJzNrIPCwwG45QXOvUkt5ppOZbnnBFEGZo1ZPWa6TmVkk8nACQAHkhJlZyjObVo0GSR3qEUIA5YbakLLcJ6VbJPYwCiAcUYBK85TnNi2bJogtPKidmvPgsmnyXmomM\\u002fMQQMMpwSlEKCq2VFczr2eleXJzgOBESBwla8qqmTXTqYWYKoAIAjF5xZaqsrR6Wnokd6fArjlj7omluirvcj1mQsWaVdXMmnJmLmaKICcnwH1K4GJaoBlXFtlaqLiLEzEDFWsm1dSbcqou5oqgAALsMaWQfNaLelJZ5BtmFEEuIJCVFNzM+tZMy8ZzdfdAAjGYwhs45+uJymnDPGOkBgCDxN2bqUt\\u002fI3E581xKlqICCEREiIaCio0c5bRhKjlSDeyakxhM8GZqm57wjJFqAm3H1fk6SzVuhGdMWUPgYBYSWKlSaErVtMmkTkjKAWIQhevMm3yDpR5r4hkjU4CCWJhdZ8b5epJq2iSpBFlNgRRByVFHUG8tj9lUc5kK8grEAQ8JCrPKRtWUhcPDpmGEgMMFHK61bVQzIrbadWbKhAiPFBGmpY+qCQm7QadhRLRbztYDzCoblVNOHLrpOSLEKVwrH5cls7SeE8LDkZGGam0bOkNGodbMDNE+5ienMK1sMisFm3GY9xMG18pH9QyJwryZuoEQHpQsYFbGqB5TJq7QqRlTJEBAecpz6ctS0WdGVGUVCncKkuA8ZZkMZFj0WQCtG1i4tTvRLMuz1Jdh0Rcmqtu4oN1ytm8AUpZLX5Z6fSbEdhxJUJHlmfTTUq\\u002fPHFGVVTRwJw6JyIs8owEv5wNpgsppZQEogTlQZHmSoSwVfZaIqqy8jSMGUp4n6adh0RcFylntDg8Gc3CeZYX0ZJj3mYC6rMgjAgnh4hGQ4SotLS+7NeNq1DSoBe2s7YE0XKGllZXG6nG1XtdRCwcQ7BHIhivzuFG9sR23W06xbc+2vQAcEuGQ4SoPV5bM6kk9bhrUQmRsyamhtLQay8vL7jouZ3UTVQrQPC4trfBwZcm0mtTjRlELyAFxcmTDFSy31ysndeO1OAKWeQCpv4eGq0P3ZqNebxpUWVAiQYQZ0mDY7+fFZFSuHZ5VERIE4nbTxb3hoJ\\u002fltjE7slZV4QnBIHKYQ\\u002frzuNna4bKKkABh15wUmHu9YttLTiAJhBvScKnXK4rxRr12eFrBBaDcyKHkMugt5f3+eKNaOzKt2HITJ44IN6TBUq9X9EYbzfo8jogBJ42QwWBYFL3xqFo7Mq2IMuMgYicLyHAw7OXFdDRbu7OsnFIQEhwUZVZWDYibtemdP95Yqyvydv1weDXpNQZwc2R6+CejtbqCt9stZ6+mg6oJ4mZtcuet62t1DQMBu+VEBHmZVVUTJM3aZNMLQsADVmZlXQelZm1y520ba3VFEcQRcPFZNmhqpkLvnK3dtrZeK1yZaB43q2tQ1hyZ3Hnb2lpdEYI4KEK0zAdVTUh6ZHLk1rX1qiFzBkDGKKeDWhFcN5O1W0ZHrGYFxFOA0PjUw2N5UtnGHQdno9qcnAEjhlUUrr4yKWP9jturjcqD0G7s0cT0oJsv92rbuOPgbKN2b19A7pJzfr2D7lGO27hFj6wSD\\u002fOVSWXrh24vR5VFu6\\u002f35OGTcEU5mcX6wYOTyZScKaACZq+nDovlSa3rh24vR6UHQBEODsPE3bE0qWzj4MHpeIZAAMbOVGeTcPOlaW1rhw5NRk0DcYkgCRJFIb1Bf8Dks\\u002fGkbpSd2l0zxCLLimG\\u002fL+GzyUSbOjnaN\\u002fkQQ56KYb8v87hG5nG75QxiQ56O4nEAYshSMewPiW02mta1cSAQbBwSWrR9IZ+Op41WHGzsHIhkSFkx7A+ZfLoxq2uVQMA5KJJFLr1hvy9k5WhaWyVOTsGOyCzylC\\u002f1BwSUG9P2Mzg7z1+GkzCnnJ20Dlc3oQi0z0MgIZGMI7wmVXdBIBhBASIhTjkHtAlv3AUIoN3HHT0n0Ma1HqwOM7c2bv7ElMBZzgGtw8wsIQCwMnmygjjlbNSoR23B7AiGE83jCnZoHerqiRxBFCCXSMxZxhHahDcWQu3L0vahhzjLKFhrWBMOQQQoBQgUEsFCCG4inNrOBEUQUXC4JJCzIoxNAOO2AAz2ECEEKcJYN+N2ywmAFz0sxoEoZPN6TcDIEiIoyIkCxAgSEuOavc4MHEzKTgRAYJLARoowMqEICnZCEBGChFtPxYgAcgqwM5JGYiLnmtzJBQHMCxOE9l4GR9sOzBECQLCzg8iDydtnVmwGtfsr2hG3a864ey\\u002fA0Y7SIDi1v+e14zYCRBROYBM4tyN3Xq5A+4MGKAjGbARyCgIFAjS\\u002fi04U1OZHtAPX2HmzP4RwAjnBU+D\\u002fAkH9cKXmgyKEAAAAAElFTkSuQmCC\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0]},\"title\":{\"text\":\"VAE reconstructions\\u003cbr\\u003esingle input shape = torch.Size([1, 28, 28])\"},\"height\":300,\"width\":750},                        {\"displaylogo\": false, \"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('45f13933-6e8d-4873-9f46-5d704cfa8bda');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=07, loss=0.6953, step=480000: 100%|##########| 118/118 [00:03<00:00, 35.90it/s]\n",
            "epoch=08, loss=0.7191, step=484096:   4%|4         | 5/118 [00:00<00:11,  9.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=08, loss=0.7115, step=489216:  13%|#2        | 15/118 [00:00<00:03, 25.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch=08, loss=0.6989, step=489728:  16%|#6        | 19/118 [00:01<00:05, 18.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n",
            "torch.Size([2, 512, 5])\n",
            "torch.Size([512, 5])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8c693ad691c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHVAEArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHVAETrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-506b7e791b59>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{epoch=:02d}, {loss=:.4f}, step={self.step:05d}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m# log every 250 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-506b7e791b59>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrec_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkl_div_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_kl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM2UGVOXHZiJ"
      },
      "source": [
        "# 2️⃣ GANs\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Understand the loss function used in GANs, and why it can be expected to result in the generator producing realistic outputs.\n",
        "> - Implement the DCGAN architecture from the paper, with relatively minimal guidance.\n",
        "> - Learn how to identify and fix bugs in your GAN architecture, to improve convergence properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeN_S2Z-HZiJ"
      },
      "source": [
        "## Reading\n",
        "\n",
        "* Google Machine Learning Education, [Generative Adversarial Networks](https://developers.google.com/machine-learning/gan) (strongly recommended, ~15 mins)\n",
        "    * This is a very accessible introduction to the core ideas behind GANs\n",
        "    * You should read at least the sections in **Overview**, and the sections in **GAN Anatomy** up to and including **Loss Functions**\n",
        "* [Unsupervised representation learning with deep convolutional generative adversarial networks](https://paperswithcode.com/method/dcgan) (optional, we'll be going through parts of this paper later on in the exercises)\n",
        "    * This paper introduced the DCGAN, and describes an architecture very close to the one we'll be building today.\n",
        "    * It's one of the most cited ML papers of all time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b43cM1PIHZiJ"
      },
      "source": [
        "## How GANs work\n",
        "\n",
        "The basic idea behind GANs is as follows:\n",
        "\n",
        "* You have two networks, the **generator** and the **discriminator**.\n",
        "* The generator's job is to produce output realistic enough to fool the discriminator, and the discriminator's job is to try and tell the difference between real and fake output.\n",
        "\n",
        "The idea is for both networks to be trained simultaneously, in a positive feedback loop: as the generator produces better output, the discriminator's job becomes harder, and it has to learn to spot more subtle features distinguishing real and fake images, meaning the generator has to work harder to produce images with those features.\n",
        "\n",
        "### Discriminator\n",
        "\n",
        "The discriminator works by taking an image (either real, or created by the generator), and outputting a single value between 0 and 1, which is the probability that the discriminator puts on the image being real. The discriminator sees the images, but not the labels (i.e. whether the images are real or fake), and it is trained to distinguish between real and fake images with maximum accuracy. The discriminator's loss function is the cross entropy between its probability estimates ($D(x)$ for real images, $D(G(z))$ for fake images) and the true labels ($1$ for real images, $0$ for fake images).\n",
        "\n",
        "### Generator\n",
        "\n",
        "The architecture of generators in a GAN setup is generally a mirror image of the discriminator, with convolutions swapped out for **transposed convolutions**. This is the case for the DCGAN paper we'll be reading (which is why they only give a diagram of the generator, not both). The generator works by taking in a vector $z$, whose elements are all normally distributed with mean 0 and variance 1. We call the space from which $z$ is sampled **latent dimension** or **latent space**, and we call $z$ a **latent vector**. The formal definition of a latent space is *an abstract multi-dimensional space that encodes a meaningful internal representation of externally observed events.* We'll dive a little deeper into what this means and the overall significance of latent spaces later on, but for now it's fine to understand this vector $z$ as a kind of random seed, which causes the generator to produce different outputs. After all, if the generator only ever produced the same image as output then the discriminator's job would be pretty easy (just subtract the image $g$ always produces from the input image, and see if the result is close to zero!). The generator's objective function is an increasing function of $D(G(z))$, in other words it tries to produce images $G(z)$ which have a high chance of fooling the discriminator (i.e. $D(G(z)) \\approx 1$).\n",
        "\n",
        "### Convergence\n",
        "\n",
        "The ideal outcome when training a GAN is for the generator to produce perfect output indistinguishable from real images, and the discriminator just guesses randomly. However, the precise nature of the situations when GANs converge is an ongoing area of study (in general, adversarial networks have very unstable training patterns). For example, you can imagine a situation where the discriminator becomes almost perfect at spotting fake outputs, because of some feature that the discriminator spots and that the generator fails to capture in its outputs. It will be very difficult for the generator to get a training signal, because it has to figure out what feature is missing from its outputs, and how it can add that feature to fool the discriminator. And to make matters worse, maybe marginal steps in that direction will only increase the probability of fooling the discriminator from almost-zero to slightly-more-than-almost-zero, which isn't much of a training signal! Later on we will see techniques people have developed to overcome problems like this and others, but in general they can't be solved completely.\n",
        "\n",
        "<details>\n",
        "<summary>Optional exercise - what conditions must hold for the discriminator's best strategy to be random guessing with probability 0.5?</summary>\n",
        "\n",
        "It is necessary for the generator to be producing perfect outputs, because otherwise the discriminator could do better than random guessing.\n",
        "\n",
        "If the generator is producing perfect outputs, then the discriminator never has any ability to distinguish real from fake images, so it has no information. Its job is to minimise the cross entropy between its output distribution $(D(x), 1-D(x))$, and the distribution of real/fake images. Call this $(p, 1-p)$, i.e. $p$ stands for the proportion of images in training which are real. Note how we just used $p$ rather than $p(x)$, because there's no information in the image $x$ which indicates whether it is real or fake. Trying to minimize the cross entropy between $(p, 1-p)$ and $(D(x), 1-D(x))$ gives us the solution $D(x) = p$ for all $x$. In other words, our discriminator guesses real/fake randomly with probability equal to the true underlying frequency of real/fake images in the data. This is 0.5 if and only if the data contains an equal number of real and fake images.\n",
        "\n",
        "To summarize, the necessary and sufficient conditions for $(\\forall x) \\; D(x) = 0.5$ being the optimal strategy are:\n",
        "\n",
        "* The generator $G$ produces perfect output\n",
        "* The underlying frequency of real/fake images in the data is 50/50\n",
        "\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/dcgan-9-solid.png\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10n2DmkdHZiJ"
      },
      "source": [
        "### Exercise - some more modules\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You'll also need to implement a few more modules, which have docstrings provided below (they should be fairly quick, and will just serve as a refresher for the structure of modules). They are:\n",
        "\n",
        "* [`Tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html) which is an activation function used by the DCGAN you'll be implementing.\n",
        "* [`LeakyReLU`](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html) which is an activation function used by the DCGAN you'll be implementing. This function is popular in tasks where we we may suffer from sparse gradients (GANs are a primary example of this).\n",
        "* [`Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html), for converting the single logit output from the discriminator into a probability.\n",
        "\n",
        "They should all be relatively short. You can go back to day 2's exercises to remind yourself of the basic syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URDbIdujHZiJ"
      },
      "outputs": [],
      "source": [
        "class Tanh(nn.Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class LeakyReLU(nn.Module):\n",
        "    def __init__(self, negative_slope: float = 0.01):\n",
        "        super().__init__()\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"negative_slope={self.negative_slope}\"\n",
        "\n",
        "\n",
        "class Sigmoid(nn.Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_Tanh(Tanh)\n",
        "tests.test_LeakyReLU(LeakyReLU)\n",
        "tests.test_Sigmoid(Sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps07wNKsHZiJ"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class Tanh(nn.Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return (t.exp(x) - t.exp(-x)) / (t.exp(x) + t.exp(-x))\n",
        "\n",
        "\n",
        "class LeakyReLU(nn.Module):\n",
        "    def __init__(self, negative_slope: float = 0.01):\n",
        "        super().__init__()\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return t.where(x > 0, x, self.negative_slope * x)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        return f\"negative_slope={self.negative_slope}\"\n",
        "\n",
        "\n",
        "class Sigmoid(nn.Module):\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return 1 / (1 + t.exp(-x))\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72TJI2xmHZiJ"
      },
      "source": [
        "## GANs\n",
        "\n",
        "Now, you're ready to implement and train your own DCGAN! You'll be basing your implementation on the [DCGAN paper](https://arxiv.org/abs/1511.06434v2). Implementing architectures based on descriptions in papers is an incredibly valuable skill for any would-be research engineer, however in these exercises we've given enough guidance on this page that you shouldn't need to refer to the paper much if at all. However, we do encourage you to skim the paper, and think about how you might go about this replication task without guidance!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imHru9h8HZiJ"
      },
      "source": [
        "### Discriminator & Generator architectures\n",
        "\n",
        "We refer back to the diagram at the start of this section for the basic discriminator and generator architectures. Rather than hardcoding a single set of values, we're going to make our architecture more flexible - giving us the ability to change the number of layers, or the sizes of each layer, by using different input arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477wzZXuHZiJ"
      },
      "source": [
        "#### Discriminator\n",
        "\n",
        "The discriminator starts with a series of blocks of the form `(Conv -> BatchNorm -> ActivationFunction)`. Following the paper's conventions:\n",
        "\n",
        "* Each convolution should have kernel size 4, stride 2, padding 1. This will halve the width and height of the image at each step. The output channels of each convolution are given by the `hidden_channels` argument. For instance, if `img_channels=3` (because the image is RGB) and `hidden_channels=[128, 256, 512]`, then there will be three convolutions: the first mapping from 3 -> 128 channels, the second from 128 -> 256, and the third from 256 -> 512.\n",
        "* All blocks have a batchnorm layer, **except for the very first one**.\n",
        "* All blocks' activation functions are `LeakyRelu`.\n",
        "\n",
        "Lastly, we flatten the output of the final convolutional block, and use a fully connected layer to map it to a single value (i.e. a vector of length `batch_size`) which we then pass through a sigmoid to get a probability that the image is real. Again, we recommend the `Rearrange` module from the `einops` library for this.\n",
        "\n",
        "None of the convolutions or linear layers should have biases (this is also true for the generator).\n",
        "\n",
        "The diagram below shows what we'd get with the following arguments:\n",
        "\n",
        "```python\n",
        "img_size = 64\n",
        "img_channels = 3\n",
        "hidden_channels = [128, 256, 512]\n",
        "```\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/dcgan-d-help-9-solid.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCF9AE8HZiJ"
      },
      "source": [
        "#### Generator\n",
        "\n",
        "The generator is essentially the mirror image of the discriminator. While the discriminator had convolutions which halved the image size on each layer, the generator has transposed convolutions which double the size on each layer (so apart from the very start of the generator / end of the discriminator, all the activations have the same shape, just in reverse).\n",
        "\n",
        "We start with the latent vector of shape `(batch_size, latent_dim_size)`, and apply a fully connected layer & reshaping to get our first tensor which has shape `(batch_size, channels, height, width)`. The parameters `channels` and `height` (which is equal to `width`) can be calculated from the `img_size` and `hidden_channels` arguments (remember that image size doubles at each transposed convolution, and after applying all the transposed convolutions we'll eventually get back to `img_size`). Then, we apply batchnorm and relu.\n",
        "\n",
        "After this, we apply a series of blocks of the form `(ConvTranspose -> BatchNorm -> ActivationFunction)`. Following the paper's conventions:\n",
        "\n",
        "* Each transposed convolution has kernel size 4, stride 2, padding 1. Like for the discriminator, the input & output channels of the convolutions are determined by the `hidden_channels` argument (although this time they're in reverse order).\n",
        "* All blocks have a batchnorm layer, except for the very last one.\n",
        "* All blocks' activation functions are `ReLU`, except for the last one which is `Tanh`.\n",
        "\n",
        "The diagram below shows what we'd get with the following arguments:\n",
        "\n",
        "```python\n",
        "img_size = 64\n",
        "img_channels = 3\n",
        "hidden_channels = [128, 256, 512]\n",
        "latent_dim_size = 100\n",
        "```\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/gan_images/dcgan-g-help-10-light.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_tyajvtHZiJ"
      },
      "source": [
        "### Exercise - building your GAN\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴🔴\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 30-50 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should implement your code below. We've provided one possible design choice and the corresponding forward functions:\n",
        "\n",
        "- The generator is made of an initial `project_and_reshape` block that performs the first linear map, and then `hidden_layers` which are a stack of blocks each consisting of a (transponsed convolution, optional batchnorm, activation fn).\n",
        "- The discriminator is made of `hidden_layers` which are a stack of (convolution, optional batchnorm, activation fn) blocks, and a final `classifier` block which flattens and maps to a single output (which represents the probability pre-sigmoid).\n",
        "\n",
        "We've also given you the `DCGAN` class - note that we've not included a `forward` method here, because you'll usually be calling your discriminator and generators' forward methods directly. You can think of the DCGAN class as essentially a wrapper for both.\n",
        "\n",
        "If you're stuck, you can import the generator and discriminator from the solutions, and compare it with yours. We've given you this option in place of test functions.\n",
        "\n",
        "```python\n",
        "print_param_count(Generator(), solutions.DCGAN().netG)\n",
        "print_param_count(Discriminator(), solutions.DCGAN().netD)\n",
        "```\n",
        "\n",
        "Lastly, remember that `torchinfo` is a useful library for inspecting the architecture of your model. Since it works by running input through your model, it provides another useful way to check your model's architecture is correct (since errors like the wrong convolution size will often cause forward passes to fail).\n",
        "\n",
        "```python\n",
        "model = DCGAN().to(device)\n",
        "x = t.randn(3, 100).to(device)\n",
        "print(torchinfo.summary(model.netG, input_data=x), end=\"\\n\\n\")\n",
        "print(torchinfo.summary(model.netD, input_data=model.netG(x)))\n",
        "```\n",
        "\n",
        "You can also check that the output of your model is the correct shape. **Note - we're using a 3-layer model rather than the 4-layer model shown in the diagram and described the paper.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD1lJt-NHZiJ"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim_size: int = 100,\n",
        "        img_size: int = 64,\n",
        "        img_channels: int = 3,\n",
        "        hidden_channels: list[int] = [128, 256, 512],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implements the generator architecture from the DCGAN paper (the diagram at the top\n",
        "        of page 4). We assume the size of the activations doubles at each layer (so image\n",
        "        size has to be divisible by 2 ** len(hidden_channels)).\n",
        "\n",
        "        Args:\n",
        "            latent_dim_size:\n",
        "                the size of the latent dimension, i.e. the input to the generator\n",
        "            img_size:\n",
        "                the size of the image, i.e. the output of the generator\n",
        "            img_channels:\n",
        "                the number of channels in the image (3 for RGB, 1 for grayscale)\n",
        "            hidden_channels:\n",
        "                the number of channels in the hidden layers of the generator (starting closest\n",
        "                to the middle of the DCGAN and going outward, i.e. in chronological order for\n",
        "                the generator)\n",
        "        \"\"\"\n",
        "        n_layers = len(hidden_channels)\n",
        "        assert img_size % (2**n_layers) == 0, \"activation size must double at each layer\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # self.project_and_reshape = ...\n",
        "        # self.hidden_layers = ...\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.project_and_reshape(x)\n",
        "        x = self.hidden_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 64,\n",
        "        img_channels: int = 3,\n",
        "        hidden_channels: list[int] = [128, 256, 512],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implements the discriminator architecture from the DCGAN paper (the mirror image of\n",
        "        the diagram at the top of page 4). We assume the size of the activations doubles at\n",
        "        each layer (so image size has to be divisible by 2 ** len(hidden_channels)).\n",
        "\n",
        "        Args:\n",
        "            img_size:\n",
        "                the size of the image, i.e. the input of the discriminator\n",
        "            img_channels:\n",
        "                the number of channels in the image (3 for RGB, 1 for grayscale)\n",
        "            hidden_channels:\n",
        "                the number of channels in the hidden layers of the discriminator (starting\n",
        "                closest to the middle of the DCGAN and going outward, i.e. in reverse-\n",
        "                chronological order for the discriminator)\n",
        "        \"\"\"\n",
        "        n_layers = len(hidden_channels)\n",
        "        assert img_size % (2**n_layers) == 0, \"activation size must double at each layer\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_layers = ...\n",
        "        self.classifier = ...\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.hidden_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x.squeeze()  # remove dummy `out_channels` dimension\n",
        "\n",
        "\n",
        "class DCGAN(nn.Module):\n",
        "    netD: Discriminator\n",
        "    netG: Generator\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim_size: int = 100,\n",
        "        img_size: int = 64,\n",
        "        img_channels: int = 3,\n",
        "        hidden_channels: list[int] = [128, 256, 512],\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.latent_dim_size = latent_dim_size\n",
        "        self.img_size = img_size\n",
        "        self.img_channels = img_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.netD = Discriminator(img_size, img_channels, hidden_channels)\n",
        "        self.netG = Generator(latent_dim_size, img_size, img_channels, hidden_channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlHXc9nHZiJ"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class Generator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        latent_dim_size: int = 100,\n",
        "        img_size: int = 64,\n",
        "        img_channels: int = 3,\n",
        "        hidden_channels: list[int] = [128, 256, 512],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implements the generator architecture from the DCGAN paper (the diagram at the top\n",
        "        of page 4). We assume the size of the activations doubles at each layer (so image\n",
        "        size has to be divisible by 2 ** len(hidden_channels)).\n",
        "\n",
        "        Args:\n",
        "            latent_dim_size:\n",
        "                the size of the latent dimension, i.e. the input to the generator\n",
        "            img_size:\n",
        "                the size of the image, i.e. the output of the generator\n",
        "            img_channels:\n",
        "                the number of channels in the image (3 for RGB, 1 for grayscale)\n",
        "            hidden_channels:\n",
        "                the number of channels in the hidden layers of the generator (starting closest\n",
        "                to the middle of the DCGAN and going outward, i.e. in chronological order for\n",
        "                the generator)\n",
        "        \"\"\"\n",
        "        n_layers = len(hidden_channels)\n",
        "        assert img_size % (2**n_layers) == 0, \"activation size must double at each layer\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Reverse hidden channels, so they're in chronological order\n",
        "        hidden_channels = hidden_channels[::-1]\n",
        "\n",
        "        self.latent_dim_size = latent_dim_size\n",
        "        self.img_size = img_size\n",
        "        self.img_channels = img_channels\n",
        "        # Reverse them, so they're in chronological order for generator\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        # Define the first layer, i.e. latent dim -> (512, 4, 4) and reshape\n",
        "        first_height = img_size // (2**n_layers)\n",
        "        first_size = hidden_channels[0] * (first_height**2)\n",
        "        self.project_and_reshape = Sequential(\n",
        "            Linear(latent_dim_size, first_size, bias=False),\n",
        "            Rearrange(\"b (ic h w) -> b ic h w\", h=first_height, w=first_height),\n",
        "            BatchNorm2d(hidden_channels[0]),\n",
        "            ReLU(),\n",
        "        )\n",
        "\n",
        "        # Equivalent, but using conv rather than linear:\n",
        "        # self.project_and_reshape = Sequential(\n",
        "        #     Rearrange(\"b ic -> b ic 1 1\"),\n",
        "        #     solutions.ConvTranspose2d(latent_dim_size, hidden_channels[0], first_height, 1, 0),\n",
        "        #     BatchNorm2d(hidden_channels[0]),\n",
        "        #     ReLU(),\n",
        "        # )\n",
        "\n",
        "        # Get list of input & output channels for the convolutional blocks\n",
        "        in_channels = hidden_channels\n",
        "        out_channels = hidden_channels[1:] + [img_channels]\n",
        "\n",
        "        # Define all the convolutional blocks (conv_transposed -> batchnorm -> activation)\n",
        "        conv_layer_list = []\n",
        "        for i, (c_in, c_out) in enumerate(zip(in_channels, out_channels)):\n",
        "            conv_layer = [ConvTranspose2d(c_in, c_out, 4, 2, 1), ReLU() if i < n_layers - 1 else Tanh()]\n",
        "            if i < n_layers - 1:\n",
        "                conv_layer.insert(1, BatchNorm2d(c_out))\n",
        "            conv_layer_list.append(Sequential(*conv_layer))\n",
        "\n",
        "        self.hidden_layers = Sequential(*conv_layer_list)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.project_and_reshape(x)\n",
        "        x = self.hidden_layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 64,\n",
        "        img_channels: int = 3,\n",
        "        hidden_channels: list[int] = [128, 256, 512],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Implements the discriminator architecture from the DCGAN paper (the mirror image of\n",
        "        the diagram at the top of page 4). We assume the size of the activations doubles at\n",
        "        each layer (so image size has to be divisible by 2 ** len(hidden_channels)).\n",
        "\n",
        "        Args:\n",
        "            img_size:\n",
        "                the size of the image, i.e. the input of the discriminator\n",
        "            img_channels:\n",
        "                the number of channels in the image (3 for RGB, 1 for grayscale)\n",
        "            hidden_channels:\n",
        "                the number of channels in the hidden layers of the discriminator (starting\n",
        "                closest to the middle of the DCGAN and going outward, i.e. in reverse-\n",
        "                chronological order for the discriminator)\n",
        "        \"\"\"\n",
        "        n_layers = len(hidden_channels)\n",
        "        assert img_size % (2**n_layers) == 0, \"activation size must double at each layer\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.img_channels = img_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "\n",
        "        # Get list of input & output channels for the convolutional blocks\n",
        "        in_channels = [img_channels] + hidden_channels[:-1]\n",
        "        out_channels = hidden_channels\n",
        "\n",
        "        # Define all the convolutional blocks (conv_transposed -> batchnorm -> activation)\n",
        "        conv_layer_list = []\n",
        "        for i, (c_in, c_out) in enumerate(zip(in_channels, out_channels)):\n",
        "            conv_layer = [\n",
        "                Conv2d(c_in, c_out, 4, 2, 1),\n",
        "                LeakyReLU(0.2),\n",
        "            ]\n",
        "            if i > 0:\n",
        "                conv_layer.insert(1, BatchNorm2d(c_out))\n",
        "            conv_layer_list.append(Sequential(*conv_layer))\n",
        "\n",
        "        self.hidden_layers = Sequential(*conv_layer_list)\n",
        "\n",
        "        # Define the last layer, i.e. reshape and (512, 4, 4) -> real/fake classification\n",
        "        final_height = img_size // (2**n_layers)\n",
        "        final_size = hidden_channels[-1] * (final_height**2)\n",
        "        self.classifier = Sequential(\n",
        "            Rearrange(\"b c h w -> b (c h w)\"),\n",
        "            Linear(final_size, 1, bias=False),\n",
        "            Sigmoid(),\n",
        "        )\n",
        "        # Equivalent, but using conv rather than linear:\n",
        "        # self.classifier = Sequential(\n",
        "        #     Conv2d(out_channels[-1], 1, final_height, 1, 0),\n",
        "        #     Rearrange(\"b c h w -> b (c h w)\"),\n",
        "        #     Sigmoid(),\n",
        "        # )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.hidden_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x.squeeze()  # remove dummy `out_channels` dimension\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOw0nEQ2HZiJ"
      },
      "source": [
        "### Exercise - Weight initialisation\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "The paper mentions at the end of page 3 that all weights were initialized from a $N(0, 0.02)$ distribution. This applies to the convolutional and convolutional transpose layers' weights (plus the weights in the linear classifier), but the BatchNorm layers' weights should be initialised from $N(1, 0.02)$ (since 1 is their default value). The BatchNorm biases should all be set to zero.\n",
        "\n",
        "You can fill in the following function to initialise your weights, and call it within the `__init__` method of your DCGAN. (Hint: you can use the functions `nn.init.normal_` and `nn.init.constant_` here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cqat2rccHZiJ"
      },
      "outputs": [],
      "source": [
        "def initialize_weights(model: nn.Module) -> None:\n",
        "    \"\"\"\n",
        "    Initializes weights according to the DCGAN paper (details at the end of page 3 of the DCGAN paper), by modifying the\n",
        "    weights of the model in place.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_initialize_weights(initialize_weights, ConvTranspose2d, Conv2d, Linear, BatchNorm2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1adHOVxHZiJ"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def initialize_weights(model: nn.Module) -> None:\n",
        "    \"\"\"\n",
        "    Initializes weights according to the DCGAN paper (details at the end of page 3 of the DCGAN paper), by modifying the\n",
        "    weights of the model in place.\n",
        "    \"\"\"\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, (ConvTranspose2d, Conv2d, Linear)):\n",
        "            nn.init.normal_(module.weight.data, 0.0, 0.02)\n",
        "        elif isinstance(module, BatchNorm2d):\n",
        "            nn.init.normal_(module.weight.data, 1.0, 0.02)\n",
        "            nn.init.constant_(module.bias.data, 0.0)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYRVcF7HZiJ"
      },
      "source": [
        "Note - the tests for this aren't maximally strict, but don't worry if you don't get things exactly right, since your model will still probably train successfully. If you think you've got the architecture right but your model still isn't training, you might want to return here and check your initialisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-q3jvlhHZiJ"
      },
      "outputs": [],
      "source": [
        "model = DCGAN().to(device)\n",
        "x = t.randn(3, 100).to(device)\n",
        "print(torchinfo.summary(model.netG, input_data=x), end=\"\\n\\n\")\n",
        "print(torchinfo.summary(model.netD, input_data=model.netG(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3eZTezyHZiK"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "Recall, the goal of training the discriminator is to maximize the probability of correctly classifying a given input as real or fake. The goal of the generator is to produce images to fool the discriminator. This is framed as a **minimax game**, where the discriminator and generator try to solve the following:\n",
        "$$\n",
        "\\min_G \\max_D V(D, G)=\\mathbb{E}_x[\\log (D(x))]+\\mathbb{E}_z[\\log (1-D(G(z)))]\n",
        "$$\n",
        "where $D$ is the discriminator function mapping an image to a probability estimate for whether it is real, and $G$ is the generator function which produces an image from latent vector $z$.\n",
        "\n",
        "The literature on minimax games is extensive, so we won't go into it here. It's better to understand this formula on an intuitive level:\n",
        "\n",
        "* Given a fixed $G$ (generator), the goal of the discriminator is to produce high values for $D$ when fed real images $x$, and low values when fed fake images $G(z)$.\n",
        "* The generator $G$ is searching for a strategy where, even if the discriminator $D$ was optimal, it would still find it hard to distinguish between real and fake images with high confidence.\n",
        "\n",
        "Since we can't know the true distribution of $x$, we instead estimate the expression above by calculating it over a batch of real images $x$ (and some random noise $z$). This gives us a loss function to train against (since $D$ wants to maximise this value, and $G$ wants to minimise this value). For each batch, we perform gradient descent on the discriminator and then on the generator.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK6MT4FIHZiK"
      },
      "source": [
        "### Training the discriminator\n",
        "\n",
        "We take the following steps:\n",
        "\n",
        "* Zero the gradients of $D$.\n",
        "    * This is important because if the last thing we did was evaluate $D(G(z))$ (in order to update the parameters of $G$), then $D$ will have stored gradients from that backward pass.\n",
        "* Generate random noise $z$, and compute $D(G(z))$. Take the average of $\\log(1 - D(G(z)))$, and we have the first part of our loss function.\n",
        "    * Note - you can use the same random noise (and even the same fake image) as in the generator step. But make sure you're using the detached version, because we don't want gradients to propagate back through the generator!\n",
        "* Take the real images  $x$ in the current batch, and use that to compute $\\log(D(x))$. This gives us the second part of our loss function.\n",
        "* We now add the two terms together, and perform gradient ascent (since we're trying to maximise this expression).\n",
        "    * You can perform gradient ascent by either flipping the sign of the thing you're doing a backward pass on, or passing the keyword argument `maximize=True` when defining your optimiser (all optimisers have this option).\n",
        "\n",
        "Tip - when calculating $D(G(z))$, for the purpose of training the discriminator, it's best to first calculate $G(z)$ then call `detach` on this tensor before passing it to $D$. This is because you then don't need to worry about gradients accumulating for $G$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ2-pEdPHZiK"
      },
      "source": [
        "### Training the generator\n",
        "\n",
        "We take the following steps:\n",
        "\n",
        "* Zero the gradients of $G$.\n",
        "* Generate random noise $z$, and compute $D(G(z))$.\n",
        "* We **don't** use $\\log(1 - D(G(z)))$ to calculate our loss function, instead we use $\\log(D(G(z)))$ (and gradient ascent).\n",
        "\n",
        "**Question - can you explain why we use $\\log(D(G(z))$? (The Google reading material mentions this but doesn't really explain it.)**\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "Early in learning, when the generator is really bad at producing realistic images, it will be easy for the discriminator to distinguish between them. So $\\log(1 - D(G(z)))$ will be very close to $\\log(1) = 0$. The gradient of $\\log$ at this point is quite flat, so there won't be a strong gradient with which to train $G$. To put it another way, a marginal improvement in $G$ will have very little effect on the loss function. On the other hand, $\\log(D(G(z)))$ tends to negative infinity as $D(G(z))$ gets very small. So the gradients here are very steep, and a small improvement in $G$ goes a long way.\n",
        "\n",
        "It's worth emphasising that these two functions are both monotonic in opposite directions, so maximising one is equivalent to minimising the other. We haven't changed anything fundamental about how the GAN works; this is just a trick to help with gradient descent.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieOLCvaNHZiK"
      },
      "source": [
        "Note - PyTorch's [`BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) clamps its log function outputs to be greater than or equal to -100. This is because in principle our loss function could be negative infinity (if we take log of zero). You might find you need to employ a similar trick if you're manually computing the log of probabilities. Aside from the clamping, the following two code snippets are equivalent:\n",
        "\n",
        "```python\n",
        "# Calculating loss manually, without clamping:\n",
        "loss = - t.log(D_G_z)\n",
        "\n",
        "# Calculating loss with clamping behaviour:\n",
        "labels_real = t.ones_like(D_G_z)\n",
        "loss = nn.BCELoss()(D_G_z, labels_real)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RcznQM2HZiK"
      },
      "source": [
        "### Optimizers\n",
        "\n",
        "The generator and discriminator will have separate optimizers (this makes sense, since we'll have separate training steps for these two, and both are \"trying\" to optimize different things). The [paper](https://arxiv.org/abs/1511.06434v2) describes using an Adam optimizer with learning rate 0.0002, and momentum parameters $\\beta_1 = 0.5, \\beta_2 = 0.999$. This is set up for you already, in the `__init__` block below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9salj9uRHZiK"
      },
      "source": [
        "### Gradient Clipping\n",
        "\n",
        "Gradient clipping is a useful technique for improving the stability of certain training loops, especially those like DCGANs which have potentially unstable loss functions. The idea is that you clip the gradients of your weights to some fixed threshold during backprop, and use these clipped gradients to update the weights. This can be done using [`nn.utils.clip_grad_norm`](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html), which is called between the `loss.backward()` and `optimizer.step()` methods (since it directly modifies the `.grad` attributes of your weights). You shouldn't find this absolutely necessary to train your models, however it might help to clip the gradients to a value like `1.0` for your generator & discriminator. We've given you this as an optional parameter to use in your `DCGANArgs` dataclass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlZcrAiTHZiK"
      },
      "source": [
        "### Exercise - implement GAN training loop\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵🔵🔵⚪\n",
        ">\n",
        "> You should spend up to 30-45 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "You should now implement your training loop below. We've filled in the `__init__` method for you, as well as `log_samples` method which determines the core structure of the training loop. Your task is to:\n",
        "\n",
        "* Fill in the two functions `training_step_discriminator` and `training_step_generator`, which perform a single gradient step on the discriminator and generator respectively.\n",
        "    * Note that the discriminator training function takes two arguments: the real and fake image (in the notation above, $x$ and $z$), because it trains to distinguish real and fake. The generator training function only takes the fake image $z$, because it trains to fool the discriminator.\n",
        "    * Also note, you should increment `self.step` only once per (discriminator & generator) step, not for both.\n",
        "* Fill in the `train` method, which should perform the training loop over the number of epochs specified in `args.epochs`. This will be similar to previous training loops, but with a few key differences we'll highlight here:\n",
        "    * You'll need to compute both losses from `training_step_generator` and `training_step_discriminator`. For the former you should pass in just the fake image (you're only training the generator to produce better fake images), for the latter you should pass in the real image and the **detached fake image** i.e. `img.detach()` (because you're training the discriminator to tell real from fake, and you don't want gradients propagating back to the generator).\n",
        "        * The fake image should be created from random noise `t.randn(batch_size, latent_dim_size)` and passing it into your generator.\n",
        "    * Once again the trainloader gives us an iterable of `(img, label)` but we don't need to use the labels (because all these images are real, and that's all we care about).\n",
        "\n",
        "Again, we recommend not using `wandb` until you've got your non-wandb based code working without errors. Once the generator loss is going down (or at least not exploding!) then you can enable it. However, an important note - **generator loss going down is does not imply the model is working, and vice-versa!** For training systems as unstable as GANs, the best you can do is often just inspecting the output. Although it varies depending on details of the hardware and dataset & model you're training with, at least for these exercises if your generator's output doesn't resemble anything like a face after the first epoch, then something's probably going wrong in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH0MZQdLHZiK"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DCGANArgs:\n",
        "    \"\"\"\n",
        "    Class for the arguments to the DCGAN (training and architecture).\n",
        "    Note, we use field(defaultfactory(...)) when our default value is a mutable object.\n",
        "    \"\"\"\n",
        "\n",
        "    # architecture\n",
        "    latent_dim_size: int = 100\n",
        "    hidden_channels: list[int] = field(default_factory=lambda: [128, 256, 512])\n",
        "\n",
        "    # data & training\n",
        "    dataset: Literal[\"MNIST\", \"CELEB\"] = \"CELEB\"\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 3\n",
        "    lr: float = 0.0002\n",
        "    betas: tuple[float, float] = (0.5, 0.999)\n",
        "    clip_grad_norm: float | None = 1.0\n",
        "\n",
        "    # logging\n",
        "    use_wandb: bool = False\n",
        "    wandb_project: str | None = \"day5-gan\"\n",
        "    wandb_name: str | None = None\n",
        "    log_every_n_steps: int = 250\n",
        "\n",
        "\n",
        "class DCGANTrainer:\n",
        "    def __init__(self, args: DCGANArgs):\n",
        "        self.args = args\n",
        "        self.trainset = get_dataset(self.args.dataset)\n",
        "        self.trainloader = DataLoader(self.trainset, batch_size=args.batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "        batch, img_channels, img_height, img_width = next(iter(self.trainloader))[0].shape\n",
        "        assert img_height == img_width\n",
        "\n",
        "        self.model = DCGAN(args.latent_dim_size, img_height, img_channels, args.hidden_channels).to(device).train()\n",
        "        self.optG = t.optim.Adam(self.model.netG.parameters(), lr=args.lr, betas=args.betas)\n",
        "        self.optD = t.optim.Adam(self.model.netD.parameters(), lr=args.lr, betas=args.betas)\n",
        "\n",
        "    def training_step_discriminator(\n",
        "        self,\n",
        "        img_real: Float[Tensor, \"batch channels height width\"],\n",
        "        img_fake: Float[Tensor, \"batch channels height width\"],\n",
        "    ) -> Float[Tensor, \"\"]:\n",
        "        \"\"\"\n",
        "        Generates a real and fake image, and performs a gradient step on the discriminator to maximize\n",
        "        log(D(x)) + log(1-D(G(z))). Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def training_step_generator(self, img_fake: Float[Tensor, \"batch channels height width\"]) -> Float[Tensor, \"\"]:\n",
        "        \"\"\"\n",
        "        Performs a gradient step on the generator to maximize log(D(G(z))). Logs to wandb if enabled.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def log_samples(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs evaluation by generating 8 instances of random noise and passing them through the generator, then\n",
        "        optionally logging the results to Weights & Biases.\n",
        "        \"\"\"\n",
        "        assert self.step > 0, \"First call should come after a training step. Remember to increment `self.step`.\"\n",
        "        self.model.netG.eval()\n",
        "\n",
        "        # Generate random noise\n",
        "        t.manual_seed(42)\n",
        "        noise = t.randn(10, self.model.latent_dim_size).to(device)\n",
        "        # Get generator output\n",
        "        output = self.model.netG(noise)\n",
        "        # Clip values to make the visualization clearer\n",
        "        output = output.clamp(output.quantile(0.01), output.quantile(0.99))\n",
        "        # Log to weights and biases\n",
        "        if self.args.use_wandb:\n",
        "            output = einops.rearrange(output, \"b c h w -> b h w c\").cpu().numpy()\n",
        "            wandb.log({\"images\": [wandb.Image(arr) for arr in output]}, step=self.step)\n",
        "        else:\n",
        "            display_data(output, nrows=1, title=\"Generator-produced images\")\n",
        "\n",
        "        self.model.netG.train()\n",
        "\n",
        "    def train(self) -> DCGAN:\n",
        "        \"\"\"Performs a full training run.\"\"\"\n",
        "        self.step = 0\n",
        "        if self.args.use_wandb:\n",
        "            wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "\n",
        "        for epoch in range(self.args.epochs):\n",
        "            progress_bar = tqdm(self.trainloader, total=len(self.trainloader), ascii=True)\n",
        "\n",
        "            for img_real, label in progress_bar:\n",
        "                # YOUR CODE HERE - fill in the training step for generator & discriminator\n",
        "\n",
        "        if self.args.use_wandb:\n",
        "            wandb.finish()\n",
        "\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4oEt-gyHZiK"
      },
      "source": [
        "<details>\n",
        "<summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def training_step_discriminator(\n",
        "    self,\n",
        "    img_real: Float[Tensor, \"batch channels height width\"],\n",
        "    img_fake: Float[Tensor, \"batch channels height width\"],\n",
        ") -> Float[Tensor, \"\"]:\n",
        "    \"\"\"\n",
        "    Generates a real and fake image, and performs a gradient step on the discriminator to maximize\n",
        "    log(D(x)) + log(1-D(G(z))). Logs to wandb if enabled.\n",
        "    \"\"\"\n",
        "    # Zero gradients\n",
        "    self.optD.zero_grad()\n",
        "\n",
        "    # Calculate D(x) and D(G(z)), for use in the objective function\n",
        "    D_x = self.model.netD(img_real)\n",
        "    D_G_z = self.model.netD(img_fake)\n",
        "\n",
        "    # Calculate loss\n",
        "    lossD = -(t.log(D_x).mean() + t.log(1 - D_G_z).mean())\n",
        "\n",
        "    # Gradient descent step (with optional clipping)\n",
        "    lossD.backward()\n",
        "    if self.args.clip_grad_norm is not None:\n",
        "        nn.utils.clip_grad_norm_(self.model.netD.parameters(), self.args.clip_grad_norm)\n",
        "    self.optD.step()\n",
        "\n",
        "    if self.args.use_wandb:\n",
        "        wandb.log(dict(lossD=lossD), step=self.step)\n",
        "    return lossD\n",
        "\n",
        "\n",
        "def training_step_generator(self, img_fake: Float[Tensor, \"batch channels height width\"]) -> Float[Tensor, \"\"]:\n",
        "    \"\"\"\n",
        "    Performs a gradient step on the generator to maximize log(D(G(z))). Logs to wandb if enabled.\n",
        "    \"\"\"\n",
        "    # Zero gradients\n",
        "    self.optG.zero_grad()\n",
        "\n",
        "    # Calculate D(G(z)), for use in the objective function\n",
        "    D_G_z = self.model.netD(img_fake)\n",
        "\n",
        "    # Calculate loss\n",
        "    lossG = -(t.log(D_G_z).mean())\n",
        "\n",
        "    # Gradient descent step (with optional clipping)\n",
        "    lossG.backward()\n",
        "    if self.args.clip_grad_norm is not None:\n",
        "        nn.utils.clip_grad_norm_(self.model.netG.parameters(), self.args.clip_grad_norm)\n",
        "    self.optG.step()\n",
        "\n",
        "    if self.args.use_wandb:\n",
        "        wandb.log(dict(lossG=lossG), step=self.step)\n",
        "    return lossG\n",
        "\n",
        "\n",
        "def train(self) -> DCGAN:\n",
        "    \"\"\"Performs a full training run.\"\"\"\n",
        "    self.step = 0\n",
        "    if self.args.use_wandb:\n",
        "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name)\n",
        "\n",
        "    for epoch in range(self.args.epochs):\n",
        "        progress_bar = tqdm(self.trainloader, total=len(self.trainloader), ascii=True)\n",
        "\n",
        "        for img_real, label in progress_bar:\n",
        "            # Generate random noise & fake image\n",
        "            noise = t.randn(self.args.batch_size, self.args.latent_dim_size).to(device)\n",
        "            img_real = img_real.to(device)\n",
        "            img_fake = self.model.netG(noise)\n",
        "\n",
        "            # Training steps\n",
        "            lossD = self.training_step_discriminator(img_real, img_fake.detach())\n",
        "            lossG = self.training_step_generator(img_fake)\n",
        "\n",
        "            # Update progress bar\n",
        "            self.step += 1\n",
        "            progress_bar.set_description(f\"{epoch=}, {lossD=:.4f}, {lossG=:.4f}, batches={self.step}\")\n",
        "\n",
        "            # Log batch of data\n",
        "            if self.step % self.args.log_every_n_steps == 0:\n",
        "                self.log_samples()\n",
        "\n",
        "    if self.args.use_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "    return self.model\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os2aTKhlHZiK"
      },
      "source": [
        "Once you've written your code, here are some default arguments for MNIST and CelebA you can try out.\n",
        "\n",
        "Note that the MNIST model is very small in comparison to CelebA - if you make it any larger, you fall into a very common GAN failure mode where the discriminator becomes perfect (loss goes to zero) and the generator is unable to get a gradient signal to produce better images - see next section for a discussion of this. Larger architectures are generally more likely to fall into this failure mode, and empirically it seems to happen more for MNIST than for CelebA which is why we generally recommend using the CelebA dataset & architecture for this exercise - although this failure mode can happen in both cases!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuIYqL4PHZiK"
      },
      "outputs": [],
      "source": [
        "# Arguments for CelebA\n",
        "args = DCGANArgs(\n",
        "    dataset=\"CELEB\",\n",
        "    hidden_channels=[128, 256, 512],\n",
        "    batch_size=32,  # if you get OOM errors, reduce this!\n",
        "    epochs=5,\n",
        "    use_wandb=False,\n",
        ")\n",
        "trainer = DCGANTrainer(args)\n",
        "dcgan = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KgCB52LHZiK"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see an example of the output you should be producing by the end of this CelebA training run.</summary>\n",
        "\n",
        "Here was my output after 250 batches (8000 images):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/celeb-gans-1.png\" width=\"700\">\n",
        "\n",
        "After 2000 batches (64000 images):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/celeb-gans-2.png\" width=\"700\">\n",
        "\n",
        "And after the end of training (5 epochs, approx 625k images):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/celeb-gans-3.png\" width=\"700\">\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrLKpwu3HZiK"
      },
      "outputs": [],
      "source": [
        "# Arguments for MNIST\n",
        "args = DCGANArgs(\n",
        "    dataset=\"MNIST\",\n",
        "    hidden_channels=[12, 24],\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    use_wandb=False,\n",
        ")\n",
        "trainer = DCGANTrainer(args)\n",
        "dcgan = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhDMttpWHZiK"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see an example of the output you should be producing by the end of this MNIST training run.</summary>\n",
        "\n",
        "Here was my output after 250 batches (32k images):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/mnist-gans-1.png\" width=\"700\">\n",
        "\n",
        "After 2000 batches (256k images):\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/mnist-gans-2.png\" width=\"700\">\n",
        "\n",
        "About 90% of the way through training, it was achieving the best results:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/mnist-gans-3.png\" width=\"700\">\n",
        "\n",
        "However after this point it broke, and produced NaNs from both the discriminator and the generator:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/mnist-gans-nan.png\" width=\"900\">\n",
        "\n",
        "This is a common problem for training GANs - they're just a pretty cursed architecture! Read on for more on this.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izq3MdDPHZiK"
      },
      "source": [
        "### Fixing bugs\n",
        "\n",
        "GANs are notoriously hard to get exactly right. I ran into quite a few bugs myself building this architecture, and I've tried to mention them somewhere on this page to help participants avoid them. If you run into a bug and are able to fix it, please send it to me and I can add it here, for the benefit of everyone else!\n",
        "\n",
        "* Make sure you apply the layer normalization (mean 0, std dev 0.02) to your linear layers as well as your convolutional layers.\n",
        "* More generally, in your function to initialise the weights of your network, make sure no layers are being missed out. The easiest way to do this is to inspect your model afterwards (i.e. loop through all the params, printing out their mean and std dev).\n",
        "\n",
        "Also, you might find [this page](https://github.com/soumith/ganhacks) useful. It provides several tips and tricks for how to make your GAN work (many of which we've already mentioned on this page)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNs96sVwHZiK"
      },
      "source": [
        "## Why so unstable during training?\n",
        "\n",
        "If you try training your GAN on MNIST, you might find that it eventually blows up (with close to zero discriminator loss, and spiking generator loss - possibly even gradients large enough to overflow and lead to `nan` values). This might also happen if you train on CelebA but your architecture is too big, or even if you train with a reasonably-sized architecture but for too long!\n",
        "\n",
        "This is a common problem with GANs, which are notoriously unstable to train. Essentially, the discriminator gets so good at its job that the generator can't latch onto a good gradient for improving its performance. Although the theoretical formulation of GANs as a minimax game is elegant, there are quite a few assumptions that have to go into it in order for there to be one theoretical optimum involving the generator producing perfect images - and in practice this is rarely achieved, even in the limit.\n",
        "\n",
        "Different architectures like diffusion models and VAEs are generally more stable, although many of the most advanced image generation architectures do still take important conceptual ideas from the GAN framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSRNVARbHZiK"
      },
      "source": [
        "## Bonus - Smooth interpolation\n",
        "\n",
        "Suppose you take two vectors in the latent space. If you use your generator to create output at points along the linear interpolation between these vectors, your image will change continuously (because it is a continuous function of the latent vector), but it might look very different at the start and the end. Can you create any cool animations from this?\n",
        "\n",
        "Instead of linearly interpolating between two vectors, you could try applying a [rotation matrix](https://en.wikipedia.org/wiki/Rotation_matrix) to a vector (this has the advantage of keeping the interpolated vector \"in distribution\", since the rotation between two standard normally distributed vectors is also standard normal, whereas the linear interpolation isn't). Are the results better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4syFxJ2HZiK"
      },
      "source": [
        "# 3️⃣ Bonus - Transposed Convolutions\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> - Learn about & implement the transposed convolution operation.\n",
        "> - Implement GANs and/or VAEs entirely from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvSfEYjHZiK"
      },
      "source": [
        "## Transposed convolutions\n",
        "\n",
        "In this section, we'll build all the modules required to implement our DCGAN.\n",
        "\n",
        "> Note - this section is similar in flavour to the bonus exercises from the \"CNNs & ResNets\" chapter, i.e. you'll be implementing transposed convolutions using low-level stride and tensor manipulation operations. That section should be considered a prerequisite for this one.\n",
        "\n",
        "Let's start by importing some useful functions from the bonus section of \"CNNs & ResNets\", where we implemented convolutions from scratch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvQglF-THZiK"
      },
      "outputs": [],
      "source": [
        "from part2_cnns.solutions import IntOrPair, Pair, conv1d_minimal, conv2d_minimal, force_pair, pad1d, pad2d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGOfrMADHZiK"
      },
      "source": [
        "Now, **what are transposed convolutions, and why should we care about them?** One high-level intuition goes something like this: most of the generator's architecture is basically the discriminator architecture in reverse. We need something that performs the reverse of a convolution - not literally the inverse operation, but something reverse in spirit, which uses a kernel of weights to project up to some array of larger size.\n",
        "\n",
        "**Importantly, a transposed convolution isn't literally the inverse of a convolution**. A lot of confusion can come from misunderstanding this!\n",
        "\n",
        "You can describe the difference between convolutions and transposed convolutions as follows:\n",
        "\n",
        "* In convolutions, you slide the kernel around inside the input. At each position of the kernel, you take a sumproduct between the kernel and that section of the input to calculate a single element in the output.\n",
        "* In transposed convolutions, you slide the kernel around what will eventually be your output, and at each position you add some multiple of the kernel to your output.\n",
        "\n",
        "Below is an illustration of both for comparison, in the 1D case (where $*$ stands for the 1D convolution operator, and $*^T$ stands for the transposed convolution operator). Note the difference in size between the output in both cases. With standard convolutions, our output is smaller than our input, because we're having to fit the kernel inside the input in order to produce the output. But in our transposed convolutions, the output is actually larger than the input, because we're fitting the kernel inside the output.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/convtranspose-1.png\" width=\"700\">\n",
        "\n",
        "\n",
        "<details>\n",
        "<summary>Question - what do you think the formula is relating <code>input_size</code>, <code>kernel_size</code> and <code>output_size</code> in the case of 1D convolutions (with no padding or stride)?</summary>\n",
        "\n",
        "The formula is `output_size = input_size + kernel_size - 1`.\n",
        "        \n",
        "Note how this exactly mirrors the equation in the convolutional case; it's identical if we swap around `output_size` and `input_size`.\n",
        "</details>\n",
        "\n",
        "<br>\n",
        "\n",
        "Now, consider the elements in the output of the transposed convolution: `z+4y+3x, 4x+3y-2x`, etc. Note that these look a bit like convolutions, since they're inner products of slices of the input with versions of the kernel. This observation leads nicely into why transposed convolutions are called transposed convolutions - because they can actually be written as convolutions, just with a slightly modified input and kernel.\n",
        "\n",
        "<details>\n",
        "<summary>Question - how can this operation be cast as a convolution? In other words, exactly what arrays <code>input</code> and <code>kernel</code> would produce the same output as the transposed convolution above, if we performed a standard convolution on them?</summary>\n",
        "\n",
        "From looking at the diagram, note that the final output (the blue row at the bottom) looks a bit like sliding the _reversed_ kernel over the input. In other words, we get elements like `z+4y+3x` which are an inner product between the input slice `input[:3] = [1, 4, 3]` and the reversed kernel `[z, y, x]`. This suggests we should be using the reversed kernel in our convolution.\n",
        "\n",
        "Can we just use a reversed kernel on our original input and call it a day? No, because the output size wouldn't be correct. Using a reversed kernel on our original input would give us just the two elements `[z+4y+3x, 4z+3y-2x]`, not the full 6-element output we actually get. The answer is that we need to pad out our input with zeros on the left and right, with the padding amount equal to `kernel_size - 1`.\n",
        "\n",
        "To conclude - with `input_modified = pad(input, kernel_size-1)` and `kernel_modified = kernel[::-1]`, we get:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/convtranspose-2A.png\" width=\"850\">\n",
        "\n",
        "Note - it's also valid to say we use the original kernel and pad & flip the input, but for the exercises below we'll stick to the former interpretation.\n",
        "\n",
        "</details>\n",
        "\n",
        ">\n",
        "\n",
        "<!-- <details>\n",
        "<summary>Hint</summary>\n",
        "\n",
        "Let `input_mod` and `kernel_mod` be the modified versions of the input and kernel, to be used in the convolution.\n",
        "\n",
        "You should be able to guess what `kernel_mod` is by looking at the diagram.\n",
        "\n",
        "Also, from the formula for transposed convolutions, we must have:\n",
        "\n",
        "```\n",
        "output_size = input_mod_size + kernel_mod_size - 1\n",
        "```\n",
        "\n",
        "But we currently have:\n",
        "\n",
        "```\n",
        "output_size = input_size - kernel_size + 1\n",
        "```\n",
        "\n",
        "which should help you figure out what size `input_mod` needs to be, relative to `input`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Hint 2</summary>\n",
        "\n",
        "`kernel_mod` should be the same size as kernel (but altered in a particular way). `input_mod` should be formed by padding `input`, so that its size increases by `2 * (kernel_size - 1)`.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "If you create `input_mod` by padding `input` with exactly `kernel_size - 1` zeros on either side, and reverse your kernel to create `kernel_mod`, then the convolution of these modified arrays equals your original transposed convolution output.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/convtranspose-2A.png\" width=\"850\">\n",
        "\n",
        "</details> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0HOAebjHZiL"
      },
      "source": [
        "### Exercise - minimal 1D transposed convolutions\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 15-25 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Now, you should implement the function `conv_transpose1d_minimal`. You're allowed to call functions like `conv1d_minimal` and `pad1d` which you wrote previously (if you didn't do these exercises, then you can import the solution versions of them - although we do recommend doing the conv from scratch exercises before these ones).\n",
        "\n",
        "One important note - in our convolutions we assumed the kernel had shape `(out_channels, in_channels, kernel_width)`. Here, the order is different: `in_channels` comes before `out_channels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDxTLxZJHZiL"
      },
      "outputs": [],
      "source": [
        "def conv_transpose1d_minimal(\n",
        "    x: Float[Tensor, \"batch in_channels width\"],\n",
        "    weights: Float[Tensor, \"in_channels out_channels kernel_width\"],\n",
        ") -> Float[Tensor, \"batch out_channels output_width\"]:\n",
        "    \"\"\"Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_conv_transpose1d_minimal(conv_transpose1d_minimal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZidewFxoHZiL"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def conv_transpose1d_minimal(\n",
        "    x: Float[Tensor, \"batch in_channels width\"],\n",
        "    weights: Float[Tensor, \"in_channels out_channels kernel_width\"],\n",
        ") -> Float[Tensor, \"batch out_channels output_width\"]:\n",
        "    \"\"\"Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\"\"\"\n",
        "    batch, in_channels, width = x.shape\n",
        "    in_channels_2, out_channels, kernel_width = weights.shape\n",
        "    assert in_channels == in_channels_2, \"in_channels for x and weights don't match up\"\n",
        "\n",
        "    x_mod = pad1d(x, left=kernel_width - 1, right=kernel_width - 1, pad_value=0)\n",
        "    weights_mod = einops.rearrange(weights.flip(-1), \"i o w -> o i w\")\n",
        "\n",
        "    return conv1d_minimal(x_mod, weights_mod)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec8I1E2JHZiL"
      },
      "source": [
        "### Exercise - 1D transposed convolutions\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴🔴\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 25-40 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Now we add in the extra parameters `padding` and `stride`, just like we did for our convolutions back in week 0.\n",
        "\n",
        "The basic idea is that both parameters mean the inverse of what they did in for convolutions.\n",
        "\n",
        "In convolutions, `padding` tells you how much to pad the input by. But in transposed convolutions, we pad the input by `kernel_size - 1 - padding` (recall that we're already padding by `kernel_size - 1` by default). So padding decreases our output size rather than increasing it.\n",
        "\n",
        "In convolutions, `stride` tells you how much to step the kernel by, as it's being moved around inside the input. In transposed convolutions, stride does something different: you space out all your input elements by an amount equal to `stride` before performing your transposed convolution. This might sound strange, but **it's actually equivalent to performing strides as you're moving the kernel around inside the output.** This diagram should help show why:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/info-arena/ARENA_img/main/misc/convtranspose-3.png\" width=\"750\">\n",
        "\n",
        "For this reason, transposed convolutions are also referred to as **fractionally strided convolutions**, since a stride of 2 over the output is equivalent to a 1/2 stride over the input (i.e. every time the kernel takes two steps inside the spaced-out version of the input, it moves one stride with reference to the original input).\n",
        "\n",
        "**Question - what is the formula relating output size, input size, kernel size, stride and padding? (note, you shouldn't need to refer to this explicitly in your functions)**\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "\n",
        "Without any padding, we had:\n",
        "\n",
        "```\n",
        "output_size = input_size + kernel_size - 1\n",
        "```\n",
        "\n",
        "Twice the `padding` parameter gets subtracted from the RHS (since we pad by the same amount on each side), so this gives us:\n",
        "\n",
        "```\n",
        "output_size = input_size + kernel_size - 1 - 2 * padding\n",
        "```\n",
        "\n",
        "Finally, consider `stride`. As mentioned above, we can consider stride here to have the same effect as \"spacing out\" elements in the input. Each non-zero element will be `stride - 1` positions apart (for instance, `stride = 2` turns `[1, 2, 3]` into `[1, 0, 2, 0, 3]`). You can check that the number of zeros added between elements equals `(input_size - 1) * (stride - 1)`. When you add this to the right hand side, and simplify, you are left with:\n",
        "\n",
        "```\n",
        "output_size = (input_size - 1) * stride + kernel_size - 2 * padding\n",
        "```\n",
        "</details>\n",
        "\n",
        "Padding should be pretty easy for you to implement on top of what you've already done. For strides, you will need to construct a strided version of the input which is \"spaced out\" in the way described above, before performing the transposed convolution. It might help to write a `fractional_stride` function; we've provided the code for you to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi1i-vfQHZiL"
      },
      "outputs": [],
      "source": [
        "def fractional_stride_1d(\n",
        "    x: Float[Tensor, \"batch in_channels width\"], stride: int = 1\n",
        ") -> Float[Tensor, \"batch in_channels output_width\"]:\n",
        "    \"\"\"Returns a version of x suitable for transposed convolutions, i.e. \"spaced out\" with zeros between its values.\n",
        "    This spacing only happens along the last dimension.\n",
        "    x: shape (batch, in_channels, width)\n",
        "    Example:\n",
        "        x = [[[1, 2, 3], [4, 5, 6]]]\n",
        "        stride = 2\n",
        "        output = [[[1, 0, 2, 0, 3], [4, 0, 5, 0, 6]]]\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_fractional_stride_1d(fractional_stride_1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItsAt320HZiL"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure how to implement <code>fractional_stride</code>.</summary>\n",
        "\n",
        "The easiest way is to initialise an array of zeros with the appropriate size, then slicing to set its elements from `x`.\n",
        "\n",
        "Warning - if you do it this way, **make sure the output has the same device as `x`**.\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def fractional_stride_1d(\n",
        "    x: Float[Tensor, \"batch in_channels width\"], stride: int = 1\n",
        ") -> Float[Tensor, \"batch in_channels output_width\"]:\n",
        "    \"\"\"Returns a version of x suitable for transposed convolutions, i.e. \"spaced out\" with zeros between its values.\n",
        "    This spacing only happens along the last dimension.\n",
        "    x: shape (batch, in_channels, width)\n",
        "    Example:\n",
        "        x = [[[1, 2, 3], [4, 5, 6]]]\n",
        "        stride = 2\n",
        "        output = [[[1, 0, 2, 0, 3], [4, 0, 5, 0, 6]]]\n",
        "    \"\"\"\n",
        "    batch, in_channels, width = x.shape\n",
        "    width_new = width + (stride - 1) * (\n",
        "        width - 1\n",
        "    )  # the RHS of this sum is the number of zeros we need to add between elements\n",
        "    x_new_shape = (batch, in_channels, width_new)\n",
        "\n",
        "    # Create an empty array to store the spaced version of x in.\n",
        "    x_new = t.zeros(size=x_new_shape, dtype=x.dtype, device=x.device)\n",
        "\n",
        "    x_new[..., ::stride] = x\n",
        "\n",
        "    return x_new\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXqqEoCmHZiL"
      },
      "outputs": [],
      "source": [
        "def conv_transpose1d(\n",
        "    x: Float[Tensor, \"batch in_channels width\"],\n",
        "    weights: Float[Tensor, \"in_channels out_channels kernel_width\"],\n",
        "    stride: int = 1,\n",
        "    padding: int = 0,\n",
        ") -> Float[Tensor, \"batch out_channels output_width\"]:\n",
        "    \"\"\"Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_conv_transpose1d(conv_transpose1d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DqVRa2aHZiL"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure how to implement <code>conv_transpose1d</code>.</summary>\n",
        "\n",
        "There are three things you need to do:\n",
        "\n",
        "* Modify `x` by \"spacing it out\" with `fractional_stride_1d` and padding it the appropriate amount\n",
        "* Modify `weights` (just like you did for `conv_transpose1d_minimal`)\n",
        "* Use `conv1d_minimal` on your modified `x` and `weights` (just like you did for `conv_transpose1d_minimal`)\n",
        "</details>\n",
        "\n",
        "\n",
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def conv_transpose1d(\n",
        "    x: Float[Tensor, \"batch in_channels width\"],\n",
        "    weights: Float[Tensor, \"in_channels out_channels kernel_width\"],\n",
        "    stride: int = 1,\n",
        "    padding: int = 0,\n",
        ") -> Float[Tensor, \"batch out_channels output_width\"]:\n",
        "    \"\"\"Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\"\"\"\n",
        "    batch, ic, width = x.shape\n",
        "    ic_2, oc, kernel_width = weights.shape\n",
        "    assert ic == ic_2, f\"in_channels for x and weights don't match up. Shapes are {x.shape}, {weights.shape}.\"\n",
        "\n",
        "    # Apply spacing\n",
        "    x_spaced_out = fractional_stride_1d(x, stride)\n",
        "\n",
        "    # Apply modification (which is controlled by the padding parameter)\n",
        "    padding_amount = kernel_width - 1 - padding\n",
        "    assert padding_amount >= 0, \"total amount padded should be positive\"\n",
        "    x_mod = pad1d(x_spaced_out, left=padding_amount, right=padding_amount, pad_value=0)\n",
        "\n",
        "    # Modify weights, then return the convolution\n",
        "    weights_mod = einops.rearrange(weights.flip(-1), \"i o w -> o i w\")\n",
        "\n",
        "    return conv1d_minimal(x_mod, weights_mod)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz0IrIb9HZiL"
      },
      "source": [
        "Another fun fact about transposed convolutions - they are also called **backwards strided convolutions**, because they are equivalent to taking the gradient of Conv2d with respect to its output. As an optional bonus, can you formally prove this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2c1RMTeHZiL"
      },
      "source": [
        "### Exercise - 2D transposed convolutions\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴🔴🔴⚪\n",
        "> Importance: 🔵⚪⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-20 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Finally, we get to 2D transposed convolutions! Since there's no big conceptual difference between this and the 1D case, we'll jump straight to implementing the full version of these convolutions, with padding and strides. A few notes:\n",
        "\n",
        "* You'll need to make `fractional_stride_2d`, which performs spacing along the last two dimensions rather than just the last dimension.\n",
        "* Defining the modified version of your kernel will involve reversing on more than one dimension. You'll still need to perform the same rearrangement flipping the output and input channel dimensions though.\n",
        "* You can use the `force_pair` function from earlier this week (it's been imported for you, as have the `Pair` and `IntOrPair` types)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJWa_mr9HZiL"
      },
      "outputs": [],
      "source": [
        "def fractional_stride_2d(\n",
        "    x: Float[Tensor, \"batch in_channels height width\"], stride_h: int, stride_w: int\n",
        ") -> Float[Tensor, \"batch in_channels output_height output_width\"]:\n",
        "    \"\"\"\n",
        "    Same as fractional_stride_1d, except we apply it along the last 2 dims of x (height and width).\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "def conv_transpose2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> Tensor:\n",
        "    \"\"\"Like torch's conv_transpose2d using bias=False\n",
        "    x: shape (batch, in_channels, height, width)\n",
        "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
        "    Returns: shape (batch, out_channels, output_height, output_width)\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "tests.test_fractional_stride_2d(fractional_stride_2d)\n",
        "tests.test_conv_transpose2d(conv_transpose2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "939_ca-hHZiL"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "def fractional_stride_2d(\n",
        "    x: Float[Tensor, \"batch in_channels height width\"], stride_h: int, stride_w: int\n",
        ") -> Float[Tensor, \"batch in_channels output_height output_width\"]:\n",
        "    \"\"\"\n",
        "    Same as fractional_stride_1d, except we apply it along the last 2 dims of x (height and width).\n",
        "    \"\"\"\n",
        "    batch, in_channels, height, width = x.shape\n",
        "    width_new = width + (stride_w - 1) * (width - 1)\n",
        "    height_new = height + (stride_h - 1) * (height - 1)\n",
        "    x_new_shape = (batch, in_channels, height_new, width_new)\n",
        "\n",
        "    # Create an empty array to store the spaced version of x in.\n",
        "    x_new = t.zeros(size=x_new_shape, dtype=x.dtype, device=x.device)\n",
        "\n",
        "    x_new[..., ::stride_h, ::stride_w] = x\n",
        "\n",
        "    return x_new\n",
        "\n",
        "\n",
        "def conv_transpose2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> Tensor:\n",
        "    \"\"\"Like torch's conv_transpose2d using bias=False\n",
        "    x: shape (batch, in_channels, height, width)\n",
        "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
        "    Returns: shape (batch, out_channels, output_height, output_width)\n",
        "    \"\"\"\n",
        "    stride_h, stride_w = force_pair(stride)\n",
        "    padding_h, padding_w = force_pair(padding)\n",
        "\n",
        "    batch, ic, height, width = x.shape\n",
        "    ic_2, oc, kernel_height, kernel_width = weights.shape\n",
        "    assert ic == ic_2, f\"in_channels for x and weights don't match up. Shapes are {x.shape}, {weights.shape}.\"\n",
        "\n",
        "    # Apply spacing\n",
        "    x_spaced_out = fractional_stride_2d(x, stride_h, stride_w)\n",
        "\n",
        "    # Apply modification (which is controlled by the padding parameter)\n",
        "    pad_h_actual = kernel_height - 1 - padding_h\n",
        "    pad_w_actual = kernel_width - 1 - padding_w\n",
        "    assert min(pad_h_actual, pad_w_actual) >= 0, \"total amount padded should be positive\"\n",
        "    x_mod = pad2d(\n",
        "        x_spaced_out, left=pad_w_actual, right=pad_w_actual, top=pad_h_actual, bottom=pad_h_actual, pad_value=0\n",
        "    )\n",
        "\n",
        "    # Modify weights\n",
        "    weights_mod = einops.rearrange(weights.flip(-1, -2), \"i o h w -> o i h w\")\n",
        "\n",
        "    # Return the convolution\n",
        "    return conv2d_minimal(x_mod, weights_mod)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_OcmonPHZiL"
      },
      "source": [
        "### Exercise - transposed conv module\n",
        "\n",
        "> ```yaml\n",
        "> Difficulty: 🔴🔴⚪⚪⚪\n",
        "> Importance: 🔵🔵⚪⚪⚪\n",
        ">\n",
        "> You should spend up to 10-15 minutes on this exercise.\n",
        "> ```\n",
        "\n",
        "Now that you've written a function to calculate the convolutional transpose, you should implement it as a module just like you've done for `Conv2d` previously. Your weights should be initialised with the uniform distribution `Unif[-sqrt(k), sqrt(k)]` where `k = 1 / (out_channels * kernel_width * kernel_height)` (this is PyTorch's standard behaviour for convolutional transpose layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If4av4vEHZiL"
      },
      "outputs": [],
      "source": [
        "class ConvTranspose2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, out_channels: int, kernel_size: IntOrPair, stride: IntOrPair = 1, padding: IntOrPair = 0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Same as torch.nn.ConvTranspose2d with bias=False.\n",
        "        Name your weight field `self.weight` for compatibility with the tests.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = force_pair(kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"batch in_channels height width\"]\n",
        "    ) -> Float[Tensor, \"batch out_channels output_height output_width\"]:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        keys = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"]\n",
        "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in keys])\n",
        "\n",
        "\n",
        "tests.test_ConvTranspose2d(ConvTranspose2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVBAe1HoHZiL"
      },
      "source": [
        "<details><summary>Solution</summary>\n",
        "\n",
        "```python\n",
        "class ConvTranspose2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, out_channels: int, kernel_size: IntOrPair, stride: IntOrPair = 1, padding: IntOrPair = 0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Same as torch.nn.ConvTranspose2d with bias=False.\n",
        "        Name your weight field `self.weight` for compatibility with the tests.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = force_pair(kernel_size)\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        sf = 1 / (self.out_channels * self.kernel_size[0] * self.kernel_size[1]) ** 0.5\n",
        "        self.weight = nn.Parameter(sf * (2 * t.rand(in_channels, out_channels, *self.kernel_size) - 1))\n",
        "\n",
        "    def forward(\n",
        "        self, x: Float[Tensor, \"batch in_channels height width\"]\n",
        "    ) -> Float[Tensor, \"batch out_channels output_height output_width\"]:\n",
        "        return conv_transpose2d(x, self.weight, self.stride, self.padding)\n",
        "\n",
        "    def extra_repr(self) -> str:\n",
        "        keys = [\"in_channels\", \"out_channels\", \"kernel_size\", \"stride\", \"padding\"]\n",
        "        return \", \".join([f\"{key}={getattr(self, key)}\" for key in keys])\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTJWSYFWHZiL"
      },
      "source": [
        "Now, you're all done! You can go back and implement GANs or VAEs using the transposed convolution module you've just written."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}